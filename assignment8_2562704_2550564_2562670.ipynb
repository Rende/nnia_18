{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 8: Optimization for Training Deep Models (Deadline: 06 Jan, 23:59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set notebook to full width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For theoretical tasks you are encouraged to write in $\\LaTeX$. Jupyter notebooks support them by default. For reference, please have a look at the examples in this short excellent guide: [Typesetting Equations](http://nbviewer.jupyter.org/github/ipython/ipython/blob/3.x/examples/Notebook/Typesetting%20Equations.ipynb)\n",
    "\n",
    "Alternatively, you can upload the solutions in the written form as images and paste them inside the cells. But if you do this, **make sure** that the images are of high quality, so that we can read them without any problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0) Preamble : Understanding Gradient Signals of Activation functions (2.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Assignment-5, we saw the importance of **activation functions** in action for achieving **non-linear** decision boundary. Because of this functionality/behaviour, these functions are sometimes called simply as **non-linearities** in Deep Learning literature. In this task, we will understand the gradient signals of three different non-linearities: *Sigmoid*, *Tanh*, and *ReLU*. \n",
    "\n",
    "(Find more information at: [Comparison_of_activation_functions](https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12.0, 8.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i**) Implement the activation functions *Sigmoid*, *Tanh*, and *ReLU* and their gradient computation using only basic ops in TensorFlow (**1.5 point**)\n",
    "\n",
    "(No need to derive the gradient expression, just use it as is (see the link above for help); You can use TensorFlow for stable implementations of $sigmoid$ and $tanh$ calculations. However, you **should do** the gradient computation yourself using *only* the basic ops such as `tf.nn.<activation_function>`, `tf.subtract`, `tf.square` etc.)\n",
    "\n",
    "Unlike $\\color{blue}{NumPy}$, TensorFlow doesn't have support for `float128` as of 2017. So, we will just stick to `tf.float64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can be used to call .eval() on tensors\n",
    "isess = tf.InteractiveSession()\n",
    "one = tf.constant(1.0, dtype=tf.float64)\n",
    "zero = tf.constant(0, dtype=tf.float64)\n",
    "two = tf.constant(2.0, dtype=tf.float64)\n",
    "# TODO: implement activation functions & their gradient computations\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    x: numpy array with the range of input values for our plot.\n",
    "       should be `upcast`ed to float64, before taking sigmoid\n",
    "    \"\"\"\n",
    "    x = tf.cast(x, tf.float64)\n",
    "    return tf.div (one, (tf.add(one, tf.exp(-x))))\n",
    "\n",
    "\n",
    "def sigmoid_grad(sigm):\n",
    "    \"\"\"\n",
    "    sigm: compute gradient from the result of sigmoid.\n",
    "          should be `upcast`ed to float64, before taking sigmoid\n",
    "    \"\"\"\n",
    "    return tf.multiply(sigm, (tf.subtract(one, sigm)))\n",
    "\n",
    "                       \n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    x: numpy array with the range of input values for our plot.\n",
    "       should be `upcast`ed to float64, before taking relu\n",
    "    \"\"\"\n",
    "    x = tf.cast(x, tf.float64)\n",
    "    return tf.maximum(zero, x)\n",
    "\n",
    "\n",
    "def relu_grad(rel):\n",
    "    \"\"\"\n",
    "    rel: numpy array of `relu`ed values for the input\n",
    "    return a boolean array of dtype `int`\n",
    "    \"\"\"\n",
    "    return (rel >= zero)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"\n",
    "    x: numpy array with the `range` of input values for our plot.\n",
    "    should be `upcast`ed to float64, before taking tanh\n",
    "    \"\"\"\n",
    "    \n",
    "    return tf.subtract(tf.div (two, (tf.add(one, tf.exp( -tf.multiply(two, x))))), one)\n",
    "\n",
    "\n",
    "def tanh_grad(tnh):\n",
    "    \"\"\"\n",
    "    tnh: numpy array of `tanh`ed values for the input\n",
    "    \"\"\"\n",
    "    return tf.subtract(one, tf.square(tnh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ii) Plot the *gradient signals* of `Sigmoid`, `Tanh`, and `ReLU` activation functions overlaid in a single plot. (**0.5 points**) <br>\n",
    "Use $200$ evenly spaced samples from the closed interval $[-10.0, 11.0]$ as the truncated [domain](https://en.wikipedia.org/wiki/Domain_of_a_function). <br>\n",
    "$\\color{green}{Hint}$: While plotting, set the $y-axis$ limit to $[0, 1.1]$ for better interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsYAAAHVCAYAAADywj0dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEEIIIQQA/w88L1BVjBdU1gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12c5e0908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: implement plotting\n",
    "samples = np.linspace(-10.0, 11.0, num=200)\n",
    "init = tf.global_variables_initializer()\n",
    "plt.ylim(0,1.1)\n",
    "sig = sigmoid(samples).eval()\n",
    "sig_der = sigmoid_grad(sig).eval()\n",
    "\n",
    "rel = relu(samples).eval()\n",
    "rel_der = relu_grad(rel).eval()\n",
    "\n",
    "tnh = tanh(samples).eval()\n",
    "tnh_der = tanh_grad(tnh).eval()\n",
    "\n",
    "plt.plot(samples, sig_der, label=\"$Sigmoid$\");\n",
    "plt.plot(samples, rel_der, label=\"$ReLu$\");\n",
    "plt.plot(samples, tnh_der, label=\"$Tanh$\");\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**iii**) What do you observe from the plot? (**0.5 points**) <br>\n",
    "$\\color{red}{Note}:$ In addition to your own explanation, your solution should include how the *gradient signal* of one activation function differs from the other, over this truncated domain.\n",
    "\n",
    "(It is *very important* to understand how these *gradient signals* behave and how one differs from the other. This understanding will be useful for the task below, and also later on when we talk about *RNNs*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{violet}{Ans}$: \n",
    "**Sigmoid Function:** Gradient towards either end of the sigmoid function, the Y values tend to respond very less to changes in X as we can see the gradient at that region is small. That means at that region, the gradient is small or has vanished (cannot make significant change because of the extremely small value). The network refuses to learn further or is drastically slow. It is called “vanishing gradients” problem. \n",
    "\n",
    "**Tanh Function:** Gradient is stronger for this function than sigmoid (derivatives are steeper). Hence optimization is easier in this method. But still it suffers from “vanishing gradients” problem.\n",
    "\n",
    "**ReLu Function:** It avoids and rectifies “vanishing gradients” problem as Relu’s gradient is constant = 1. However for negative X, the gradient can go towards 0 (not visible in the truncated domain). It can cause a weight update which will makes it never activate on any data point again. That means ReLu could result in Dead Neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Exploding/Vanishing Gradients (7.5 points)\n",
    "\n",
    "In the lecture we've discussed some of the challenges in Neural Network Optimization. One among them is the so-called **Exploding/[Vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)** .\n",
    "\n",
    "To be more specific, this problem happens only during the backward pass in training (very deep) Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i**) Assume that you've a *100-layer* Feed Forward Neural Network with $sigmoid$ activation function as non-linearities. Explain mathematically why the *exploding or vanishing gradient problem* occurs only during the backward pass (and not during the forward pass) (**1 point**) .\n",
    "\n",
    "$\\color{green}{Hint}$: Think about what will happen to the *magnitude of gradient signal* when it reaches the first few layers (*i.e. layers near the input*), during *backpropagation*. The plot you did in *Preamble* section should also give you some intuition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{violet}{Ans}$: If we look at the gradient signals of sigmoid function the maximum is 1/4 ($\\sigma'(0) = 1/4$). The range of the derivative of the cost function is always in $(0,1/4] $. In a 100-layer Feed Forward Neural Network, each neuron or “activity” is derived from the previous: it is the previous activity multiplied by some weight and then fed through an activation function. If we use the standard approach to initializing the weights in the network, then we'll choose the weights using Gaussian so the weights will usually satisfy $|w_j|<1$. While propagating the error through the network to update the weights, we apply chain rule such as \n",
    "\n",
    "\n",
    "$$ \\frac{\\partial error}{\\partial w_1} = \\frac{\\partial error}{\\partial output} * \\frac{\\partial output}{\\partial hidden_{99}} * ... * \\frac{\\partial hidden_1}{\\partial w_1}$$\n",
    "If we take the last layer $ z_{100}= hidden_{99} * w_{100}$\n",
    "$$\\frac{\\partial output}{\\partial hidden_{99}} = \\frac{\\partial sigmoid(z_{100})}{\\partial z_{100}} * w_{100}$$\n",
    "The layer before the last one $ z_{99}= hidden_{98} * w_{99}$\n",
    "$$\\frac{\\partial hidden_{99}}{\\partial hidden_{98}} = \\frac{\\partial sigmoid(z_{99})}{\\partial z_{99}} * w_{99}$$\n",
    "\n",
    "so on so forth.\n",
    "Now, look at the magnitude of the terms in our chain rule expression:\n",
    "\n",
    "$\\frac{\\partial error}{\\partial w_1} = \\frac{\\partial error}{\\partial output} * \\overbrace{\\frac{\\partial sigmoid(z_{100})}{\\partial z_{100}}}^\\text{<1/4} *  \\overbrace{w_{100}}^\\text{<1} \\overbrace{\\frac{\\partial sigmoid(z_{99})}{\\partial z_{99}}}^\\text{<1/4} * \\overbrace{w_{99}}^\\text{<1}... * \\frac{\\partial hidden_1}{\\partial w_1}$\n",
    "\n",
    "As we've seen, the terms are typically less than 1/4 in magnitude. That means every time the gradient signal flows through a sigmoid gate, its magnitude always diminishes by one quarter (or more). As we backpropagate further back, we’d have many more small numbers in a product, creating an even tinier gradient. This problem occurs because of the gradient signals of sigmoid function. Since we do not need the derivative of sigmoid function during the forward pass, we only have it in the backward pass.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ii**) Explain how we can avoid the problem of gradient explosion. Your explanation should talk about what we do with the *norm of the gradient*. (**1 point**) <br>\n",
    "$\\color{green}{Hint}$: You can consult the [DL-book](http://www.deeplearningbook.org/) & TensorFlow API docs for more information; Also, we expect the formula and its explanation in your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{violet}{Ans}$: We can avoid the problem of gradient explosion by **gradient clipping**(or norm-clipping). Basically, we prevent gradients from blowing up by rescaling them so that their norm is at most a particular value $n$. I.e., if $||g|| > n$, where g is the gradient, we set\n",
    "$$ g \\leftarrow \\frac{ng}{||g||} $$ \n",
    "There are many instantiations of this idea (i.e. ||.|| could be L_2 norm) but the main concept is to limit the gradient to a maximum number and if the gradient exceeds that number, rescale the gradient parameters so the are limited within it. This customization retains the direction but limits the step size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### iii) Vanishing Gradient Problem in Action (5.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you will implement a four layer FFN network in TensorFlow to (visually) understand the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries and data\n",
    "import warnings\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using MNIST dataset for this task, but not all 55K samples. Only a subset of 5K samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data_MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting data_MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting data_MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting data_MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# read MNIST data\n",
    "mnist = input_data.read_data_sets(\"data_MNIST/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a subset of 5K samples (evenly distributed among all classes)\n",
    "\n",
    "# extract zero-based indices of 1-hot locations in the array.\n",
    "onehot_indices = [np.where(x==1)[0][0] for x in mnist.train.labels]\n",
    "sss = StratifiedShuffleSplit(y=onehot_indices, n_iter=1, test_size=5000, random_state=0)\n",
    "\n",
    "for _, subset_indices in sss:\n",
    "    subset_images = mnist.train.images[subset_indices]\n",
    "    subset_labels = mnist.train.labels[subset_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 10)\n"
     ]
    }
   ],
   "source": [
    "# training set\n",
    "train_images = subset_images\n",
    "train_labels = subset_labels\n",
    "\n",
    "print (train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a *four layer FF Neural Network* in TensorFlow, with first hidden layer size 50, second hidden layer size 50, third hidden layer size 50 (i.e. 50 hidden units), and output layer size corresponding to the size of `num_labels`. Specifically, the network will be: `inputs->hidden1->hidden2->hidden3->output`\n",
    "\n",
    "$\\color{red}{Note}$: $\\star$ Initialize the weight matrices using [Xavier initialization](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/xavier_initializer). Biases can be initialized with [zeros](https://www.tensorflow.org/api_docs/python/tf/zeros). <br>\n",
    "$\\hspace{2em}$ $\\star$ Use `tf.float32` for all tensors, for faster computation. <br>\n",
    "$\\hspace{2em}$ $\\star$ Use **sigmoid activation** function as the non-linearity. <br>\n",
    "$\\hspace{2em}$ $\\star$ Use $L_2$ regularization (i.e. weight decay) with alpha ($\\alpha$) value of *1e-6*. See [tf-api](https://www.tensorflow.org/api_docs/python/tf/nn/l2_loss) for more details. <br>\n",
    "$\\hspace{2em}$ $\\star$ Use *softmax cross entropy* as the loss function as in Assignment-5. (Note that, generally, this is used only during training time.)<br>\n",
    "$\\hspace{2em}$ $\\star$ Use `Gradient Descent Optimizer` with learning rate of 0.5<br>\n",
    "$\\hspace{2em}$ $\\star$ Here we are doing *batch training* (i.e. we use all 5K samples at once).<br>\n",
    "\n",
    "$\\color{green}{Hint}$: To understand how the *gradient magnitude (of weights)* changes during training, we need the gradients for the *weights*. Look into [tf-processing-gradients](https://www.tensorflow.org/api_docs/python/tf/train/Optimizer#processing_gradients_before_applying_them) for help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these in mind, now build the computation graph of our model below. (**1.5 points**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement\n",
    "    \n",
    "def model(input_shape, output_shape, n_hidden_units, x):\n",
    "    W1 = tf.get_variable('weights-hidden1', shape=[input_shape[1], n_hidden_units], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.get_variable('biases-hidden1', shape=[n_hidden_units], dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "    a1 = tf.nn.sigmoid(tf.matmul(x, W1) + b1)\n",
    "    \n",
    "    W2 = tf.get_variable('weights-hidden2', shape=[n_hidden_units, n_hidden_units], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.get_variable('biases-hidden2', shape=[n_hidden_units], dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "    a2 = tf.nn.sigmoid(tf.matmul(a1, W2) + b2)\n",
    "    \n",
    "    W3 = tf.get_variable('weights-hidden3', shape=[n_hidden_units, n_hidden_units], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.get_variable('biases-hidden3', shape=[n_hidden_units], dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "    a3 = tf.nn.sigmoid(tf.matmul(a2, W3) + b3)\n",
    "    \n",
    "    W4 = tf.get_variable('weights-output', shape=[n_hidden_units, output_shape[1]], dtype=tf.float32, initializer=tf.random_normal_initializer())\n",
    "    b4 = tf.get_variable('biases-output', shape=[output_shape[1]], dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "    a4 = tf.matmul(a3, W4) + b4\n",
    "    return W1, W2, W3, W4, b1, b2, b3, b4, a4\n",
    "\n",
    "\n",
    "# hparams\n",
    "# ---------\n",
    "# parameter for regularization.\n",
    "alpha = 1e-6\n",
    "\n",
    "# learning rate\n",
    "lr = 0.5\n",
    "\n",
    "# TODO: Implement\n",
    "g = tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "    n_hidden_units = 50\n",
    "    input_shape = (None,train_images.shape[1])\n",
    "    output_shape = (None,train_labels.shape[1])\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape=input_shape, name='X')\n",
    "    y = tf.placeholder(tf.float32, shape=output_shape, name='y')\n",
    "    \n",
    "    W1, W2, W3, W4, b1, b2, b3, b4, unscaled_logits = model(input_shape, output_shape, n_hidden_units, x)\n",
    "    predictions = tf.nn.sigmoid(unscaled_logits)\n",
    "    \n",
    "    # Loss\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=unscaled_logits))\n",
    "    regularizers = tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2)+ tf.nn.l2_loss(W3)+ tf.nn.l2_loss(W4)\n",
    "    loss = loss + alpha * regularizers\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = compute_accuracy(predictions, y)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "    variables = [W1, W2, W3, W4, b1, b2, b3, b4]\n",
    "    grads_and_vars = optimizer.compute_gradients(loss, variables)\n",
    "    grads = [(gv[0]) for gv in grads_and_vars]\n",
    "    step = optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function below which computes accuracy. (**0.5 points**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement\n",
    "def compute_accuracy(predictions, ground_truth):\n",
    "    model_prediction = tf.greater_equal(predictions, 0.5)\n",
    "    model_prediction = tf.cast(model_prediction, tf.float32)\n",
    "    correct_prediction = tf.equal(model_prediction, ground_truth)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \"\"\"\n",
    "    compute & return accuracy (in percentage).\n",
    "    predictions: predictions from the network\n",
    "    ground_truth: 1-hot array of ground truth labels\n",
    "    \"\"\"\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement code which performs network training (with `num_steps = 3001`). (**1.5 point**) <br>\n",
    "$\\color{green}{Hint}$: To understand how the *gradient magnitude (of weights)* (i.e. their absolute values) changes across layers and with each of the 3K steps, we need to cache the *mean of absolute value of gradients (of weights)* of all 4 layers in each step of the total 3K steps. We'll need these gradients for plotting later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Accuracy: 0.60\n",
      "Epoch: 100, Training Accuracy: 0.90\n",
      "Epoch: 200, Training Accuracy: 0.90\n",
      "Epoch: 300, Training Accuracy: 0.90\n",
      "Epoch: 400, Training Accuracy: 0.90\n",
      "Epoch: 500, Training Accuracy: 0.90\n",
      "Epoch: 600, Training Accuracy: 0.91\n",
      "Epoch: 700, Training Accuracy: 0.92\n",
      "Epoch: 800, Training Accuracy: 0.93\n",
      "Epoch: 900, Training Accuracy: 0.94\n",
      "Epoch: 1000, Training Accuracy: 0.94\n",
      "Epoch: 1100, Training Accuracy: 0.95\n",
      "Epoch: 1200, Training Accuracy: 0.95\n",
      "Epoch: 1300, Training Accuracy: 0.95\n",
      "Epoch: 1400, Training Accuracy: 0.95\n",
      "Epoch: 1500, Training Accuracy: 0.96\n",
      "Epoch: 1600, Training Accuracy: 0.96\n",
      "Epoch: 1700, Training Accuracy: 0.96\n",
      "Epoch: 1800, Training Accuracy: 0.97\n",
      "Epoch: 1900, Training Accuracy: 0.97\n",
      "Epoch: 2000, Training Accuracy: 0.97\n",
      "Epoch: 2100, Training Accuracy: 0.97\n",
      "Epoch: 2200, Training Accuracy: 0.97\n",
      "Epoch: 2300, Training Accuracy: 0.97\n",
      "Epoch: 2400, Training Accuracy: 0.97\n",
      "Epoch: 2500, Training Accuracy: 0.97\n",
      "Epoch: 2600, Training Accuracy: 0.98\n",
      "Epoch: 2700, Training Accuracy: 0.98\n",
      "Epoch: 2800, Training Accuracy: 0.98\n",
      "Epoch: 2900, Training Accuracy: 0.98\n",
      "Epoch: 3000, Training Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "# TODO: implement network training\n",
    "w1_list = []\n",
    "w2_list = []\n",
    "w3_list = []\n",
    "w4_list = []\n",
    "\n",
    "epochs = 3001\n",
    "# Start a Session \n",
    "with tf.Session(graph=g) as sess:\n",
    "    # Initialize all variables\n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle training data\n",
    "        indices = list(range(len(train_images)))\n",
    "        feed_dict = {x: train_images[indices], y: train_labels[indices]}\n",
    "        sess.run(step, feed_dict=feed_dict)\n",
    "        acc = sess.run(accuracy, feed_dict=feed_dict)\n",
    "        \n",
    "        gradients = sess.run(grads, feed_dict=feed_dict)\n",
    "        mean_w1 = np.mean(np.absolute(gradients[0].flatten()))\n",
    "        w1_list.append(mean_w1)\n",
    "        mean_w2 = np.mean(np.absolute(gradients[1].flatten()))\n",
    "        w2_list.append(mean_w2)\n",
    "        mean_w3 = np.mean(np.absolute(gradients[2].flatten()))\n",
    "        w3_list.append(mean_w3)\n",
    "        mean_w4 = np.mean(np.absolute(gradients[3].flatten()))\n",
    "        w4_list.append(mean_w4)\n",
    "        \n",
    "        # Print accuracy\n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch: {:d}, Training Accuracy: {:.2f}'.format(epoch, acc))\n",
    "    \n",
    "    \n",
    "# \n",
    "# TODO: cache the mean of absolute value of gradients\n",
    "#\n",
    "# TODO: print stats for every 100 steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network for 3K steps. Print the *loss* and *training accuracy* after every 100 steps. <br> (This will be helpful as a sanity check for your implementation and also for the later analysis question) <br>\n",
    "$\\color{green}{Hint}$: If you implement it correctly, and train it for 3K steps with the provided hyperparameters, the execution time on a 4GB machine would be *ca. 12 mins*, and the *training accuracy* would be around 99.9%. Also, here we're doing only training; i.e. no validation and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**iv**) **Plotting**\n",
    "\n",
    "Now, plot the value of gradient magnitudes of all 4 layers, overlaid in a single plot. (**1 point**) <br>\n",
    "$\\color{green}{Hint}$: Your $x-axis$ should be $\\#steps$ and $y-axis$ should have the *mean value of gradient magnitudes (of weights)* corresponding to all 4 layers.\n",
    "\n",
    "$\\color{red}{Note}$: The important part of this exercise is the plot that explains the *vanishing gradient problem* and the observations that you make out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtMAAAHVCAYAAAAzRXexAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nktTTT9N6EKAH2MNdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x130287c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: implement plotting\n",
    "#\n",
    "# \n",
    "\n",
    "steps = np.arange(epochs)\n",
    "plt.plot(steps, w1_list, label=\"$W1$\");\n",
    "plt.plot(steps, w2_list, label=\"$W2$\");\n",
    "plt.plot(steps, w3_list, label=\"$W3$\");\n",
    "plt.plot(steps, w4_list, label=\"$W4$\");\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1) What are your observations from the above plot? How does the *flow of gradient signal* through different layers changes with the increase in number of steps (during training)? (**0.5 points**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{violet}{Ans}$: The mean values of gradient magnitudes start decreasing from for W4 = .015,  W3 = .011, W2 = .003, W1 = .00 (almost none).\n",
    "While the change in the last layer (W4) very big (.014), in the first layer the change is so close to 0. It is obvious that the gradient signal is exponentially decreasing with the increase in number of steps. This causes that while W4 is adaptated very well within the first steps, W1 remains almost the same during the whole training.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2) Now, analyze the *training accuracy & loss values* from the stats of the whole training loop. At the same time (in parallel), observe from the plot how the gradient signal(magnitude) changes across layers as the steps increase. *Relate these two observations* and explain your findings. (**0.5 points**) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{violet}{Ans}$: For the first 100 steps the accuracy shows remarkable improvement, at the same time the gradient signals of the layers decrease instantly. Between 700 - 1500 steps the accuracy improves but so slowly and we can see the changes in the gradient signals which are very small. After 2000 steps the accuracy doesn't improve at all and correspondently the gradient signals of the weights are almost 0. So we can understand that in the last 1000 steps, we reached the tails of the gradient of the sigmoid function therefore cannot make significant improvement because of the extremely small value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Understanding Optimization Algorithms (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization algorithms for Deep Neural Networks\n",
    "\n",
    "In this task, you will study and understand various optimization methods such as *Vanilla Gradient Descent, Gradient Descent with Momentum, AdaGrad, Adam, and RMSProp*. <br>\n",
    "For a terse & high level overview of these algorithms, see here: [SGD-variants](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Extensions_and_variants)\n",
    "\n",
    "$\\color{red}{Note}$: Adam was *not introduced* in the lecture but it's an important algorithm to know, particularly for NLP & Computer Vision problems. Refer [section 8.5.3](http://www.deeplearningbook.org/contents/optimization.html) for details. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Stochastic Gradient Descent (SGD) Optimizer (2 points)\n",
    "\n",
    "i) Is it important to use *learning rate decay* for (mini-batch) SGD based learning? Justify. (**0.5 + 1 points**) <br>\n",
    "ii) What is the advantage of using a *small batch size* during training instead of the *full set of examples at once* in the training data? (**0.5 points**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{violet}{Ans}$:\n",
    "\n",
    "i) It is important to use learning rate decay in (mini-batch) SGD, else the algorithm may miss/overshoot the (local) optimum and as a consequence fluctuate around the minimum or diverge. This stems from the fact that the random sampling of m training examples introduces noise that does not vanish even when arriving at a minimum. <br>\n",
    "ii) The advantage of using small batch size is that in this way the model is implicitly regularized. Another advantage is that this approach does not cause problems with cache memory limits. The advantage of using batches/SGD at all is that compared with Batch gradient descent it converges more quickly and is allowed to incorporate new training examples in a so-called online fashion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Momentum Optimizer (2 points)\n",
    "\n",
    "Familiarize yourself with the algorithm from the lecture slide and [DL book](http://www.deeplearningbook.org/contents/optimization.html) and understand how it works.<br>\n",
    "i) Now describe, in what way Momentum method is different from *vanilla SGD*? (**1 point**)\n",
    "\n",
    "ii) It is known that the cost function of NNs usually has many saddle points. How does momentum method help to alleviate the problem of getting stuck in these saddle points when compared to vanilla SGD? (**1 point**) <br>\n",
    "$\\color{red}{Note}$: You can assume that, in both cases, we start at the same initial point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{violet}{Ans}$:\n",
    "\n",
    "i) In contrast to vanilla SGD, Momentum accumulates in $v$ the gradient elements from the previous iterations. <br>\n",
    "$v = \\alpha v - \\triangledown_{\\theta} \\left( \\frac{1}{m} \\sum^{m}_{i=1}L(f(x^{(i)}; \\theta), y^{(i)})\\right)\\\\\n",
    "\\theta = \\theta + v$ <br>\n",
    "The momentum term increases for dimensions whose gradients point in the same directions as at previous iterations and reduces the updates for dimensions whose gradients change directions. In this way Momentum can handle ravines and dampen oscillations as well as accelerate convergence.\n",
    "<br>\n",
    "ii) Momentum would be able to recover from entering a saddle point because it can be formuleted as the interdependance of two forces. The first takes into account the direction of the previous gradients and accelerates proportionally to the negative gradient of the cost function $-\\triangledown_{\\theta}J(\\theta)$. The second is opposed to it and proportional to $-v(t)$; it corresponds to viscous drag in physics terminology and acts as a \"friction\" slowing down movement in certain direction. \"Viscous drag is weak enough that the gradient can continue to cause motion until a minimum is reached, but strong enough to prevent motion if the gradient does not justify moving\" (DL book, p.299). In this case when reaching a saddle point, the algorithm will first go back and forth in one direction, dampening the progress. If there has consistently been positive progress in another direction, it ends up building up and going down that way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) AdaGrad optimizer (2 points)\n",
    "\n",
    "Further reading: [Adaptive SubGradient Methods for Stochastic Optimization](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n",
    "\n",
    "$\\color{red}{Note}$: An important algorithm for NN training for NLP and CV problems.\n",
    "\n",
    "Refer the slides and the [DL book](http://www.deeplearningbook.org/contents/optimization.html) and understand how AdaGrad algorithm works.  <br>\n",
    "i) Describe the basic idea behind AdaGrad algorithm. (**1 point**) <br>\n",
    "ii) What is the disadvantage of this algorithm, in practice? (**0.5 points**)<br>\n",
    "iii) What is the important property of this algorithm? (**0.5 points**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{violet}{Ans}$:  <br>\n",
    "i)  The basic idea behind AdaGrad is that the learning rate adapts based on the parameters in such way, that it decreases for more frequent parameters and increases for rarer ones. This is an advantage especially in settings where data is sparse and sparse parameters are more informative. <br>\n",
    "ii) The learning rate decreases monotonically (as the term $\\frac{\\eta}{\\sqrt{G_{j,j}+\\epsilon}}$ accumulates the squared gradients in the denominator and their sum grows) to a point where the algorithm stops learning. \n",
    "<br>\n",
    "iii) One important property of the algorithm is that the learning rate adapts automatically and does not need to be adapted manually. Moreover, as already mentioned, the algorithm distinguishes between more and less sparse parameters and updates them accordingly.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) RMSProp optimizer (2 points)\n",
    "\n",
    "i) How does RMSProp differ from AdaGrad?  (**1 point**) <br>\n",
    "ii) What disadvantage(s) in AdaGrad are solved by RMSProp? Explain it using the formula. (**1 point**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{violet}{Ans}$:  <br>\n",
    "i)  <br> The $g_t $ term is calculated by exponentially decaying average and not the sum of gradients.\n",
    "\n",
    "ii) <br> When the gradient is very large, alpha is reduced and vice-versa. \n",
    "$g_(t+1)=γg_t+(1−γ)δL(θ)^2$\n",
    "\n",
    "Here $g_t$ is called the second order moment of δL.Additionally, a first order moment $ m_t $ can also be introduced.\n",
    "$m_(t+1)=γm_t+(1−γ)δL(θ)$\n",
    "\n",
    "$g_(t+1)=γg_t+(1−γ)δL(θ)^2$\n",
    "\n",
    "Adding momentum as in the first case,\n",
    "$v_(t+1)=μvt−\\frac{αδL(θ)}{\\sqrt{g_(t+1)−m_(t+1)^2+ϵ}} $\n",
    "And finally collecting new theta \n",
    "$θ_(t+1)=θ_t+v_(t+1)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Adam Optimizer (2 points)\n",
    "\n",
    "Further reading: [ADAM: A Method for Stochastic Optimization](https://arxiv.org/pdf/1412.6980.pdf)\n",
    "\n",
    "$\\color{red}{Note}$: Adam was *not introduced* in the lecture but it's an important algorithm to know, particularly for NLP & Computer Vision problems. Refer [section 8.5.3](http://www.deeplearningbook.org/contents/optimization.html) for details. <br>\n",
    "\n",
    "i) Is there any similarity between RMSProp and Momentum? Is it possible to combine them? (**1 point**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) Describe the way in which Adam algorithm performs parameter update.<br>\n",
    "What are the advantage(s) of such an update rule when compared to RMSProp (with momentum)? (**1 point**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{violet}{Ans}$:  <br>\n",
    "i)  <br> RMSProp and Momentum try to adjust the step size so that the step is on the same scale as the gradients - as the average gradient gets smaller, the coefficient in the SGD update gets bigger to compensate.\n",
    "\n",
    "To combine them,we could add the momentum, and normalize the learning rate using the moving average squared gradient.\n",
    "    \n",
    "ii) <br>Adam algorithm computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients.\n",
    "Instead of adapting the parameter learning rates based on the average first moment (the mean) as in RMSProp, Adam also makes use of the average of the second moments of the gradients (the uncentered variance).\n",
    "Specifically, the algorithm calculates an exponential moving average of the gradient and the squared gradient, and the parameters beta1 and beta2 control the decay rates of these moving averages.\n",
    "\n",
    "Compared to RMSProp,it achieves good results fast.Its bias-correction helps Adam slightly outperform RMSprop towards the end of optimization as gradients become sparser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Christmas Bonus (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where and how could you apply a deep neural network for ensuring safety of humans during Christmas & New Year celebrations?\n",
    "\n",
    "1.Real-time analysis of behaviors to find the weird actions of terrorist when there is a crow of  person to avoid terrorist attack。\n",
    "\n",
    "2.Understand an event with videos of differt perspectives after some accidents and could provide the process of how it has happened.\n",
    "\n",
    "3.After the festival, many people drive back to the big city to work, so on the high way,the auto driving could help release the burden of car flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\color{red}{\\star \\star We\\hspace{0.5em}Wish\\hspace{0.5em}You\\hspace{0.5em}a\\hspace{0.5em}Merry\\hspace{0.5em}Christmas! \\star \\star} $ $\\color{violet}{\\&}$ $\\color{green}{Happy\\hspace{0.5em}New\\hspace{0.5em}Year!} $ - NNIA Team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "You should provide a single Jupyter notebook as the solution. The naming should include the assignment number and matriculation IDs of all members in your team in the following format:\n",
    "**assignment-8_matriculation1_matriculation2_matriculation3.ipynb** (in case of 3 members in a team). \n",
    "Make sure to keep the order matriculation1_matriculation2_matriculation3 the same for all assignments.\n",
    "\n",
    "Please submit the solution to your tutor (with **[NNIA][assignment-8]** in email subject):\n",
    "1. Maksym Andriushchenko <s8mmandr@stud.uni-saarland.de>\n",
    "2. Marius Mosbach <s9msmosb@stud.uni-saarland.de>\n",
    "3. Marimuthu Kalimuthu <s8makali@stud.uni-saarland.de>\n",
    "4. Rajarshi Biswas <rbisw17@gmail.com>\n",
    "\n",
    "Note :  **If you are in a team, you should submit only 1 solution to only 1 tutor.** <br>\n",
    "$\\hspace{2em}$ **Submissions violating these rules will not be graded.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
