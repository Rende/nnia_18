{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 4: Machine Learning Fundamentals & Linear Regression (Deadline: 01 Dec 23:59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Fundamentals(7 points)\n",
    "For theoretical tasks you are encouraged to write in $\\\\LaTeX$. Jupyter notebooks support them by default. For reference, please have a look at the examples in this short excellent guide: [Typesetting Equations](http://nbviewer.jupyter.org/github/ipython/ipython/blob/3.x/examples/Notebook/Typesetting%20Equations.ipynb)\n",
    "\n",
    "Alternatively, you can upload the solutions in the written form as images and paste them inside the cells. But if you do this, **make sure** that the images are of high quality, so that we can read them without any problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. Sigmoid Function (1.5 points)\n",
    "The special case of the logistic function is the *sigmoid function* which is defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "  \\sigma(a) = \\frac{1}{1 + e^{-a}}\n",
    "\\end{equation*}\n",
    "\n",
    "a) Compute its gradient analytically. (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "(1) \\; \\; \\; &\\sigma'(a) = \\frac{d}{da}(1+e^{-a})^{-1} = (-1)(1+e^{-a})^{-2}\\frac{d}{da}(1+e^{-a}) =\\\\\n",
    "    &(-1)(1+e^{-a})^{-2}(0 +\\frac{d}{da}e^{-a}) = (-1)(1+e^{-a})^{-2}(e^{-a}\\frac{d}{da}(-a)) =\\\\\n",
    "    &(-1)(1+e^{-a})^{-2}e^{-a}(-1) = \\frac{e^{-a}}{(1+e^{-a})^{2}}\\\\[15pt]\n",
    "    &\\sigma'(a) =  \\frac{e^{-a}+1-1}{(1+e^{-a})^{2}} =  \\frac{1 + e^{-a}}{(1+e^{-a})^{2}} -\\frac{1}{(1+e^{-a})^{2}} =\\\\\n",
    "(2)\\; \\; \\; &\\frac{1}{(1+e^{-a})} - \\frac{1}{(1+e^{-a})^{2}} = \\frac{1}{(1+e^{-a})}\\left(1-\\frac{1}{(1+e^{-a})}\\right) = \\sigma(a)(1-\\sigma(a))\\\\\n",
    " \\end{align}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) What are the inherent properties that you observe from the above computed gradient? (0.5 points) <br />\n",
    "   *Hint: Think about how would the gradient signal be for the whole domain of the sigmoid function*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in (2) the calculation of $\\sigma'(a)$ is very easy and can be very efficiently reused and modified in the course of the training. However, for very big or very small $a$ the values of $\\sigma'(a)$ saturate to 0. Eventually this might cause that the signal that flows to the neuron is too small and the network doesn't learn. One should also take this into account when choosing initial weights for the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Prove that the sigmoid function is symmetric. (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the point of symmetry of the sigmoid function is not (0,0) but (0, $\\frac{1}{2}$) and the function is odd, we want to prove, that $f(x) = \\sigma(x) - \\frac{1}{2}$ is symmetric at the origin (0,0). Odd functions are symmetric iff $f(-x) = -f(x)$ <br/>\n",
    "\\begin{align} \n",
    "f(x) &=  \\sigma(x) - \\frac{1}{2} = \\frac{1}{1+e^{-x}} - \\frac{1}{2} = \\frac{2-1-e^{-x}}{2(1+e^{-x})} = \\frac{1-e^{-x}}{2(1+e^{-x})} = \\frac{1-\\frac{1}{e^{x}}}{2(1+\\frac{1}{e^{x}})} = \\frac{\\frac{e^{x}-1}{e^{x}}}{2(\\frac{e^{x}+1}{e^{x}})} =\\frac{e^{x}-1}{2(e^{x}+1)}\\\\\n",
    "-f(x) &= -\\frac{e^{x}-1}{2(e^{x}+1)} = \\frac{1 - e^{x}}{2(e^{x}+1)}\\\\[15pt]\n",
    "f(-x) &=  \\sigma(-x) - \\frac{1}{2} = \\frac{1}{1+e^{x}} - \\frac{1}{2} = \\frac{2-1-e^{x}}{2(1+e^{x})} = \\frac{1-e^{x}}{2(e^{x}+1)}\\\\\n",
    "\\end{align}\n",
    "$f(-x)$ = $-f(x)$, so we have shown, that $f(x)$ is odd and symmetric at the origin. From this follows that $\\sigma(x)$ is symmetric at (0, $\\frac{1}{2}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. Regularization (3.5 points)\n",
    "\n",
    "In the lecture, we've seen that we can add a *regularizer* to our cost function to avoid *over or underfitting*. For example, consider the following training criterion for linear regression:\n",
    "\n",
    "\\begin{equation*}\n",
    "  J(\\textbf{w}) = \\frac{1}{m}\\sum_{i=1}^{m} \\Vert\\hat{y}^{(i)} - y^{(i)}\\Vert^{2} + \\lambda\\Omega(\\textbf{w})\n",
    "\\end{equation*}\n",
    "where $\\Omega(\\textbf{w}) = \\textbf{w}^{T}\\textbf{w}$ is the regularizer.\n",
    "\n",
    "a) In the above criterion, what is the role of the regularization parameter $\\lambda$ on the regularizer (i.e. parameters of our model) while minimizing $J(\\textbf{w})$? (1.0 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda$ should be a non-negative real bumber, with $\\lambda=0$ corresponding to no regularization, and larger values of $\\lambda$ corresponding to more regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Is $\\lambda$ the model parameter or a hyperparameter? Justify.(0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter,weighs the relative contribution of the norm penalty term $\\Omega$.We cannot estimate these parameters directly, but we need to account for them.Here it is obvioud,  $\\lambda$ is not estimated from data.\n",
    "\n",
    "\n",
    "A model parameter is a configuration variable that is internal to the model and whose value can be estimated from data.Hyperparameter is a configuration that is external to the model and whose value cannot be estimated from data,and it must be set manually and tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Derive the closed form solution for the weights ($\\textbf{w}$) in the above criterion.(2.0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "  J(\\textbf{w}) = \\frac{1}{m}\\sum_{i=1}^{m} \\Vert\\hat{y}^{(i)} - y^{(i)}\\Vert^{2} + \\lambda\\Omega\n",
    "\\end{equation*}\n",
    "\n",
    "$$J(\\textbf{w}) = \\frac{1}{m}|| \\textbf{X}\\textbf{w}-\\textbf{Y} ||_2^2 + \\lambda\\textbf{w}^{T}\\textbf{w}$$\n",
    "\n",
    "$$J(\\textbf{w}) = \\frac{1}{m}(\\textbf{w}^{T}\\textbf{X}^{T}\\textbf{X}\\textbf{w} - 2\\textbf{w}^{T}\\textbf{X}^{T}\\textbf{Y} + \\textbf{Y}^{T}\\textbf{Y}) + \\lambda\\textbf{w}^{T}\\textbf{w}$$\n",
    "\n",
    "first derivative of J over $\\textbf{w}$ is then $$\\frac{1}{m}(2\\textbf{X}^{T}\\textbf{X}\\textbf{w} - 2\\textbf{X}^{T}\\textbf{Y}) + 2\\lambda\\textbf{w}$$\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "  \\nabla_wJ(\\textbf{w}) = 2X^T({X\\textbf{w}-\\textbf{y}})+2\\lambda\\textbf{w}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3. Maximum Likelihood Estimation (MLE) (2 points)\n",
    "Consider the density function of a ***univariate Gaussian distribution***\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    " p(x;\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left(-\\frac{1}{2\\sigma^2}(x-\\mu)^{2}\\right)\n",
    "\\end{equation*}\n",
    "where $\\mu$ is the $\\textit{mean}$ and $\\sigma^{2}$ is the $\\textit{variance}$. \n",
    "\n",
    "Let's say you're given *N* samples (i.e. $x_1, x_2, x_3, ..., x_N$) which are drawn from the above stated distribution. Also, you can assume that these samples are **i.i.d** (i.e. [independent and identically distributed](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)).\n",
    "\n",
    "Now, please derive the *MLE step-by-step* for:\n",
    "\n",
    "a) *mean* $(\\mu)$. (1.0 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For a simple random sample of n normal random variables,the MLE:\n",
    "    \\begin{equation*}\n",
    " L(\\mu,\\sigma^2|x)= (\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left(-\\frac{1}{2\\sigma^2}(x_1-\\mu)^{2}\\right))*(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left(-\\frac{1}{2\\sigma^2}(x_2-\\mu)^{2}\\right))...(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left(-\\frac{1}{2\\sigma^2}(x_n-\\mu)^{2}\\right))\n",
    "                  =\\frac{1}{\\sqrt{(2\\pi\\sigma^2)^n}}exp\\left(-\\frac{1}{2\\sigma^2}\\right)\\sum_{i=1}^N (a_i-\\mu)^2\n",
    "\\end{equation*}\n",
    "$log(\\mu,\\sigma^2|x)=\\sum_{i=1}^N log(\\frac{1}{\\sqrt{2\\pi\\sigma^2}})-\\frac{1}{2\\sigma^2}(x_i-\\mu)^{2}$\n",
    "    \n",
    "    \n",
    "Take derivatives of this with respect to $\\mu$ and get:\n",
    "    $\\frac{\\partial log(\\mu,\\sigma^2|x) }{\\partial \\mu}=\\sum_{i=1}^N (x_i-\\mu)^2$\n",
    "    \n",
    "    \n",
    "   Set the left side to 0, we have:\n",
    "     $0=\\sum_{i=1}^N  \\frac{x_i-\\mu}{\\sigma^2}$,\n",
    "     $0=\\sum_{i=1}^N x_i-\\mu$,$\\sum_{i=1}^N \\mu=\\sum_{i=1}^N x_i$,$N\\mu=\\sum_{i=1}^N x_i$,$u=\\frac{\\sum_{i=1}^N x_i}{N}\\sum_{i=1}^N (x_i-\\mu)^2$\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) *variance* $(\\sigma^2)$. (1.0 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take derivatives of this with respect to $\\sigma^2$ and get:\n",
    "    $\\frac{\\partial log(\\mu,\\sigma^2|x) }{\\partial \\sigma^2}=\\frac{-N}{2\\sigma^2}+\\frac{1}{2(\\sigma^2)^2}\\sum_{i=1}^N (x_i-\\mu)^2=\\frac{-N}{2(\\sigma^2)^2}(\\sigma^2-\\frac{1}{N}\\sum_{i=1}^N (x_i-\\mu)^2))$\n",
    "We know $\\mu$ is the mean, we obtain \n",
    "          $\\sigma^2(x)=\\frac{1}{N}\\sum_{i=1}^N (x_i-\\mu)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression (13 points)\n",
    "\n",
    "#### 1. Introduction\n",
    "As we have seen in first assignment sheet, when we have one independent (or explanatory) variable and a scalar dependent variable, it is called **simple linear regression**.\n",
    "But, when there are more than one explanatory variable (i.e. $x^{(1)}, x^{(2)}, ...,x^{(k)}$), and a single scalar dependent variable (*y*), then it's called $\\textit{multiple linear regression}$. (Please don't confuse this with *multivariate linear regression* where we predict more than one (correlated) dependent variable.)\n",
    "\n",
    "Here, we will implement a **multiple linear regression** model in Python/NumPy using the *Gradient Descent* algorithm. Particularly, we will be using $\\textit{stochastic gradient descent}$ (*SGD*) where one performs the update step using a small set of training samples of size *batch_size* which we will set to 64. This is again a hyperparameter but in this exercise we will just use a fixed batch-size of *64* (i.e. we go through the training samples sampling 64 at a time and perform gradient descent.) Such a procedure is sometimes called *mini-batch gradient descent* in the deep learning community.\n",
    "\n",
    "Going through all the training samples *once* is called an **epoch**. Ideally, the algorithm has to go through multiple epochs over the training samples, each time shuffling it, until a convergence criterion has been satisfied. <br />\n",
    "\n",
    "Here, we will set a *tolerance value* for the difference in error (i.e. change in MSE values between subsequent epochs) that we will accept. Once this difference falls below the *tolerance value*, we terminate our training phase and return the parameters. \n",
    "\n",
    "We repeat the above training procedure for all possible hyperparameter combinations. Later on, using these parameters (*i.e. weight vectors*), we compute the prediction for validation data and the corresponding MSE values. And then, we pick the hyperparameter combination which yielded the least MSE.\n",
    "\n",
    "As a next step, we will combine training data and validation data and make it as our *new training data*. We keep the test data as it is. Using the hyperparameter combination (for the least MSE) that we found above, we train the model again with the *new training data* and obtain the parameter (*i.e. weight vector*) after convergence according to our *tolerance value*.\n",
    "\n",
    "Phew! That will be our much desired *weight vector*. This is then used on the *test data*, which has not been seen by our algorithm so far, to make a prediction. The resulting MSE value will be the so-called [*generalization error*](https://en.wikipedia.org/wiki/Generalization_error).\n",
    "\n",
    "It is this *generalization error* that we want it to be as low as possible for *unseen data* (implies that we can achieve higher accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Dataset\n",
    "For our task, we will be using the *Wine Quality* dataset and predict the quality of white wine based on 11 features such as acidity, citric acid content, residual sugar etc. ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.0              0.27         0.36            20.7      0.045   \n",
      "1            6.3              0.30         0.34             1.6      0.049   \n",
      "2            8.1              0.28         0.40             6.9      0.050   \n",
      "3            7.2              0.23         0.32             8.5      0.058   \n",
      "4            7.2              0.23         0.32             8.5      0.058   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
      "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
      "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
      "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
      "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      8.8        6  \n",
      "1      9.5        6  \n",
      "2     10.1        6  \n",
      "3      9.9        6  \n",
      "4      9.9        6  \n",
      "[[  7.     0.27   0.36 ...,   0.45   8.8    6.  ]\n",
      " [  6.3    0.3    0.34 ...,   0.49   9.5    6.  ]\n",
      " [  8.1    0.28   0.4  ...,   0.44  10.1    6.  ]\n",
      " ..., \n",
      " [  6.5    0.24   0.19 ...,   0.46   9.4    6.  ]\n",
      " [  5.5    0.29   0.3  ...,   0.38  12.8    7.  ]\n",
      " [  6.     0.21   0.38 ...,   0.32  11.8    6.  ]]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get data\n",
    "data_url = 'http://mlr.cs.umass.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'\n",
    "data = pd.read_csv(data_url, sep=';')\n",
    "\n",
    "# inspect data\n",
    "print(data.head())\n",
    "#print(data.shape)\n",
    "\n",
    "# data as np array\n",
    "data = data.values\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Loss function\n",
    "We will use a *regularized* form of the MSE loss function. In matrix form it can be written as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "    J(\\textbf{w}) = \\frac{1}{2} \\Vert{X\\textbf{w}-\\textbf{y}}\\Vert^{2} + \\frac{\\lambda}{2}\\Vert{\\textbf{w}}\\Vert^{2}\n",
    "\\end{equation*}\n",
    "\n",
    "It's important to note that, in the above equation, $X$, called *design matrix*, is the horizontal concatenation of shape *(batch_size, num_features)* according to the *order* of the polynomial. To make things easier, you can add the *bias* term as the first column of $X$. Take care to have the *weight* vector $\\textbf{w}$ with matching dimensions.\n",
    "\n",
    "$\\textit{Hint}$: see [Design_matrix#Multiple_regression](https://en.wikipedia.org/wiki/Design_matrix#Multiple_regression) for how $X$ with 2 features looks like for $1^{st}$ degree polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Derive the gradient (w.r.t $\\textbf{w}$) for the regularized loss function given in **3**. (1.0 point)\n",
    "\n",
    "\\begin{equation*}\n",
    "    J(\\textbf{w}) = \\frac{1}{2m} ({X\\textbf{w}-\\textbf{y}})^{T}({X\\textbf{w}-\\textbf{y}}) + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}{\\textbf{w}_j}^{2}\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "\\nabla_wJ(\\textbf{w}) = X^T({X\\textbf{w}-\\textbf{y}})+ \\lambda\\textbf{w}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Matrix format for higher order polynomial\n",
    "\n",
    "Written in matrix form, linear regression model for second order would look like: <br />\n",
    "$$\\hat{\\textbf{y}} = X\\textbf{w}_{1} + X^{2}\\textbf{w}_{2} + \\textbf{b}$$\n",
    "\n",
    "where $X^{2}$ is the element-wise squaring of the original design matrix $X$, $\\textbf{w}_1$ and $\\textbf{w}_2$ are the *weight* vectors, and **b** is the *bias* vector.\n",
    "\n",
    "a) Now, please write down the matrix format for a $9^{th}$ order linear regression model (0.5 points)\n",
    "$$\\hat{\\textbf{y}} =\\textbf{b} + \\sum_{i=1}^{9} w_{i}x^{i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Hyperparameters\n",
    "we will experiment with three hyperparameters:\n",
    "\n",
    "i) regularization parameter $\\lambda$ <br />\n",
    "ii) learning rate $\\epsilon$ <br />\n",
    "iii) order of polynomial *p*\n",
    "\n",
    "And do a grid search over the values that these hyperparameters can take in order to select the best combination (i.e. the one that achieves lowest test error). This approach is called **hyperparameter optimization or tuning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_order = [1, 5, 9]\n",
    "learning_rates = [1e-5, 1e-8]\n",
    "lambdas = [0.1, 0.8]\n",
    "\n",
    "#hyperparams combination\n",
    "comb_gen = itertools.product(*(polynomial_order, learning_rates, lambdas))\n",
    "hparams_comb = list(comb_gen)\n",
    "\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Normalization\n",
    "First of all, inspect the data, and understand its structure and features. Ideally, before starting to train our learning algorithm, we would want the data to be normalized. Here, we normalize the data (i.e. normalize each column) using the formula:\n",
    "\n",
    "\\begin{equation*}\n",
    "  norm\\_x_i = \\frac{x_i - min(x)}{max(x) - min(x)}\n",
    "\\end{equation*}\n",
    "where $x_i$ is the $i^{th}$ sample in feature $x$\n",
    "\n",
    "a) Complete the following function which performs normalization (i.e. normalizes columns of $X$). (0.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.30769231  0.18627451  0.21686747 ...,  0.26744186  0.12903226  0.5       ]\n",
      " [ 0.24038462  0.21568627  0.20481928 ...,  0.31395349  0.24193548  0.5       ]\n",
      " [ 0.41346154  0.19607843  0.24096386 ...,  0.25581395  0.33870968  0.5       ]\n",
      " ..., \n",
      " [ 0.25961538  0.15686275  0.11445783 ...,  0.27906977  0.22580645  0.5       ]\n",
      " [ 0.16346154  0.20588235  0.18072289 ...,  0.18604651  0.77419355\n",
      "   0.66666667]\n",
      " [ 0.21153846  0.12745098  0.22891566 ...,  0.11627907  0.61290323  0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "def data_normalization(data):\n",
    "    data_normalized = (data - np.min(data,0)) / (np.max(data,0) - np.min(data,0))\n",
    "    return data_normalized\n",
    "\n",
    "# perform data normalization\n",
    "data_normalized = data_normalization(data)\n",
    "data_npr = data_normalized\n",
    "print (data_npr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data_npr):\n",
    "    # (in-place) shuffling of data_npr along axis 0\n",
    "    np.random.shuffle(data_npr)\n",
    "\n",
    "    n_tr = 3898\n",
    "    n_va = n_tr + 500\n",
    "    n_te = n_va + 500\n",
    "    \n",
    "    X_train = data_npr[0:n_tr, 0:-1]\n",
    "    Y_train = data_npr[0:n_tr, -1]\n",
    "    \n",
    "    X_val = data_npr[n_tr:n_va, 0:-1]\n",
    "    Y_val = data_npr[n_tr:n_va, -1]\n",
    "    \n",
    "    X_test = data_npr[n_va:, 0:-1]\n",
    "    Y_test = data_npr[n_va:, -1]\n",
    "    \n",
    "    return [(X_train, Y_train), (X_val, Y_val), (X_test, Y_test)]\n",
    "\n",
    "\n",
    "# shuffle only the training data along axis 0\n",
    "def shuffle_train_data(X_train, Y_train):\n",
    "    \"\"\"called after each epoch\"\"\"\n",
    "    perm = np.random.permutation(len(Y_train))\n",
    "    Xtr_shuf = X_train[perm]\n",
    "    Ytr_shuf = Y_train[perm]\n",
    "    \n",
    "    return Xtr_shuf, Ytr_shuf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 7. Implementation of required functions\n",
    "\n",
    "Complete the following function which computes the MSE value. (0.5 point) <br />\n",
    "(i.e. just a vanilla version of it.) That is, you can ignore the regularization term and also the constants $\\frac{1}{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.3333333333\n"
     ]
    }
   ],
   "source": [
    "def compute_mse(prediction, ground_truth):\n",
    "    # TODO: implement\n",
    "    ground_truth = np.reshape(ground_truth, (np.shape(ground_truth)[0], 1))\n",
    "    mse = np.mean(np.square(prediction - ground_truth))\n",
    "    return mse\n",
    "\n",
    "prediction = np.array([1, 3, 6])\n",
    "truth = np.array([0, 0, 0])\n",
    "mse = compute_mse(prediction, truth)\n",
    "print (mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function which computes the prediction of your model. (0.5 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.2  3.2  4.   2.8  6. ]\n"
     ]
    }
   ],
   "source": [
    "def get_prediction(X, W):\n",
    "    # TODO: implement\n",
    "    Yhat = np.matmul(X, W)\n",
    "    return Yhat\n",
    "\n",
    "dataset = [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5]]\n",
    "coef = [0.4, 0.8]\n",
    "for row in dataset:\n",
    "    yhat = get_prediction(dataset, np.transpose(coef))\n",
    "    \n",
    "print (yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function which computes the gradient of your loss function. (1.0 point) <br />\n",
    "*Hint: Just implementing the gradient computed in **3.** (a)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, Y, Yhat, W, lambda_):\n",
    "    # TODO: implement\n",
    "    Y = np.reshape(Y, (np.shape(Y)[0],1))\n",
    "    X_T = np.transpose(X)\n",
    "    gradient = np.matmul(X_T,(Yhat - Y)) + lambda_ * W\n",
    "    return gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function which performs a single update step of SGD. (0.5 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: avoid in-place modification\n",
    "def sgd(gradient, lr, cur_W):\n",
    "    # TODO: implement\n",
    "    new_W = cur_W - (lr * gradient)\n",
    "    return new_W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following function which reformats your data as a design matrix. (0.5 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate X acc. to order of polynomial; likewise do it for W\n",
    "# where X is design matrix, W is the corresponding weight vector\n",
    "# [1 X X^2 X^3], [1 W1 W2 W3].T\n",
    "\n",
    "#this function is not done yet\n",
    "def prepare_data_matrix(X, W, order):\n",
    "    # TODO: implement\n",
    "    X_mat = np.ones(shape=(np.shape(X)[0],1))\n",
    "    W_vec = np.ones(1)\n",
    "    for i in range(order):\n",
    "        X_mat = np.c_[X_mat, X ** (i+1)]\n",
    "        W_vec = np.transpose(np.c_[np.transpose(W_vec), np.transpose(W)])\n",
    "    return X_mat, W_vec\n",
    "\n",
    "# X = np.array([[1, 2, 3], [4, 5, 6]], np.int32)\n",
    "# W = np.array([[1, 2, 3]], np.int32)\n",
    "# x_mat, w_vec = prepare_data_matrix(X, W, 3)\n",
    "# print (x_mat.shape)\n",
    "# print (w_vec)\n",
    "# # prediction = get_prediction(x_mat, w_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 8. Training\n",
    "Complete the code in the following cell such that it performs **mini-batch gradient descent** on the training data for all possible hyperparameter combinations. (4.0 points)\n",
    "\n",
    "Note: You can also define a function, named appropriately, which performs training. But, take care to do correct bookkeeping of hyperparameter combinations, weight vectors, and the MSE values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order: 1 , learning rate: 1e-05 , regularizer: 0.1 \n",
      "Convergence after epoch 5034 with MSE 0.017686844383519158 \n",
      "\n",
      "order: 1 , learning rate: 1e-05 , regularizer: 0.8 \n",
      "Convergence after epoch 2814 with MSE 0.01714418983754633 \n",
      "\n",
      "order: 1 , learning rate: 1e-08 , regularizer: 0.1 \n",
      "Convergence after epoch 35571 with MSE 0.3441923916322346 \n",
      "\n",
      "order: 1 , learning rate: 1e-08 , regularizer: 0.8 \n",
      "Convergence after epoch 57575 with MSE 0.31106245848506714 \n",
      "\n",
      "order: 5 , learning rate: 1e-05 , regularizer: 0.1 \n",
      "Convergence after epoch 9481 with MSE 0.020005671894088087 \n",
      "\n",
      "order: 5 , learning rate: 1e-05 , regularizer: 0.8 \n",
      "Convergence after epoch 3685 with MSE 0.017380584634099635 \n",
      "\n",
      "order: 5 , learning rate: 1e-08 , regularizer: 0.1 \n",
      "Convergence after epoch 422113 with MSE 0.4327605675239676 \n",
      "\n",
      "order: 5 , learning rate: 1e-08 , regularizer: 0.8 \n",
      "Convergence after epoch 409942 with MSE 0.319571430184832 \n",
      "\n",
      "order: 9 , learning rate: 1e-05 , regularizer: 0.1 \n",
      "Convergence after epoch 11481 with MSE 0.01907552919679825 \n",
      "\n",
      "order: 9 , learning rate: 1e-05 , regularizer: 0.8 \n",
      "Convergence after epoch 4449 with MSE 0.0173674356997743 \n",
      "\n",
      "order: 9 , learning rate: 1e-08 , regularizer: 0.1 \n",
      "Convergence after epoch 529099 with MSE 0.5537248924072721 \n",
      "\n",
      "order: 9 , learning rate: 1e-08 , regularizer: 0.8 \n",
      "Convergence after epoch 509152 with MSE 0.3786654114358816 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "splits = split_data(data_npr)\n",
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = itertools.chain(*splits)\n",
    "\n",
    "tolerance = 1e-6\n",
    "start = 1\n",
    "\n",
    "# initialize weight vector from normal distribution\n",
    "# TODO: implement\n",
    "W_init = np.array([np.random.randn(np.shape(X_train)[1])])\n",
    "W_init = np.transpose(W_init)\n",
    "\n",
    "\n",
    "# cache weights for each hyperparam combination\n",
    "# TODO: implement\n",
    "weights_hist = {}\n",
    "for order in polynomial_order:\n",
    "    for lr in learning_rates:\n",
    "        for lamb in lambdas:\n",
    "            weights_hist[(order, lr, lamb)] = W_init\n",
    "\n",
    "# keep track of MSE for each hparam combination. will be useful for plotting\n",
    "# TODO: implement\n",
    "mse_hist = {}\n",
    "for order in polynomial_order:\n",
    "    for lr in learning_rates:\n",
    "        for lamb in lambdas:\n",
    "            mse_hist[(order, lr, lamb)] = np.finfo(np.float64).max\n",
    "\n",
    "# find optimal hyperparameters\n",
    "for order in polynomial_order:\n",
    "    for lr in learning_rates:\n",
    "        for lamb in lambdas:\n",
    "            # initialize necessary stuffs\n",
    "            # TODO: implement\n",
    "            \n",
    "            # design matrix needed at this point\n",
    "            # use the function that we defined above\n",
    "            # TODO: implement\n",
    "            X_mat, W_vec = prepare_data_matrix(X_train, W_init, order)\n",
    "            Y_vec = Y_train\n",
    "\n",
    "            epochs = 1\n",
    "            # goes through multiple epochs\n",
    "            while True:\n",
    "                # good idea to shuffle the train data\n",
    "                # TODO: implement\n",
    "                X_mat, Y_vec = shuffle_train_data(X_mat, Y_vec)\n",
    "                \n",
    "                # some more initialization\n",
    "                # TODO: implement\n",
    "                bs = 0\n",
    "                nsamples = np.shape(X_mat)[0]\n",
    "#                 prediction = np.empty(Y_vec.shape)\n",
    "                # goes through 1 epoch\n",
    "                while bs < nsamples:\n",
    "                    X = X_mat[bs:bs + batch_size, ]\n",
    "                    Y = Y_vec[bs:bs + batch_size, ]\n",
    "                    predictions = get_prediction(X, W_vec)\n",
    "                    gradient = compute_gradient(X, Y, predictions, W_vec, lamb)\n",
    "                    W_vec = sgd(gradient, lr, W_vec)\n",
    "                    bs = bs + batch_size\n",
    "                    # complete code for 1 epoch\n",
    "                    # TODO: implement\n",
    "                    \n",
    "                # after each epoch\n",
    "                # get prediction for whole X_train\n",
    "                # compute the MSE\n",
    "                # might need to do bookkeeping of mse values as well\n",
    "                batch_predictions = get_prediction(X_mat, W_vec)\n",
    "                mse = compute_mse(batch_predictions, Y_vec)\n",
    "                mse_h = mse_hist[(order, lr, lamb)]\n",
    "                mse_hist[(order, lr, lamb)] = mse\n",
    "#               print(\"Convergence after epoch {} with MSE {}\".format(epochs, mse), \"\\n\")\n",
    "\n",
    "                # stopping/convergence criterion\n",
    "                # check whether diff-in-mse < tolerance\n",
    "                # TODO: implement\n",
    "                diff_in_mse = np.abs(mse_h - mse)\n",
    "                if(diff_in_mse < tolerance):\n",
    "                    weights_hist[(order, lr, lamb)] = W_vec\n",
    "                    print(\"order: {} , learning rate: {} , regularizer: {} \".format(order, lr, lamb))\n",
    "                    print(\"Convergence after epoch {} with MSE {}\".format(epochs, mse), \"\\n\")\n",
    "                    break\n",
    "                epochs += 1\n",
    "                \n",
    "                    # cache weight vector for later use\n",
    "                    # but we also need the hparam combination\n",
    "                    # TODO: implement\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following function which selects the best hyperparameter combination (i.e. the one that gives lowest MSE on **validation data**). (0.5 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1e-05, 0.8)\n"
     ]
    }
   ],
   "source": [
    "# find hparams of minimum MSE on Validation data\n",
    "def find_best_hparams(weights_hist):\n",
    "    # TODO: implement\n",
    "    mse_best = np.finfo(np.float64).max\n",
    "    for order in polynomial_order:\n",
    "        for lr in learning_rates:\n",
    "            for lamb in lambdas:\n",
    "                X_mat, W_vec = prepare_data_matrix(X_val, weights_hist[(order, lr, lamb)], order)\n",
    "                Y_vec = Y_val \n",
    "                prediction = get_prediction(X_mat, weights_hist[(order, lr, lamb)])\n",
    "                mse = compute_mse(prediction, Y_vec)\n",
    "                if(mse < mse_best):\n",
    "                    mse_best = mse\n",
    "                    best_hpm_combination = (order, lr, lamb)\n",
    "    return best_hpm_combination\n",
    "                    \n",
    "\n",
    "best_hpm_combination = find_best_hparams(weights_hist)\n",
    "print (best_hpm_combination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 9. Re-Training on Train + Validation data\n",
    "Complete the following function which does re-training on the combined training and validation data. (**1 point**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order: 1 , learning rate: 1e-05 , regularizer: 0.8 \n",
      "Convergence after epoch 2518 with MSE 0.01689573533070153 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# re-run the training on X_train + X_val combined\n",
    "# Later test it on X_test; That will be our best possible MSE on test data\n",
    "# this will be more or less the same training code as you did above\n",
    "# but, here we just have only one value for each hyperparameter.\n",
    "X_com = np.transpose(np.c_[np.transpose(X_train), np.transpose(X_val)])\n",
    "Y_train = np.reshape(Y_train, (np.shape(Y_train)[0],1))\n",
    "Y_val = np.reshape(Y_val, (np.shape(Y_val)[0],1))\n",
    "Y_com = np.transpose(np.c_[np.transpose(Y_train), np.transpose(Y_val)])\n",
    "\n",
    "\n",
    "\n",
    "X_mat, W_vec = prepare_data_matrix(X_com, W_init, order)\n",
    "Y_vec = Y_com\n",
    "\n",
    "order, lr, lamb = best_hpm_combination\n",
    "mse_list = []\n",
    "index = 0\n",
    "mse_list.append(np.finfo(np.float64).max)\n",
    "epochs = 1\n",
    "# goes through multiple epochs\n",
    "while True:\n",
    "    \n",
    "    X_mat, Y_vec = shuffle_train_data(X_mat, Y_vec)\n",
    "    bs = 0\n",
    "    nsamples = np.shape(X_mat)[0]\n",
    "    while bs < nsamples:\n",
    "        X = X_mat[bs:bs + batch_size, ]\n",
    "        Y = Y_vec[bs:bs + batch_size, ]\n",
    "        predictions = get_prediction(X, W_vec)\n",
    "        gradient = compute_gradient(X, Y, predictions, W_vec, lamb)\n",
    "        W_vec = sgd(gradient, lr, W_vec)\n",
    "        bs = bs + batch_size\n",
    "                    \n",
    "    batch_predictions = get_prediction(X_mat, W_vec)\n",
    "    mse = compute_mse(batch_predictions, Y_vec)\n",
    "    mse_h = mse_list[index]\n",
    "    index += 1\n",
    "    mse_list.append(mse)\n",
    "    diff_in_mse = np.abs(mse_h - mse)\n",
    "    if(diff_in_mse < tolerance):\n",
    "        weights_best = W_vec\n",
    "        print(\"order: {} , learning rate: {} , regularizer: {} \".format(order, lr, lamb))\n",
    "        print(\"Convergence after epoch {} with MSE {}\".format(epochs, mse), \"\\n\")\n",
    "        break\n",
    "    epochs += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10a365438>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a365470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the convergence of MSE values using matplotlib\n",
    "# i.e. #epochs on X-axis and MSE values on Y-axis\n",
    "# TODO: implement\n",
    "plot = plt.subplot(1, 1, 1)\n",
    "plot.plot(range(epochs),mse_list[1:len(mse_list)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 10. Evaluation on Test set\n",
    "Evaluate your model on test data. (1.0 point)\n",
    "\n",
    "**Please note that you should keep X_test undisturbed throughout this whole phase.** Else restart the kernel and start from beginning. The whole point of this exercise would not make sense if test data has been *seen in training*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finally!!! MSE achieved on X_test is : 0.018475\n"
     ]
    }
   ],
   "source": [
    "# finally!!!\n",
    "# test it on X_test with the Weight vector that you found above\n",
    "# this will be the generalization error of our model!!\n",
    "# TODO: implement\n",
    "X_mat, _ = prepare_data_matrix(X_test, W_init, order)\n",
    "# print(weights_best)\n",
    "predictions = get_prediction(X_mat, weights_best)\n",
    "mse_test = compute_mse(predictions, Y_test)\n",
    "print(\"Finally!!! MSE achieved on X_test is : {}\".format(round(mse_test, 6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 11. Results\n",
    "Please report the following\n",
    "\n",
    "a) MSE value on Test data. (0.5 points)\n",
    "\n",
    "MSE achieved on X_test is : 0.018475\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Which hyperparameter combination turned out to be the best? In your understanding, why do you think such a combination turned out to be the best for this task? (1.0 point)\n",
    "\n",
    "(1, 1e-05, 0.8) combination comes the best in our implementation. It seems that for the current data, a simple linear function is enough to capture the relation between the attributes and the classes. If the correlation between the weights and the prediction is not complex enough, using a polynomial function would suffer from overfitting since the mse would be too specific to the training data. Additionally $\\lambda$ is at the biggest value that means the weights are forced  to be smaller. We vary the amount of weight decay ($\\lambda$) to prevent these high-degree models from overfitting. Again the task is not complex so it is not required to have big weigths overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus (2 points)\n",
    "\n",
    "Now, please repeat the whole *training, validation, re-training, and testing* procedure that we talked about above with the following hyperparameter combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_order = [1]\n",
    "learning_rates = [0.1]\n",
    "lambdas = [0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are your observations during the training phase? Please explain why such a behaviour happened."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "You should provide a single Jupyter notebook as the solution. The naming should include the assignment number and matriculation IDs of all members in your team in the following format:\n",
    "**assignment-4_matriculation1_matriculation2_matriculation3.ipynb** (in case of 3 members in a team). \n",
    "Make sure to keep the order matriculation1_matriculation2_matriculation3 the same for all assignments.\n",
    "\n",
    "Please submit the solution to your tutor (with **[NNIA][assignment-4]** in email subject):\n",
    "1. Maksym Andriushchenko <s8mmandr@stud.uni-saarland.de>\n",
    "2. Marius Mosbach <s9msmosb@stud.uni-saarland.de>\n",
    "3. Rajarshi Biswas <rbisw17@gmail.com>\n",
    "4. Marimuthu Kalimuthu <s8makali@stud.uni-saarland.de>\n",
    "\n",
    "Note: **If you are in a team, please submit only 1 solution to only 1 tutor.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
