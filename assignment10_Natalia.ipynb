{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 10: CNN part 2 (deadline: 20 Jan, 23:59)\n",
    "\n",
    "## 1. Implementing CNN (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you will implement a simple, but efficient Convolutional Neural Network (CNN) in TensorFlow for image classification task on CIFAR-10 dataset ([link and description](https://www.cs.toronto.edu/~kriz/cifar.html), but please note that the section \"Baseline results\" is completely outdated). \n",
    "\n",
    "We strongly recommend you first to go through a very good [tutorial from TensorFlow](https://www.tensorflow.org/get_started/mnist/pros). You can borrow implementation details from them, especially for convolutions and pooling layers, but make sure to implement our architecture! If you just apply the architecture from that tutorial to CIFAR-10 dataset, you will not receive many points. And please note, that this tutorial considers MNIST dataset, which has grayscale digits. However, CIFAR-10 has images in RGB. There is also another [tutorial by TensorFlow](https://www.tensorflow.org/tutorials/deep_cnn), which shows AlexNet architecture (now outdated!) applied to CIFAR-10. But it is much more advanced than the first one.\n",
    "\n",
    "Also you may find useful to check these excellent materials http://cs231n.github.io/convolutional-networks/ and http://cs231n.github.io/understanding-cnn/ from Stanford University for general explanations about CNNs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically for this exercise we decided to invent a slightly new CNN architecture, which combines some ideas from recent research papers. Here it is:\n",
    "\n",
    "| Layer | Number of units |\n",
    "|-------| ---|\n",
    "| input layer   | 32x32x3 units |\n",
    "| 5x5 conv+ReLU, stride 1, 10 filters| 32x32x10 units |\n",
    "| 3x3 max pool, stride 2 | 16x16x10 units |\n",
    "| 4x4 conv+ReLU, stride 1, 20 filters | 16x16x20 units | \n",
    "| 3x3 max pool, stride 2 | 8x8x20 units |\n",
    "| 3x3 conv+ReLU, stride 1, 30 filters | 8x8x30 units | \n",
    "| global average pooling | 30 units |\n",
    "| fully-connected | 10 units (classes) | \n",
    "| softmax | 10 units (classes) |\n",
    "| cross-entropy loss with L2 regularization | 1 unit (objective) |\n",
    "\n",
    "**Our main expectation for this task is \"good\" test error (around 35-40% after 10 epochs) and correct implementation of the given architecture.** Of course, it is very far away from the state-of-the-art for this dataset, which is around 3%. But unfortunately, really deep CNNs cannot be trained on ordinary laptops.\n",
    "\n",
    "\n",
    "Implementation details:\n",
    "- Initialize your weights with He initialization (for your interest, [original paper](https://arxiv.org/pdf/1502.01852.pdf)), which is simply a normal distribution with mean=0 and std=sqrt(2/n), where n is the number of units on the previous layer (e.g. for 1st conv. layer we have $n=32 \\cdot 32 \\cdot 3$, for 2nd conv. layer $n=16 \\cdot 16 \\cdot 10$).\n",
    "- Convolution should preserve dimensions (use 'SAME'). \n",
    "- Global average pooling means that we simply average all our values inside each feature map. So on the 3-rd convolutional layer we have 30 feature maps of size 8x8. After global average pooling we get only 30 values of averaged 8x8 feature maps.\n",
    "- Use ReLU as activaton function\n",
    "- We use softmax + cross-entropy loss, and no elementwise sigmoids. Make sure that you use average cross-entropy per batch in order to make an update! Otherwise recommended lambda and learning rate will not be a good choice.\n",
    "- For optimization you can use Adam with the learning rate given above (0.005), and all other parameters set by default. \n",
    "- Please train the model for 10 epochs with batch size = 100. Please note, that 10 epochs is not actually a lot, and we can get substantial improvement if we go beyond 10 epochs. We just want to be sure that 10 epochs can be done in a reasonable amount of time on any kind of laptop that you may have.\n",
    "- Use TensorBoard to **display the computation graph** defined by your model. Additonally, use TensorBoard to **log the test error, training error and training loss function** of your network for each epoch (again, you can use [here](https://www.tensorflow.org/get_started/summaries_and_tensorboard) for reference). To speed up the training process, it's not necessary to report training accuracy on all 50000 examples, you can just take first 1000 examples.\n",
    "- Don't forget about L2 regularization of all weights (both convolutional and fully connected). Recommended $\\lambda$ is 0.0001.\n",
    "- Of course, feel free to use different hyperparameters if you find better ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code to load and standardize features of CIFAR-10 dataset. So the simplest version is to load CIFAR-10 using [Tensorpack](https://github.com/ppwwyyxx/tensorpack), which you have to install first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[0120 15:50:10 @fs.py:89]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Env var $TENSORPACK_DATASET not set, using /home/tatjana/tensorpack_data for datasets.\n",
      "\u001b[32m[0120 15:50:10 @cifar.py:33]\u001b[0m Found cifar10 data in /home/tatjana/tensorpack_data/cifar10_data.\n",
      "\u001b[32m[0120 15:50:12 @cifar.py:33]\u001b[0m Found cifar10 data in /home/tatjana/tensorpack_data/cifar10_data.\n"
     ]
    }
   ],
   "source": [
    "import tensorpack.dataflow.dataset as dataset\n",
    "import numpy as np\n",
    "train, test = dataset.Cifar10('train'), dataset.Cifar10('test')\n",
    "\n",
    "# useful to reduce this number to 1000 for debugging purposes\n",
    "n = 50000\n",
    "x_train = np.array([train.data[i][0] for i in range(n)], dtype=np.float32)\n",
    "y_train = np.array([train.data[i][1] for i in range(n)], dtype=np.int32)\n",
    "x_test = np.array([ex[0] for ex in test.data], dtype=np.float32)\n",
    "y_test = np.array([ex[1] for ex in test.data], dtype=np.int32)\n",
    "\n",
    "del(train, test)  # frees approximately 180 MB\n",
    "\n",
    "# standardization\n",
    "x_train_pixel_mean = x_train.mean(axis=0)  # per-pixel mean\n",
    "x_train_pixel_std = x_train.std(axis=0)   # per-pixel std\n",
    "x_train -= x_train_pixel_mean\n",
    "x_train /= x_train_pixel_std\n",
    "x_test -= x_train_pixel_mean\n",
    "x_test /= x_train_pixel_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that you do not have a validation set here, only training and test sets. Of course, a more principled way is to select your hyperparameters based on the validation set and then retrain the model on training and validation sets combined and then report the final performance on the test set. But it would be too complicated for this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hint1: it should be possible to train this CNN on an average laptop (for reference, let's take the processor Intel® Core™ i5-3210M CPU @ 2.50GHz × 4) in 12 minutes and using up to 1 GB RAM (only the dataset needs 60000 images * 32 pixels * 32 pixels * 3 colors * 4 bytes $\\approx$ 737 MB after standardization which necessarily converts uint8 to float32). The final test error after 10 epochs should be around 35-40%. For the reference, you can see the optimization log of the correct solution (but of course, you will have random initialization and random shuffling of batches, which makes exactly these numbers not reproducible):*\n",
    "\n",
    "| Epoch | Test error | Train error |\n",
    "| --- | -- |\n",
    "| 1  | 49.300%  | 50.200% |\n",
    "| 2  | 44.310%  | 45.000% |\n",
    "| 3  | 43.110%  | 40.700% |\n",
    "| 4  | 39.550%  | 38.700% |\n",
    "| 5  | 39.820%  | 39.900% |\n",
    "| 6  | 38.480%  | 37.000% |\n",
    "| 7  | 36.180%  | 35.100% |\n",
    "| 8  | 36.810%  | 37.600% |\n",
    "| 9  | 35.340%  | 34.800% |\n",
    "| 10 | 35.210%  | 34.900% |\n",
    "\n",
    "*Hint2: you may find `tf.nn.sparse_softmax_cross_entropy_with_logits` useful if you don't want to convert the labels into 1-hot encoding.*\n",
    "\n",
    "*Hint3: you should shuffle batches on each iteration, so you can make use of this function:*\n",
    "```\n",
    "def get_next_batch(X, Y, batch_size):\n",
    "    n_batches = len(X) // batch_size\n",
    "    rand_idx = np.random.permutation(len(X))[:n_batches * batch_size]\n",
    "    for batch_idx in rand_idx.reshape([n_batches, batch_size]):\n",
    "        batch_x, batch_y = X[batch_idx], Y[batch_idx]\n",
    "        yield batch_x, batch_y\n",
    "```\n",
    "        \n",
    "*Hint4: if you have limited amount of RAM on your laptop (if you don't have any problems with memory, then it's not necessary), you can evaluate test and training errors also by batches and then combine them into the final test/train error. For example like this:*\n",
    "```\n",
    "def eval_error(X_np, Y_np, sess, batch_size):\n",
    "    \"\"\"Get all predictions for a dataset by running it in small batches.\"\"\"\n",
    "    n_batches = len(X_np) // batch_size\n",
    "    err = 0.0\n",
    "    for batch_x, batch_y in get_next_batch(X_np, Y_np, batch_size):\n",
    "        err += sess.run(error_rate, feed_dict={x: batch_x, y: batch_y})\n",
    "    return err / n_batches\n",
    "```\n",
    "\n",
    "*Hint5: you can implement the weight decay in the following way (if you named all your weights variable with the name that contains some `var_pattern`):*\n",
    "```\n",
    "def weight_decay(var_pattern):\n",
    "    \"\"\"\n",
    "    L2 weight decay loss, based on all weights that have var_pattern in their name\n",
    "\n",
    "    var_pattern - a substring of a name of weights variables that we want to use in Weight Decay.\n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    for var in tf.trainable_variables():\n",
    "        if var.op.name.find(var_pattern) != -1:\n",
    "            costs.append(tf.nn.l2_loss(var))\n",
    "    return tf.add_n(costs)\n",
    "```\n",
    "\n",
    "*Hint6: it is a very good idea to debug your code only on a subset of CIFAR-10 (let's say, 1000 examples), so you can quickly see if things go wrong.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 100\n",
    "height = 32\n",
    "width = 32\n",
    "depth = 3\n",
    "\n",
    "# placeholders for train data and variables for weights and biases\n",
    "x = tf.placeholder(tf.float32, (None, height, width, depth), name=\"x\")\n",
    "y = tf.placeholder(tf.float32, (None,), name=\"y\")\n",
    "\n",
    "# weights and biases\n",
    "\n",
    "def conv2d(x_in, W):\n",
    "    return tf.nn.conv2d(x_in, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "w_1 = tf.Variable(tf.random_normal([5,5,3,10], mean=0,stddev=tf.sqrt(2/3072)), name=\"w_1\")\n",
    "w_2 = tf.Variable(tf.random_normal([4,4,10,20], mean=0,stddev=tf.sqrt(2/2560)), name=\"w_2\")\n",
    "w_3 = tf.Variable(tf.random_normal([3,3,20,30], mean=0,stddev=tf.sqrt(2/1280)), name=\"w_3\")\n",
    "w_4 = tf.Variable(tf.random_normal([30,10], mean=0,stddev=tf.sqrt(2/30)), name=\"w_4\")\n",
    "\n",
    "bias_1 = tf.Variable(tf.zeros([10], tf.float32), name=\"bias_1\")\n",
    "bias_2 = tf.Variable(tf.zeros([20], tf.float32), name=\"bias_2\")\n",
    "bias_3 = tf.Variable(tf.zeros([30], tf.float32), name=\"bias_3\")\n",
    "bias_4 = tf.Variable(tf.zeros([10], tf.float32), name=\"bias_4\")\n",
    "\n",
    "weights = [w_1, w_2, w_3, w_4]\n",
    "biases = [bias_1, bias_2, bias_3, bias_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_batch(X, Y, batch_size):\n",
    "    n_batches = len(X) // batch_size\n",
    "    rand_idx = np.random.permutation(len(X))[:n_batches * batch_size]\n",
    "    for batch_idx in rand_idx.reshape([n_batches, batch_size]):\n",
    "        batch_x, batch_y = X[batch_idx], Y[batch_idx]\n",
    "        yield batch_x, batch_y\n",
    "        \n",
    "def weight_decay(var_pattern):\n",
    "    \"\"\"\n",
    "    L2 weight decay loss, based on all weights that have var_pattern in their name\n",
    "\n",
    "    var_pattern - a substring of a name of weights variables that we want to use in Weight Decay.\n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    for var in tf.trainable_variables():\n",
    "        if var.op.name.find(var_pattern) != -1:\n",
    "            costs.append(tf.nn.l2_loss(var))\n",
    "    return tf.add_n(costs)\n",
    "\n",
    "def forward(x, weights, biases):\n",
    "    conv_1 = tf.nn.relu(conv2d(x, weights[0]) + biases[0])\n",
    "    pool_1 = tf.nn.max_pool(conv_1, [1, 3, 3, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    conv_2 = tf.nn.relu(conv2d(pool_1, weights[1]) + biases[1])\n",
    "    pool_2 = tf.nn.max_pool(conv_2, [1, 3, 3, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    conv_3 = tf.nn.relu(conv2d(pool_2, weights[2]) + biases[2])\n",
    "    avg_pool = tf.nn.avg_pool(conv_3, [1, 8, 8, 1], [1, 1, 1, 1], padding='VALID')  \n",
    "    pool_flat = tf.reshape(avg_pool, [-1, 1 * 1 * 30])\n",
    "    logits = tf.layers.dense(inputs=pool_flat, units=10)\n",
    "    return(logits, tf.nn.softmax(logits))\n",
    "    \n",
    "def compute_cost(unscaled_logits, y, weights, alpha):\n",
    "    l2 = tf.reduce_sum(weight_decay(\"w\"))\n",
    "    y = tf.one_hot(indices=tf.cast(y, tf.int32), depth=10)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=unscaled_logits))\n",
    "    loss = tf.add(loss, alpha*l2)\n",
    "    return loss\n",
    "\n",
    "def compute_accuracy(predictions, ground_truth):\n",
    "    \"\"\"\n",
    "    compute & return accuracy (in percentage).\n",
    "    predictions: predictions from the network\n",
    "    ground_truth: 1-hot array of ground truth labels\n",
    "    \"\"\"\n",
    "    ground_truth = ground_truth.reshape(-1).astype(int)\n",
    "    ground_truth = np.eye(10)[ground_truth]        \n",
    "    return np.sum([np.argmax(ground_truth[x]) != np.argmax(predictions[x]) for x in range(len(predictions))]) / predictions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train error: 65.382 Test error: 55.73\n",
      "Epoch: 2 Train error: 51.326 Test error: 49.78\n",
      "Epoch: 3 Train error: 47.174 Test error: 46.99\n",
      "Epoch: 4 Train error: 44.114 Test error: 42.13\n",
      "Epoch: 5 Train error: 42.798 Test error: 43.94\n",
      "Epoch: 6 Train error: 41.486 Test error: 41.72\n",
      "Epoch: 7 Train error: 40.604 Test error: 41.64\n",
      "Epoch: 8 Train error: 39.704 Test error: 39.96\n",
      "Epoch: 9 Train error: 39.352 Test error: 38.84\n",
      "Epoch: 10 Train error: 38.674 Test error: 38.35\n",
      "Epoch: 11 Train error: 38.432 Test error: 38.04\n",
      "Epoch: 12 Train error: 38.17 Test error: 37.63\n",
      "Epoch: 13 Train error: 37.316 Test error: 37.32\n",
      "Epoch: 14 Train error: 37.424 Test error: 37.55\n",
      "Epoch: 15 Train error: 37.074 Test error: 37.61\n",
      "Epoch: 16 Train error: 36.752 Test error: 36.89\n",
      "Epoch: 17 Train error: 36.28 Test error: 36.29\n",
      "Epoch: 18 Train error: 36.054 Test error: 36.11\n",
      "Epoch: 19 Train error: 35.966 Test error: 36.02\n",
      "Epoch: 20 Train error: 35.968 Test error: 36.22\n",
      "Epoch: 21 Train error: 35.824 Test error: 35.52\n",
      "Epoch: 22 Train error: 35.388 Test error: 37.86\n",
      "Epoch: 23 Train error: 35.45 Test error: 36.9\n",
      "Epoch: 24 Train error: 35.05 Test error: 35.99\n",
      "Epoch: 25 Train error: 35.102 Test error: 35.16\n",
      "Epoch: 26 Train error: 34.976 Test error: 35.75\n",
      "Epoch: 27 Train error: 34.354 Test error: 35.92\n",
      "Epoch: 28 Train error: 34.536 Test error: 36.32\n",
      "Epoch: 29 Train error: 34.506 Test error: 36.94\n",
      "Epoch: 30 Train error: 34.294 Test error: 36.43\n",
      "Epoch: 31 Train error: 34.138 Test error: 35.77\n",
      "Epoch: 32 Train error: 34.37 Test error: 35.37\n",
      "Epoch: 33 Train error: 34.484 Test error: 37.1\n",
      "Epoch: 34 Train error: 34.122 Test error: 34.85\n",
      "Epoch: 35 Train error: 33.956 Test error: 35.25\n",
      "Epoch: 36 Train error: 33.65 Test error: 35.96\n"
     ]
    }
   ],
   "source": [
    "# construct model, define loss and optimizer\n",
    "unscaled_logits, predictions = forward(x, weights, biases)\n",
    "cost = compute_cost(unscaled_logits, y, weights, 0.0001)\n",
    "optimizer = tf.train.AdamOptimizer(0.005).minimize(cost)\n",
    "\n",
    "# perform training\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for epoch in range(int(n/batch_size)):\n",
    "        err = 0\n",
    "        nb = 0\n",
    "        for x_batch, y_batch in get_next_batch(x_train, y_train, batch_size):\n",
    "            nb += 1\n",
    "            _, pred, tar = sess.run([optimizer, predictions, y], feed_dict={x: x_batch, y: y_batch})\n",
    "            err += compute_accuracy(pred, tar)\n",
    "        tr_err = (err/nb)*batch_size\n",
    "        \n",
    "        _, test_pred, test_tar = sess.run([optimizer, predictions, y], feed_dict={x: x_test, y: y_test})\n",
    "        test_accuracy = compute_accuracy(test_pred, test_tar)\n",
    "        print(\"Epoch:\",str(epoch+1), \"Train error:\", tr_err, \"Test error:\", str(test_accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding CNN (5 points + 3 bonus points)\n",
    "** You should do all these tasks using TensorBoard! It is much more convenient than trying to achieve the same in numpy+matplotlib. To get visualizations all you need is to use tf.summary.image() with correct tensor, which is properly reshaped if it's needed.**\n",
    "\n",
    "a. Now we will try to understand what CNNs actually learned. Let's do the following:\n",
    "- Visualize all convolutional filters on all the layers. So you have to visualize 10 conv. filters (5x5) on the 1st layer, 20 conv. filters (4x4) on the 2nd layer and 30 conv. filters (3x3) on the 3-rd layer. Note, that each filter on the 1st layer has 3 input channels, which correspond to red, green and blue colors, so you can display them **in color**. And each filter on other layers has more input channels that have nothing to do with color. So you can just pick 1 of the input channels and display all conv. filters (again, 20 on 2nd layer and 30 on 3rd layer) **in grayscale** that correspond to a particular input channel. Please, provide here a few received images that you find interesting. (1 point)\n",
    "- Visualize all feature maps (of size 32x32, 16x16, 8x8), i.e. values of neurons after applying activation function (and before the pooling operation) for each convolutional layer for any 5 training examples. Please, provide here a few received images that you find interesting. (1 point)\n",
    "- Can you notice some interesting patterns on the visualizations? Are all images perfectly interpretable? Discuss both images of convolutional filters and of feature maps on different layers. Are there duplicate filters that look almost identical? (1 point)\n",
    "\n",
    "b. And now about the optimization process:\n",
    "- Modify your code to plot the L2 norm of flattened gradients of the weights of each layer after each epoch. So your gradient for a convolutional layer is a tensor of dimension n_in x width x height x n_out (obtained by Optimizer.compute_gradients(loss)), so you should flatten it into a long vector and then take L2 norm of it. Please, also include the gradient magnitude immediately after random initialization. Make sure to calculate gradient magnitudes on the training set (or again, you can use only a subset of 1000 examples). (2 points)\n",
    "- (BONUS, 3 points) Can you explain why the gradient magnitude doesn't always decrease over time? Can you confirm the picture from Deep Learning book on the page 281, figure 8.1, where the gradient magnitude strictly grows with training? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Small questions about CNNs (5 points)\n",
    "**Please, make sure to provide enough arguments and explanations to your answers.**\n",
    "\n",
    "\n",
    "a. How many parameters do you have in total? In which layer you have the most of them? Is there any redundancy? (1 point)\n",
    "\n",
    "On the first convolutional layer we have 10 filters with a size of 5x5 and stride=1. The input dimension is 3:\n",
    "\n",
    "conv1_params = $(5*5*3+1)*10 = 750$ (for weights and biases)\n",
    "\n",
    "The second convolutional layer has 20 filters with a size of 4x4:\n",
    "\n",
    "conv2_params = $(4*4*10+1)*20 = 3220$\n",
    "\n",
    "On the third convolutional layer we find 30 3x3 filters:\n",
    "\n",
    "conv3_params = $(3*3*20+1)*30 = 5430$\n",
    "\n",
    "Finally, we have a fully connected layer:\n",
    "\n",
    "fc_params = $(30+1)*10 = 310$\n",
    "\n",
    "We get 9720 parameters in total.\n",
    "\n",
    "The third convolutional layer has most of the parameters (5430). Since we use parameter sharing among spatial positions we get optimal number of parameters at each layer.  \n",
    "\n",
    "b. Is it possible to represent a 3x3 convolution operation (for simplicity, let’s assume that we have a single color channel) as a matrix multiplication? If yes, state the form of the matrix. If no, explain why. (2 point)\n",
    "\n",
    "Yes, 3x3 convolution operation can be represented as a matrix multiplication which has a special form. For 3x3 matrices, convolution can be described as a process of flipping both the rows and columns of the kernel and then multiplying entries at corresponding positions and taking sum over them.\n",
    "\n",
    "$\n",
    " \\begin{bmatrix}\n",
    "  k_{1,1} & k_{1,2} & k_{1,3} \\\\\n",
    "  k_{2,1} & k_{2,2} & k_{2,3} \\\\\n",
    "  k_{3,1} & k_{3,2} & k_{3,3} \n",
    "  \\end{bmatrix}\n",
    "  \\cdot\n",
    "   \\begin{bmatrix}\n",
    "  i_{1,1} & i_{1,2} & i_{1,3} \\\\\n",
    "  i_{2,1} & i_{2,2} & i_{2,3} \\\\\n",
    "  i_{3,1} & i_{3,2} & i_{3,3} \n",
    "  \\end{bmatrix}\n",
    "  =\n",
    "  \\begin{bmatrix}\n",
    "  k_{3,3} & k_{3,2} & k_{3,1} \\\\\n",
    "  k_{2,3} & k_{2,2} & k_{2,1} \\\\\n",
    "  k_{1,3} & k_{1,2} & k_{1,1} \n",
    "  \\end{bmatrix}\n",
    "  \\cdot\n",
    "   \\begin{bmatrix}\n",
    "  i_{1,1} & i_{1,2} & i_{1,3} \\\\\n",
    "  i_{2,1} & i_{2,2} & i_{2,3} \\\\\n",
    "  i_{3,1} & i_{3,2} & i_{3,3} \n",
    "  \\end{bmatrix}\n",
    "$\n",
    "\n",
    "\n",
    "E.g. for the central entry (position [2,2]) we get:\n",
    "$\n",
    "k_{3,3}*i_{1,1}+k_{3,2}*i_{1,2}+k_{3,1}*i_{1,3}+k_{2,3}*i_{2,1}+k_{2,2}*i_{2,2}+k_{2,1}*i_{2,3}+k_{1,3}*i_{3,1}+k_{1,2}*i_{3,2}+k_{1,1}*i_{3,3}\n",
    "$\n",
    "\n",
    "Therefore, the element at position [2, 2] can be represented as a combination of weighted entries of the original image matrix.\n",
    "\n",
    "\n",
    "c. Applying dropout to convolutional layers may improve your test performance (for you interest, e.g. here [Wide Residual Networks](https://arxiv.org/pdf/1605.07146.pdf)). How could you explain this? (1 point)\n",
    "\n",
    "Dropout deletes a random sample of the activations in the training phase and helps to prevent overfitting by eliminating the co-adaptation of units. It regularizes the network by adding some noise to the output feature maps on each layer. This allows to achieve better performance because the network becomes more robust to the variation of input features.\n",
    "\n",
    "However, since dropout causes the loss of information it's usually a good practice to start with small values in the first layer and then gradually increase them. \n",
    "\n",
    "d. What is the role of Global Average Pooling that you already implemented in the task 1 (if you are curious, it was proposed in [Network in Network](https://arxiv.org/pdf/1312.4400.pdf))? What is the advantage of using it instead of concatenating all feature maps from the last convolutional layer and using a fully-connected layer on top of them? (1 point)\n",
    "\n",
    "Global Average Pooling is a structural regularizer and it treats feature maps as confidence maps for given categories. Instead of adding fully connected layers on top of the feature maps, we can take the average of each feature map, and pass the result directly to the activation function (e.g. softmax).  This helps to emphasize correspondences between feature maps and categories. Our feature maps can be interpreted as confidence maps of the categories.\n",
    "\n",
    "Furthermore, we don't need to optimize any parameters on the pooling layer and this can help us with overfitting. We also sum out the spatial information with Global Average Pooling. Therefore, neural network becomes more robust to the spatial transformations of the input images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus task (1 place: 15 pts, 2 place: 10 pts, 3 place: 5 pts)\n",
    "Now you are free to implement any CNN architecture in order to get **as low test error rate as possible** on CIFAR-10. You can train the model for arbitrary number of epochs, it can have any number of layers and neurons, you can use different data preprocessing techniques, data augmentation, pooling, dropout, model averaging and much more. \n",
    "\n",
    "You also can use research papers for inspiration. For example, here is a [collection](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html) of best published results, however it is slightly outdated.\n",
    "\n",
    "#### Main rules:\n",
    "- Your final test error should be **reproducible**.\n",
    "- You must describe your main architecture decisions and why they make sense for the given task.\n",
    "- You are not allowed to copy and paste CNN architectures from the internet. It should be self-written TF code. We will check this.\n",
    "- Obviously, you are not allowed to train on test data.\n",
    "- If you want to take part in the competition, you should submit a **separate python file** which does the training of your model and that outputs test errors for each epoch (evaluated on all 10000 test examples). It will be more convenient format for us to reproduce your results.\n",
    "\n",
    "*Note: of course, people who have an access to more computational resources have an advantage in this task. However, even if you have an old laptop, you still can perform well and test many models. For example, you can train and evaluate your model only on a subset of the dataset (e.g. 1000 examples), which significantly speeds up the process. You can leave expensive hyperparameter tuning overnight. And of course, you are encouraged to collaborate with your teammates, which also have computational resources.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "You should provide a single Jupyter notebook as a solution. The naming should include the assignment number and matriculation IDs of all team members in the following format:\n",
    "**assignment-10_matriculation1_matriculation_2_matriculation3.ipynb** (in case of 3 team members). \n",
    "Make sure to keep the order matriculation1_matriculation_2_matriculation3 the same for all assignments.\n",
    "\n",
    "Please, submit your solution to your tutor (with **[NNIA][assignment-10]** in email subject):\n",
    "1. Maksym Andriushchenko <s8mmandr@stud.uni-saarland.de>\n",
    "2. Marius Mosbach <s9msmosb@stud.uni-saarland.de>\n",
    "3. Rajarshi Biswas <rbisw17@gmail.com>\n",
    "4. Marimuthu Kalimuthu <s8makali@stud.uni-saarland.de>\n",
    "\n",
    "**If you are in a team, please submit only 1 solution to only 1 tutor.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
