{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 9: Optimization Algorithms and Convolution Neural Networks (deadline: 12 Jan, 23:59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For theoretical tasks you are encouraged to write in $\\\\LaTeX$. Jupyter notebooks support them by default. For reference, please have a look at the examples in this short excellent guide: [Typesetting Equations](http://nbviewer.jupyter.org/github/ipython/ipython/blob/3.x/examples/Notebook/Typesetting%20Equations.ipynb)\n",
    "\n",
    "Alternatively, you can upload the solutions in the written form as images and paste them inside the cells. But if you do this, **make sure** that the images are of high quality, so that we can read them without any problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1. Visualizing Optimization Algorithms in Deep Neural Networks (6 points)\n",
    "\n",
    "**Goal:** Study applying optimization algorithms to an Error Surface.\n",
    "\n",
    "In this task, we will get familiar with various optimization methods such as *Vanilla Gradient Descent (GD), [Gradient Descent with Momentum](https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer), [RMSProp](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer) and [AdaGrad](https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer)* by implementing them in TensorFlow and *visualizing* the path (convergence) towards minima using [Matplotlib 3D/Contour plots](https://matplotlib.org/mpl_toolkits/mplot3d/tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "mpl.rcParams['figure.figsize'] = (12.0, 8.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3D Error Surface**\n",
    "\n",
    "The 3D error surface `z` that we will use for visualizing the behaviour of different optimization methods is defined as:\n",
    "\n",
    "$$ term1 = \\frac{2}{\\sqrt{(2\\pi \\sigma_{1}^{2})^{2}}} * \\exp{\\left(- \\left[ \\frac{(x-\\mu_1)^2}{(\\frac{\\sigma_{1}}{2})^2} + \\frac{(y-\\mu_1)^2}{(\\sigma_1)^2}     \\right] \\right)} $$\n",
    "\n",
    "$$ term2 = \\frac{1}{\\sqrt{(2\\pi \\sigma_{2}^{2})^{2}}} * \\exp{\\left(- \\left[ \\frac{(x-\\mu_2)^2 + (y-\\mu_2)^2}{(\\sigma_2)^2} \\right] \\right)} $$\n",
    "\n",
    "$$ term3 = \\frac{1}{20} * \\left(x^2  + xy + y^2 \\right) $$ <br>\n",
    "\n",
    "$$ z = term1 - term2 + term3 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) To make yourself comfortable working with this function and to get a good visualization of how this function looks like, implement the function (you can use $\\color{blue}{NumPy}$) below at `# TODO`. This function will be plotted using [matplotlib-3D-wireframe](https://matplotlib.org/devdocs/gallery/mplot3d/wire3d.html) using the code and parameter values provided. (**1 point**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params of our error surface `z`\n",
    "sigma_1 = 1.0\n",
    "sigma_2 = 2.0\n",
    "mu_1 = 0.5\n",
    "mu_2 = 0.0\n",
    "range_x, range_y = np.arange(-2.0, 3.0, 0.5), np.arange(-2.0, 2.0, 0.5)\n",
    "\n",
    "def func_z(X, Y):\n",
    "    \"\"\"\n",
    "    function definition of our 3D error surface\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO Implement the 3D Error Surface described above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sparse, Y_sparse = np.meshgrid(range_x, range_y)\n",
    "Z_sparse = func_z(X_sparse, Y_sparse)\n",
    "\n",
    "# Implement wireframe plot for func_z\n",
    "plt.clf()\n",
    "fig = plt.figure(figsize=(16, 7))\n",
    "ax1 = fig.add_subplot(111,projection='3d')\n",
    "\n",
    "ax1.plot_wireframe(X_sparse, Y_sparse, Z_sparse, linewidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of this task, we will be using 3D wireframe plot as the left `subplot` and a [contour plot](https://matplotlib.org/examples/pylab_examples/contour_demo.html) as the right `subplot` for visulization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x,y values for `Wireframe` plot\n",
    "x_wireframe, y_wireframe = np.arange(-2.0, 3.0, 0.5), np.arange(-2.0, 2.0, 0.5)\n",
    "\n",
    "# x,y values for `Contour` plot\n",
    "x_contour, y_contour = np.arange(-2.0, 3.0, 0.1), np.arange(-2.0, 2.0, 0.1)\n",
    "\n",
    "# Following code implements the plotting the Error Surface\n",
    "X_sparse, Y_sparse = np.meshgrid(x_wireframe, y_wireframe)\n",
    "Z_sparse = func_z(X_sparse, Y_sparse)\n",
    "\n",
    "X_dense, Y_dense = np.meshgrid(x_contour, y_contour)\n",
    "Z_dense = func_z(X_dense, Y_dense)\n",
    "\n",
    "plt.clf()\n",
    "fig = plt.figure(figsize=(16, 7))\n",
    "ax1 = fig.add_subplot(121,projection='3d')\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "ax1.plot_wireframe(X_sparse, Y_sparse, Z_sparse, linewidth=1, cmap=cm.jet, zorder=1, alpha=0.6)\n",
    "ax2.contour(X_dense, Y_dense, Z_dense, 32,  cmap=cm.jet)\n",
    "\n",
    "ax1.set_xlabel(r'$x$',fontsize=18)\n",
    "ax1.set_ylabel(r'$y$',fontsize=18)\n",
    "ax1.set_title('3D Surface', fontsize=18)\n",
    "\n",
    "ax2.contour(X_dense, Y_dense, Z_dense, 32,  cmap=cm.jet)\n",
    "ax2.autoscale(False)\n",
    "ax2.set_title('Contour plot', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Next, implement the 3D Error Surface described above using tensorflow variables defined in the `problem_3d` function below. Write your code as specified by `# TODO` **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following variables  will come in handy when implementing the error surface using tensorflow functions below\n",
    "tf_x, tf_y, tf_z, = None, None, None\n",
    "tf_reinit_x, tf_reinit_y = None, None\n",
    "session = None\n",
    "\n",
    "def problem_3d(start_x, start_y):\n",
    "    global session\n",
    "    global tf_x, tf_y, tf_z\n",
    "    global tf_reinit_x, tf_reinit_y\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    session = tf.InteractiveSession()\n",
    "\n",
    "    with tf.variable_scope('opt'):\n",
    "        tf_x = tf.get_variable('x', initializer=tf.constant(start_x, shape=None, dtype=tf.float32))\n",
    "        tf_y = tf.get_variable('y', initializer=tf.constant(start_y, shape=None, dtype=tf.float32))\n",
    "\n",
    "    with tf.variable_scope('opt', reuse=True):\n",
    "        tf_reinit_x = tf.assign(tf.get_variable('x'), start_x)\n",
    "        tf_reinit_y = tf.assign(tf.get_variable('y'), start_y)\n",
    "    \n",
    "    # TODO Implement 3D error surface using the above defined variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running and plotting Gradient Descent, [Gradient Descent with Momentum](https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer), [RMSProp](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer) and [AdaGrad](https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer)**\n",
    "\n",
    "c) Fill code at `# TODO` to call the named optimizers below. The following code will plot the path taken by the minimization process from the starting point to minima. The plot will display this information in a single plot of 3D wireframe. **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting position\n",
    "start_x, start_y = 0.55, 0.6\n",
    "n_steps = 25\n",
    "\n",
    "# Initialize the Error Surface related variables\n",
    "problem_3d(start_x,start_y)\n",
    "\n",
    "# Write code to call GD, Momentum, RMSProp and Adagrad implementations on tf_z global variable defined by problem_3d\n",
    "\n",
    "with tf.variable_scope('gd'):\n",
    "    # Call Gradient Descent Optimizer with learning rate = 0.1\n",
    "    tf_optimize_z = # TODO\n",
    "with tf.variable_scope('momentum'):\n",
    "    # Call Gradient Descent with Nestrov's Momentum Optimizer with learning rate = 0.1 and momentum = 0.9\n",
    "    tf_mom_optimize_z = # TODO\n",
    "with tf.variable_scope('rmsprop'):\n",
    "    # Call RMSProp with learning rate = 0.1\n",
    "    tf_rms_optimize_z = # TODO\n",
    "with tf.variable_scope('adagrad'):\n",
    "    # Call Adagrad Optimizer with learning rate = 0.1\n",
    "    tf_ada_optimize_z = # TODO\n",
    "\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "# Run vanilla GD on Error Surface\n",
    "session.run([tf_reinit_x, tf_reinit_y])\n",
    "\n",
    "opt_gd_points_x, opt_gd_points_y, opt_gd_points_z = [],[],[]\n",
    "opt_gd_points_x.append(start_x)\n",
    "opt_gd_points_y.append(start_y)\n",
    "opt_gd_points_z.append(func_z(start_x,start_y))\n",
    "\n",
    "print('Vanilla GD Optimization started')\n",
    "for step in range(n_steps):\n",
    "    session.run(tf_optimize_z)\n",
    "    x, y, z = session.run([tf_x, tf_y, tf_z])    \n",
    "    opt_gd_points_x.append(x)\n",
    "    opt_gd_points_y.append(y)\n",
    "    opt_gd_points_z.append(z)\n",
    "\n",
    "\n",
    "# Run Nestrov's Momentum GD on Error Surface\n",
    "session.run([tf_reinit_x, tf_reinit_y])\n",
    "\n",
    "opt_mom_points_x, opt_mom_points_y, opt_mom_points_z = [],[],[]\n",
    "opt_mom_points_x.append(start_x)\n",
    "opt_mom_points_y.append(start_y)\n",
    "opt_mom_points_z.append(func_z(start_x,start_y))\n",
    "\n",
    "print('RMSProp Optimization started')\n",
    "for step in range(n_steps):\n",
    "    session.run(tf_rms_optimize_z)\n",
    "    x, y, z = session.run([tf_x, tf_y, tf_z])    \n",
    "    opt_rms_points_x.append(x)\n",
    "    opt_rms_points_y.append(y)\n",
    "    opt_rms_points_z.append(z)\n",
    "\n",
    "\n",
    "print(\"Nesterov's Momentum Optimization started\")\n",
    "for step in range(n_steps):\n",
    "    session.run(tf_mom_optimize_z)\n",
    "    x, y, z = session.run([tf_x, tf_y, tf_z])    \n",
    "    opt_mom_points_x.append(x)\n",
    "    opt_mom_points_y.append(y)\n",
    "    opt_mom_points_z.append(z)\n",
    "    \n",
    "# RMSProp\n",
    "session.run([tf_reinit_x, tf_reinit_y])\n",
    "\n",
    "opt_rms_points_x, opt_rms_points_y, opt_rms_points_z = [],[],[]\n",
    "opt_rms_points_x.append(start_x)\n",
    "opt_rms_points_y.append(start_y)\n",
    "opt_rms_points_z.append(func_z(start_x,start_y))\n",
    "\n",
    "\n",
    "# Run AdaGrad on Error Surface\n",
    "session.run([tf_reinit_x, tf_reinit_y])\n",
    "\n",
    "opt_ada_points_x, opt_ada_points_y, opt_ada_points_z = [],[],[]\n",
    "opt_ada_points_x.append(start_x)\n",
    "opt_ada_points_y.append(start_y)\n",
    "opt_ada_points_z.append(func_z(start_x,start_y))\n",
    "\n",
    "\n",
    "print('Adagrad Optimization started')\n",
    "for step in range(n_steps):\n",
    "    session.run(tf_ada_optimize_z)\n",
    "    x, y, z = session.run([tf_x, tf_y, tf_z])    \n",
    "    opt_ada_points_x.append(x)\n",
    "    opt_ada_points_y.append(y)\n",
    "    opt_ada_points_z.append(z)\n",
    "\n",
    "    \n",
    "range_x,range_y = np.arange(-1.0,2.0,0.2), np.arange(-2.0,2.0,0.2)\n",
    "X_lowres, Y_lowres = np.meshgrid(range_x, range_y)\n",
    "Z_lowres = func_z(X_lowres,Y_lowres)\n",
    "\n",
    "range_x,range_y = np.arange(-1.0,2.0,0.1), np.arange(-2.0,2.0,0.1)\n",
    "X_hires, Y_hires = np.meshgrid(range_x, range_y)\n",
    "Z_hires = func_z(X_hires,Y_hires)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above optimizer outputs will be plotted in a single plot using the code below showing the path from the starting point to the minima. <br>\n",
    "\n",
    "$\\color{red}{Note}$: The plot will use the following colors for different optimizers.<br>\n",
    "$\\hspace{2em}$ SGD - $\\color{blue}{blue}$<br>\n",
    "$\\hspace{2em}$ SGD with Nesterov's momentum - $\\color{yellow}{yellow}$ <br>\n",
    "$\\hspace{2em}$ RMSProp - $\\color{purple}{purple}$ <br>\n",
    "$\\hspace{2em}$ AdaGrad - $\\color{green}{green}$ <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subplots visualizing the minimization steps\n",
    "\n",
    "plt.clf()\n",
    "fig = plt.figure(figsize=(16, 7))\n",
    "\n",
    "epsilon = 0.0\n",
    "ax1 = fig.add_subplot(121,projection='3d')\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "# plot\n",
    "ax1.plot_wireframe(X_lowres, Y_lowres, Z_lowres, linewidth=1, cmap=cm.jet, zorder=1, alpha=0.6)\n",
    "ax2.contour(X_hires, Y_hires, Z_hires, 32,  cmap=cm.jet)\n",
    "ax2.autoscale(False)\n",
    "\n",
    "# vanilla GD\n",
    "for idx, (x,y,z) in enumerate(zip(opt_sgd_points_x, opt_sgd_points_y, opt_sgd_points_z)):\n",
    "    if idx != len(opt_sgd_points_x)-1:\n",
    "        ax1.scatter(x,y,z + epsilon , color='blue', alpha=(idx+10)/(n_steps+10.0), zorder=100)\n",
    "        ax2.scatter(np.asarray(x),np.asarray(y) , color='blue')\n",
    "    else:\n",
    "        ax1.scatter(x,y,z + epsilon , color='blue', alpha=(idx+10)/(n_steps+10.0), label='SGD', zorder=100)\n",
    "        ax2.scatter(x,y,z + epsilon , color='blue', label='SGD')\n",
    "\n",
    "# GD with momentum\n",
    "for idx, (x,y,z) in enumerate(zip(opt_mom_points_x, opt_mom_points_y, opt_mom_points_z)):\n",
    "    if idx != len(opt_mom_points_x)-1:\n",
    "        ax1.scatter(x,y , color='yellow', alpha=(idx+10)/(n_steps+10.0), zorder=100)\n",
    "        ax2.scatter(x,y , color='yellow', alpha=(idx+10)/(n_steps+10.0))\n",
    "    else:\n",
    "        ax1.scatter(x,y,z + epsilon , color='yellow', alpha=(idx+10)/(n_steps+10.0), label='Momentum', zorder=100)\n",
    "        ax2.scatter(x,y, color='yellow', alpha=(idx+10)/(n_steps+10.0), label='Momentum')\n",
    "\n",
    "# RMSProp\n",
    "for idx, (x,y,z) in enumerate(zip(opt_rms_points_x, opt_rms_points_y, opt_rms_points_z)):\n",
    "    if idx != len(opt_rms_points_x)-1:\n",
    "        ax1.scatter(x,y,z + epsilon , color='purple', alpha=(idx+10)/(n_steps+10.0), zorder=100)\n",
    "        ax2.scatter(x,y , color='purple', alpha=(idx+10)/(n_steps+10.0))\n",
    "    else:\n",
    "        ax1.scatter(x,y,z + epsilon , color='purple', alpha=(idx+10)/(n_steps+10.0), label='RMSProp', zorder=100)\n",
    "        ax2.scatter(x,y, color='purple', alpha=(idx+10)/(n_steps+10.0), label='RMSProp')\n",
    "        \n",
    "# AdaGrad\n",
    "for idx, (x,y,z) in enumerate(zip(opt_ada_points_x, opt_ada_points_y, opt_ada_points_z)):\n",
    "    if idx != len(opt_ada_points_x)-1:\n",
    "        ax1.scatter(x,y,z + epsilon , color='green', alpha=(idx+10)/(n_steps+10.0), zorder=100)\n",
    "        ax2.scatter(x,y , color='green', alpha=(idx+10)/(n_steps+10.0), zorder=100)\n",
    "    else:\n",
    "        ax1.scatter(x,y,z + epsilon , color='green', alpha=(idx+10)/(n_steps+10.0), label='AdaGrad', zorder=100)\n",
    "        ax2.scatter(x,y,color='green', alpha=(idx+10)/(n_steps+10.0), label='AdaGrad', zorder=100)\n",
    "\n",
    "ax1.set_xlabel(r'$x$', fontsize=18)\n",
    "ax1.set_ylabel(r'$y$', fontsize=18)\n",
    "ax1.set_title(\"Error surface 3D\", fontsize=18)\n",
    "ax2.set_title('GD vs Nesterov Momentum vs RMSProp vs AdaGrad ', fontsize=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Evaluate the function `z` at the termination points for each algorithm. Which algorithm has made better progress in minimizing `z`?. Explain. **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2. Edge Detection (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Study applying convolution to an image by choosing appropriate kernel and strides.\n",
    "\n",
    "Below is a function (**edge_detector**) to help you apply a kernel as part of a convolution operation to an image (File dog.png, available on Piazza resource's page). Using this function, you will perform an edge detection on the given image. This image is also used to perform such an operation in Chapter 9 of Deep Learning Book (Fig 9.6 available in the **DL book** :http://www.deeplearningbook.org/contents/convnets.html). \n",
    "\n",
    "a) For this assignment you will implement the same kernel (denote as `kernel1` in code) as used in Fig. 9.6, with appropriate strides variable to obtain results similar to Fig 9.6 (also slide 11 from Chapter 8). To complete this task, fill in code at `# TODO`. The image dimensions should not change by more than 1 pixel. **(1 point)**\n",
    "\n",
    "b) Then, use a new kernel (kernel2 = $\\left( \\begin{smallmatrix} 0& 1& 0\\\\ 1& -4& 1\\\\ 0& 1& 0\\end{smallmatrix} \\right)$) with appropriate strides with the same function below. Comment on any differences you see between the results obtained using `kernel1` and `kernel2`. **(1.5 points)**\n",
    "\n",
    "c) For the two kernels and the given image, calculate the number of operations performed to convolve the repsective kernel and the image (refer Fig 9.6 from **DL book**) . **(2 points)**\n",
    "\n",
    "d) Next, copy the function `edge_detector` into `edge_detector2` and then implement a max pooling operation (with strides 2) on top of the convolution operation in the `edge_dectector` function and plot edge_detection result for `kernel1`. Use a pool size of 3 while pooling only the along the y axis. **(2 points)**\n",
    "\n",
    "e) Without the max pooling layer, increase the strides for kernel1 to obtain an image size which is different from the result in d) by atmost one pixel in different dimensions. **(1.5 points)**\n",
    "\n",
    "f) Elaborate on the differences between procedures followed by d) & e). Which one would you prefer and why? **(2 points)**\n",
    "\n",
    "Note: For this assignment, you will need to install tensorflow and Pillow (do not install package named PIL) to work with the following code. Also, we will be working with a gray image converted to have one input and one output channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "img = Image.open('dog.png')\n",
    "img = img.convert('L', (0.2989, 0.5870, 0.1140, 0)) # convert to gray scale\n",
    "img = np.asarray(img, dtype='float32')\n",
    "\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "def edge_detector(img, kernel, strides):\n",
    "    with tf.Graph().as_default():\n",
    "                \n",
    "        # reshape image to conform with conv2d requirements\n",
    "        img_shape = img.shape\n",
    "        img_conv = img.reshape(1, img_shape[0], img_shape[1], 1)\n",
    "        x = tf.placeholder('float32', [1, None, None, 1])\n",
    "        k = tf.get_variable('k', initializer=tf.to_float(kernel))\n",
    "        \n",
    "        # apply convolution\n",
    "        conv = tf.nn.conv2d(x, k, strides=strides, padding='VALID')\n",
    "        \n",
    "        # Implement Max pooling layer\n",
    "        # TODO        \n",
    "        \n",
    "        init = tf.global_variables_initializer()    \n",
    "        with tf.Session() as session:\n",
    "            session.run(init)\n",
    "            conv_op = session.run(conv, feed_dict={x: img_conv})\n",
    "            plt.imshow(conv_op[0, :, :, 0], cmap='gray')\n",
    "            print (conv_op.shape)\n",
    "            plt.show()\n",
    "\n",
    "# Implement kernel1 and store respective strides to strides1\n",
    "# TODO\n",
    "\n",
    "edge_detector(img,kernel1,strides1)\n",
    "\n",
    "# Implement kernel2 and store respective strides to strides2\n",
    "# TODO\n",
    "\n",
    "edge_detector(img,kernel2,strides2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3. Convolution in Neural Networks: Theory (4 points)\n",
    "\n",
    "**Goal:** To answer the following question with True (T) and False (F) with **justification**.\n",
    "\n",
    "Following are statements that you should answer with either a True or a False. And, also provide a justification as to why you think so. To answer these questions, you will need to revisit the lecture slides and read the chapter 9 of the Deep learning book: [ConvNets](http://www.deeplearningbook.org/contents/convnets.html).\n",
    "\n",
    "a) Pooling needs to be removed for handling inputs of varying size. \\[T/F\\] **(1 point)**\n",
    "\n",
    "b) Given a multilayered convolution neural network, a cell in a second convolutional layer has the same-sized receptive field as a cell in the first convolutional layer. \\[T/F\\] **(1 point)**\n",
    "\n",
    "c) In the context of edge detection, a convolutional neural network learns features for each pixel separately. \\[T/F\\] **(1 point)**\n",
    "\n",
    "d) There is an exponential increase in kernel parameters when convolutional net's capabilities are increased to handle transformations like rotation, scaling etc. \\[T/F\\] **(1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission instructions\n",
    "You should provide a single Jupyter notebook as the solution. The naming should include the assignment number and matriculation IDs of all members in your team in the following format:\n",
    "**assignment-9_matriculation1_matriculation2_matriculation3.ipynb** (in case of 3 members in a team). \n",
    "Make sure to keep the order matriculation1_matriculation2_matriculation3 the same for all assignments.\n",
    "\n",
    "Please submit the solution to your tutor (with **[NNIA][assignment-9]** in email subject):\n",
    "1. Maksym Andriushchenko <s8mmandr@stud.uni-saarland.de>\n",
    "2. Marius Mosbach <s9msmosb@stud.uni-saarland.de>\n",
    "3. Rajarshi Biswas <rbisw17@gmail.com>\n",
    "4. Marimuthu Kalimuthu <s8makali@stud.uni-saarland.de>\n",
    "\n",
    "Note :  **If you are in a team, you should submit only 1 solution to only 1 tutor.** <br>\n",
    "$\\hspace{2em}$ **Submissions violating these rules will not be graded.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
