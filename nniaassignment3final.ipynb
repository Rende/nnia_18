{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment sheet 3: Numerical Computation and Prinicipal Component Analysis (Deadline: Nov 24, 23:59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set notebook to full width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Issues with Softmax $~$ (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture you were introduced to the softmax function which is used to generate probabilities corresponding to the output labels. Typically, the input to the softmax function is a vector of numerical values over the labels and the output is a vector(of same dimension as the input vector) of corresponding probabilities.\n",
    "**Softmax function is given by,** $~$\n",
    "$$Softmax(x)_i = \\frac{exp(x_i)}{\\sum_{j=1}^n exp(x_j)}$$\n",
    "\n",
    "**Numerical issues might occur when computing softmax functions on a computer which can perform computations\n",
    "only upto a certain precision.** [Suggested reading $-$ [chapter 4.1 of DeepLearningBook](http://www.deeplearningbook.org/contents/numerical.html)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$1$. Name these numerical issues and explain them. ($1$ points)\n",
    "\n",
    "Numerical issues might happen when we are dealing with too small or too big numbers since we need to represent infinitely many real numbers with a finite number of bit patterns.\n",
    "One issue is underflow; it occurs when numbers near zero are rounded to zero.\n",
    "The other one is overflow; it occurs when numbers with large magnitude are approximated as $\\infty$ or $-\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$2$. Suggest a remedy (with explanation on why it works) to overcome these numerical issues occuring with Softmax computation. Prove that this remedy actually does not change the softmax criteria. Describe a situation where the proposed remedy still fails to remove instability. ($1$ point)\n",
    "\n",
    "These difficulties can be resolved by instead evaluating $softmax(z)$ where $ z = x - max_ix_i$. Since we only add or subtract a scalar from the input vector, the result is not effected.\n",
    "\n",
    "However the error still can occur if we implement log softmax(x) by first running the softmax subroutine then passing the result to the log function, we could erroneously obtain $-\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$3$. First write a naive Softmax implementation, in numpy, that can produce numerical instability. Then write a modified Softmax implementation which is numerically stable.  ($0.5 + 0.5 = 1$ points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  5 10 15 20 25 30 35 40 45 50 55]\n",
      "[  1.29082491e-24   1.91575403e-22   2.84323108e-20   4.21972907e-18\n",
      "   6.26263322e-16   9.29457180e-14   1.37943676e-11   2.04726568e-09\n",
      "   3.03841167e-07   4.50940274e-05   6.69254707e-03   9.93262053e-01]\n",
      "[  1.29082491e-24   1.91575403e-22   2.84323108e-20   4.21972907e-18\n",
      "   6.26263322e-16   9.29457180e-14   1.37943676e-11   2.04726568e-09\n",
      "   3.03841167e-07   4.50940274e-05   6.69254707e-03   9.93262053e-01]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO : Define inputs\n",
    "inputs = np.arange(0,60,5)\n",
    "print (inputs)\n",
    "\n",
    "def softmax_naive(inputs):\n",
    "    exp_array = np.exp(inputs)\n",
    "    return (exp_array/(np.sum(exp_array)))\n",
    "\n",
    "def softmax_modified(inputs):\n",
    "    z = inputs - np.amax(inputs) \n",
    "    exp_array = np.exp(z)\n",
    "    return (exp_array/(np.sum(exp_array)))\n",
    "\n",
    "print (softmax_naive(inputs))\n",
    "print (softmax_modified(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis $~$ (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$4$. Is PCA supervised or unsupervised, logically explain your answer. Which is the tunable parameter in PCA?\n",
    "Briefly explain the role of this parameter in PCA.  ($1+0.5+0.5 = 2$ points)\n",
    "\n",
    " 1) Unsupervised,PCA helps in producing low dimensional representation of the dataset by identifying a set of linear combination of features which have maximum variance and are mutually un-correlated. This linear dimensionality technique could be helpful in understanding latent interaction between the variable in an unsupervised setting. In a supervised setting such as Classification or Regression, one observes both a set of input variables(X1, .. Xn ) and response or output variables (Y). However, in un-supervised setting the goal is to identify “meaningful” informative patterns in the given data. There is no corresponding output Y of each input X here in PCA. \n",
    "\n",
    " 2) Tunable parameter is what we want to reduce the dimension to,here in this Task 4, we reduce dimensions from 2 to 1. \n",
    "\n",
    " 3) The encoding formula,  $f(x)=D^Tx$, D contains the eigenvectors corresponding to the larger eigenvalues of $X^TX$, so the column of D represents the degree of matrix after reducing dimensions.In PCA, the eigendecomposition is to tune this parameter, we want to use lower dimensions of data to represents the oringinal one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$5$. Consider the following data:\n",
    "\n",
    "setA: ${\\bf x}^{(1)}$=$(2, 4)^T$, ${\\bf x}^{(2)}$=$(2, 2)^T$, ${\\bf x}^{(3)}$=$(3, 1)^T$, ${\\bf x}^{(4)}$=$(5, 1)^T$ \n",
    "\n",
    "setB: ${\\bf x}^{(1)}$=$(-1, 1)^T$, ${\\bf x}^{(2)}$=$(-2, 2)^T$, ${\\bf x}^{(3)}$=$(-1, 3)^T$, ${\\bf x}^{(4)}$=$(-1, 4)^T$\n",
    "\n",
    "$(a)$ Compress the above sets of vectors into a one-dimensional set using PCA, i.e., derive the encoder function $f(x)=D^{T}x$ as defined in the lecture. Then apply f to the datasets inorder to compress them. ($1.5 + 1.5$ points)\n",
    "\n",
    "$\\mathbf{SetA}=\\left[\\begin{array}{cccc}\n",
    "   2 & 4\\\\\n",
    "   2 & 2\\\\\n",
    "   3 & 1\\\\\n",
    "   5 & 1\\\\\n",
    "  \\end{array}\\right]$   ,    Mean of vectors in Set A: $\\mathbf{MeanA}=\\left[\\begin{array}{cccc}\n",
    "   3 & 2\\\\ \n",
    "  \\end{array}\\right]$\n",
    "  Remove mean, $\\mathbf{X}=\\left[\\begin{array}{cccc}\n",
    "   -1 & 2\\\\\n",
    "   -1 & 0\\\\\n",
    "   0 & -1\\\\\n",
    "   2 & 1\\\\\n",
    "  \\end{array}\\right]$\n",
    "  \n",
    "  $\\mathbf{x=X^T}=\\left[\\begin{array}{cccc}\n",
    "   -1 & -1 & 0 & 2\\\\\n",
    "   2 & 0 & -1 & -1\\\\\n",
    "  \\end{array}\\right]$\n",
    "  \n",
    "  \n",
    "  $$\\mathbf{X^TX}=\\left[\\begin{array}{cccc}\n",
    "   6 & -4\\\\\n",
    "  -4 &  6\\\\\n",
    "  \\end{array}\\right] $$Make Eigendecomposition of $X^TX$,\n",
    " let $|X^TX-\\lambda I|=0$\n",
    " The larger eigenvalue is:$\\lambda_1=10$\n",
    " \n",
    " From $|X^TX-10I|=0$, we have D containing the eigenvectors corresponding to the largest eigenvalues of $X^TX$,\n",
    "  $\\mathbf{D_1}=\\left[\\begin{array}{cccc}\n",
    "   \\frac{\\sqrt{2}}{2}\\\\\n",
    "  \\frac{\\sqrt{2}}{-2}\n",
    "  \\end{array}\\right]$ \n",
    "\n",
    "Now we encode the data,$\\mathbf{D_1^Tx}=\\left[\\begin{array}{cccc}\n",
    "   \\frac{-3\\sqrt{2}}{2} &\n",
    "   \\frac{\\sqrt{2}}{-2}  &\n",
    "   \\frac{\\sqrt{2}}{2}   &\n",
    "   \\frac{3\\sqrt{2}}{2}\\\\\n",
    "  \\end{array}\\right]$ \n",
    "\n",
    "As to $\\mathbf{SetB}=\\left[\\begin{array}{cccc}\n",
    "   -1 & 1\\\\\n",
    "   -2 & 2\\\\\n",
    "   -1 & 3\\\\\n",
    "   -1 & 4\\\\\n",
    "  \\end{array}\\right]$,\n",
    "following the same steps, we have $\\mathbf{Y}=\\left[\\begin{array}{cccc}\n",
    "   0.25 & -1.5\\\\\n",
    "   -0.75 & 0.5\\\\\n",
    "   0.25 & 0.5\\\\\n",
    "   0.25 & 1.5\\\\\n",
    "  \\end{array}\\right]$ , as to Y,\n",
    "the larger eigenvalue is $\\lambda_2=5.058$,\n",
    "  $\\mathbf{D_2}=\\left[\\begin{array}{cccc}\n",
    "   0.1153\\\\\n",
    "   0.9933\\\\\n",
    "  \\end{array}\\right]$ \n",
    "  Now we encode the data,$\\mathbf{D_2^Ty}=\\left[\\begin{array}{cccc}\n",
    "   -1.4611 &-0.5831 & 0.5255 & 1.5188\n",
    "   \\\\\n",
    "  \\end{array}\\right]$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(b)$. For both the above sets sketch the corresponding datasets in a separate figure. \n",
    "Also include the reconstructed vectors into the corresponding figures. ($2$ points)\n",
    "\n",
    "$ reconstructed vector_1=D_1D_1^Tx=\\left[\\begin{array}{cccc}\n",
    "   -1.5 & -.5 & 0.5 & 1.5\\\\\n",
    "   1.5 & 0.5 &-.5 & -1.5\\\\\n",
    "  \\end{array}\\right]$\n",
    "  \n",
    "$ reconstructed vector_2=D_2D_2^Ty=\\left[\\begin{array}{cccc}\n",
    "   -0.1685 & -0.0672 & 0.0606 & 0.1751\\\\\n",
    "   -1.4513 & -0.5792 & 0.5220 & 1.5083\\\\\n",
    "  \\end{array}\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent and Newton's method $~$ (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppose $f(x) = 2x^3 - 5x + 6$ **\n",
    "\n",
    "$6$. Write down the mathematical expressions for minimizing f(x) using Gradient descent(GD) and then using Newton's Method(NM). ($1$ points)\n",
    "\n",
    "- Gradient descent : $x' = x - \\epsilon \\nabla_x f(x)$ where $x'$ is the updated point, $\\nabla_x f(x)$ is the gradient of $f$ and $\\epsilon$ is the learning rate, a positive scalar determining the size of the step.\n",
    "    For the above function $\\nabla_x f(x) = 6x^2 - 5$ therefore $x' = x - \\epsilon (6x^2 - 5)$.\n",
    "- Newton's Method : $x' = x - H(f)(x)^{-1} \\nabla_x f(x)$ where $ H(f)(x) $ Hessian matrix of $f$. \n",
    "    Since we have only one variable in the function the Hessian is $ H(f)(x) = f''(x) $ therefore $x' = x - (12x)^{-1} (6x^2 -5)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$7$. Report the updated values of x, both for GD and NM, at $x = 0$. what do you observe? ($1$ points)\n",
    "\n",
    "- GD: $\\nabla_x f(x) = 6x^2 - 5$ at $x = 0$;  $\\nabla_x f(0) = - 5$ \n",
    "\n",
    "    $x' = 0 - \\epsilon \\nabla_x f(0)$ therefore the updated value $x' = 5\\epsilon $\n",
    "    assuming $\\epsilon = 0.01 $ then we have $x' = 0.05$\n",
    "    \n",
    "- NM: $ H(f)(x)_{i,j} = \\frac{\\partial^2} {\\partial x_i \\partial x_j} \\rightarrow  H(f)(x) = [12x] $ \n",
    "     \n",
    "    $x' = x - (12x)^{-1} (6x^2 -5)$ for $x = 0$ we have $x' = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$8$. Perform GD and NM for the above function using Tensorflow. ($1.5 + 1.5$ points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "fx = 2*x**3 - 5*x + 6\n",
    "dfx = tf.gradients(fx, x)[0]\n",
    "eps = 0.01\n",
    "x_upd = x - eps*dfx\n",
    "x_upd_out = sess.run(x_upd, feed_dict = {x: 0.0})\n",
    "\n",
    "print (x_upd_out)\n",
    "# TODO : Implement Gradient Descent with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "fx = 2*x**3 - 5*x + 6\n",
    "dfx = tf.gradients(fx, x)[0]\n",
    "ddfx = tf.gradients(dfx, x)[0]\n",
    "x_upd = x - dfx*(ddfx)**(-1)\n",
    "x_upd_out = sess.run(x_upd, feed_dict = {x: 0.0})\n",
    "\n",
    "print (x_upd_out)\n",
    "# TODO : Implement Newton's Method with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent computation and visualisation $~$ (3 + 2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now visualize the Gradient Descent algorithm to fit a straight line to data generated using  $y = \\theta_{true}x$ $~$, i.e., use this expression to first produce the data (see code below the lines starting with m=20 and following) and then try to fit a straight line to this data. Fitting a straight line means that you have to approximate this $\\theta_{true}$ parameter using the hypothesis or predictive model by minimizing the cost function defined below.\n",
    "\n",
    "**For this task you should minimize a cost function of the form:**\n",
    "$$\\frac{1}{2m}\\sum_{i=1}^m [h_{\\theta}(x^i)-y^i]^2$$\n",
    "where\n",
    "- $x^i$ is the $i^{th}$ input \n",
    "\n",
    "- $y^i$ is the true $i^{th}$ response or output\n",
    "\n",
    "- $h_{\\theta}(x)$ is the hypothesis or predictive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assume $~$ $h_{\\theta}(x) = \\theta x$ $~$ to be the hypothesis or predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.2 36.1052631579\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMqlTP+eIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114edd518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate the true data which is to be fitted\n",
    "m = 20                      # number of data points for x\n",
    "theta_true = 0.5            # corresponds to the true slope\n",
    "x = np.linspace(-1,1,m)     # x values or inputsm\n",
    "y = theta_true * x          # True response\n",
    "\n",
    "# Create a subplot window\n",
    "# On the left window plot the true data and the approximation that you obtain with different estimates of the slope theta_true\n",
    "# on the right window plot the cost function \n",
    "# TODO : Create the subplot window\n",
    "\n",
    "def hypothesis(x, theta):\n",
    "    \"\"\"Our \"hypothesis or predictive model\", a straight line through the origin.\"\"\"\n",
    "    return x*theta\n",
    "\n",
    "def cost_func(theta):\n",
    "    \"\"\"The cost function describing the goodness of fit.\"\"\"  \n",
    "    y_hypo = hypothesis(x, theta)\n",
    "    return (1/2*m)* np.sum((y_hypo-y)**2)\n",
    "\n",
    "\n",
    "# First construct a grid of theta parameter and their corresponding\n",
    "# cost function values.\n",
    "theta_grid = np.linspace(-0.2,1,50)\n",
    "\n",
    "cost_values = [cost_func(i) for i in theta_grid]\n",
    "J_grid = np.matrix([theta_grid, cost_values])\n",
    "\n",
    "# Find the cost function values to be stored in J_grid\n",
    "# TODO : Create J_grid\n",
    "\n",
    "\n",
    "# Plot the cost function as a function of theta.\n",
    "# TODO : Do the plot\n",
    "theta1=0.7\n",
    "y_theta1=theta1 * x \n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(x,y)\n",
    "plt.plot(x,y_theta1)\n",
    "plt.subplot(122)\n",
    "plt.plot(theta_grid, cost_values)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Take N steps with learning rate alpha down the steepest gradient,\n",
    "# starting at theta = 0.\n",
    "N = 10\n",
    "alpha = 1 \n",
    "\n",
    "#try also 0.02 to see what happens\n",
    "\n",
    "# this is just a starting value of alpha, \n",
    "# you must consider different values of alpha (try using large values)\n",
    "# and redo the steps below to generate different plots\n",
    "theta = [0]\n",
    "\n",
    "\n",
    "# TODO :Compute the N steps down the steepest gradient\n",
    "\n",
    "# TODO : Annotate the cost function plot with coloured points indicating the\n",
    "# parameters chosen and red arrows indicating the steps down the gradient.\n",
    "# Also plot the fit function on the left window of the subplot in a matching colour.\n",
    "\n",
    "# TODO : Put the labels, titles and a legend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now assume that the data is generated using  $y = \\theta_1x + \\theta_0$\n",
    "** Following the same logic you applied for the above task define a predictive model \n",
    "and perform 5 steps of gradient descent with learning rate alpha = 0.7 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the true data which is to be fitted\n",
    "m = 20\n",
    "theta0_true = 2\n",
    "theta1_true = 0.5\n",
    "x = np.linspace(-1,1,m)\n",
    "y = theta0_true + theta1_true * x\n",
    "\n",
    "# Create the sub-plot: left window is the data, right window will be the cost function.\n",
    "# TODO\n",
    "\n",
    "\n",
    "def hypothesis(x, theta0, theta1):\n",
    "    \"\"\"Our \"hypothesis function\", a straight line.\"\"\"\n",
    "    \n",
    "    # TODO : Implement\n",
    "    pass\n",
    "\n",
    "def cost_func(theta0, theta1):\n",
    "    \"\"\"The cost function, J(theta0, theta1) describing the goodness of fit.\"\"\"\n",
    "    \n",
    "    # TODO : Implement\n",
    "    pass\n",
    "\n",
    "\n",
    "# First construct a grid of (theta0, theta1) parameter pairs and their\n",
    "# corresponding cost function values.\n",
    "theta0_grid = np.linspace(-1,4,101)\n",
    "theta1_grid = np.linspace(-5,5,101)\n",
    "\n",
    "# TODO : Compute the cost function values\n",
    "\n",
    "\n",
    "# TODO : Do a labeled contour plot for the cost function on right window of the above subplot\n",
    "\n",
    "\n",
    "# TODO : Take 5 steps with learning rate alpha = 0.7 down the steepest gradient,\n",
    "# starting at (theta0, theta1) = (0, 0).\n",
    "\n",
    "\n",
    "# TODO : Annotate the cost function plot with coloured points indicating the\n",
    "# parameters chosen and red arrows indicating the steps down the gradient.\n",
    "# Also plot the fit function on the left window in a matching colour.\n",
    "\n",
    "\n",
    "# TODO : Add the labels, titles and a legend to the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Algebra Bonus\n",
    "- [Additional material - Linear Algebra Basics](http://www.cs.ubc.ca/~schmidtm/Documents/2009_Notes_LinearAlgebra.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trace of a Matrix $~$ (3 points)\n",
    "- [Reading material on Trace](https://en.wikipedia.org/wiki/Trace_(linear_algebra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prove that the trace of a ***symmetric positive definite*** matrix is the sum of its eigenvalues.    ($0.5$ points)\n",
    "\n",
    "Suppose $\\mathbf{Y}$ is a $m \\times n$ matrix with $m \\leq n$ and has ***full rank***, then\n",
    "\n",
    "$(a)$.   Give the rank of $\\mathbf{Y}$.                                                                 ($0.5$ points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(b)$.  Show that trace of $\\mathbf{Y}^{T}(\\mathbf{Y}^T\\mathbf{Y})^{-1}\\mathbf{Y}$ = rank($\\mathbf{Y}$)                                     ($1$ points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(c)$. Prove that $\\mathbf{Y}^{T}(\\mathbf{Y}^T\\mathbf{Y})^{-1}\\mathbf{Y}$ is the projection matrix w.r.t space defined by $\\mathbf{Y}$.     ($1$ points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jacobian $~$ (3 points)\n",
    "\n",
    "***[Reading material on Jacobian](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that the Jacobian determinant of $\\frac{\\partial(fg, h)}{\\partial(u, v)}$ is equal to $\\frac{\\partial(f, h)}{\\partial(u, v)}g + f\\frac{\\partial(g, h)}{\\partial(u, v)}$,\n",
    "\n",
    "where $f$,$g$, and $h$ are functions of $u$ and $v$ (i.e., $f(u,v)$, $g(u,v)$, and $h(u,v)$)   ($3$ points)\n",
    "\n",
    "Hint: Use the property $\\frac{\\partial(y, x)}{\\partial(u, v)} = \\frac{\\partial(y)}{\\partial(u)}\\frac{\\partial(x)}{\\partial(v)}-\\frac{\\partial(y)}{\\partial(v)}\\frac{\\partial(x)}{\\partial(u)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hessian $~$ (2 points)\n",
    "***[Reading material on Hessian](https://en.wikipedia.org/wiki/Hessian_matrix)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathbf{M}=\\left[\\begin{array}{cccc}\n",
    "   5 & 1 & 0 & 1\\\\\n",
    "   1 & 4 & 1 & 0\\\\\n",
    "   0 & 1 & 3 & 1\\\\\n",
    "   1 & 0 & 1 & 2\\\\\n",
    "  \\end{array}\\right]$\n",
    "  \n",
    "denote the Hessian matrix at particular point for a particular function.\n",
    "\n",
    "$(a)$. What properties of the functional can you infer from the above information.(give mathematical reasons) ($1$ point)\n",
    "\n",
    "Set $|M-\\lambda I|=0$\n",
    "The eingenvalues are $1, 3, 4 ,6 $ which are pisitive values, so M is positive defined. It means at this particular point, it is a local minimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(b)$. Provide a generic mathematical representation (e.g. the generic representation of a straight line is $ax+by+c=0$) for the above function. ($1$ point)\n",
    "$ \\frac{5}{2}x_1^2+x_1x_2+x_1x_4+2x_2^2+x_2x_3+x_3x_4+x_4^2+\\frac{3x_3^2 }{2}+c=0$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
