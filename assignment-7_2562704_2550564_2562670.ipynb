{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7: Regularization - Bagging, Early Stopping and Dropout (deadline: 22 Dec, 23:59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set notebook to full width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1. Regularization: Bagging (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Study the effects of **bagging** regularization on Decision Tree based methods against a single instance of such a classifier.\n",
    "\n",
    "Bagging, briefly mentioned in the Lecture 6, refers to an ensemble machine learning method. The Bagging scheme, suggested to be used in this exercise, samples instances from the training data with replacement and creates multiple training subsets. For each of these subsets, a new regressor is constructed internally and finally, all combined to produce the result. For more details read: \n",
    "\n",
    "1. Scikit Learn Documentation for Bagging Regressor. http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html#id6\n",
    "\n",
    "2. Bootstrap Aggregating Wikipedia Article. https://en.wikipedia.org/wiki/Bootstrap_aggregating\n",
    "\n",
    "Implement a bagging regularization scheme using ***DecisionTreeRegressor***, a Decision Tree based classifier from the python package ***sklearn.tree***. To implement the bagging scheme you can use ***BaggingRegressor*** available in the python package ***sklearn.ensemble***. Fill in the code pieces marked by \"# TODO\" in the following notebook to complete this assignment. Finally, comment on the results you obtain.\n",
    "\n",
    "Note: to run the following code you will need to download **data.csv** from the NNIA's resource page on Piazza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Estimators: Create an array of two estimators.\n",
    "# First a \"Tree\" using \"DecisionTreeRegressor\" and second a Regularized version obtain using \"BaggingRegressor\"\n",
    "# on this Tree, labelled \"Bagging (Tree)\". (2 points)\n",
    "\n",
    "estimators = [(\"Tree\", DecisionTreeRegressor(random_state=0)\n",
    "              ),\n",
    "              (\"Bagging (Tree)\", BaggingRegressor(DecisionTreeRegressor(random_state=0))\n",
    "              )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = len(estimators)\n",
    "np.random.seed(0)\n",
    "\n",
    "#Load Data\n",
    "data = pd.read_csv('/Users/aydanrende/Documents/Informatics/nnia_18/data.csv')\n",
    "\n",
    "#Drop Player Field\n",
    "data = data.drop('Player', axis=1)\n",
    "\n",
    "#Set variable y with 'Salary' column and then drop it from data\n",
    "y = data['Salary'].as_matrix()\n",
    "data = data.drop('Salary', axis=1)\n",
    "\n",
    "#Convert data to an numpy array x\n",
    "x = data.as_matrix()\n",
    "\n",
    "# Split x in X_train (80 %) and X_test (20 %),  while constructing the corresponding y_train and y_test\n",
    "n_train = np.int(0.8 * len(x) )\n",
    "n_test = len(x) - n_train\n",
    "X_train = x[:n_train,:]\n",
    "y_train = y[:n_train]\n",
    "X_test = x[n_train:,:]\n",
    "y_test = y[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree: 152196.4062 (error)\n",
      "Bagging (Tree): 55107.1808 (error)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnIAAADSCAYAAADOrS1SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c449cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Figures and report error using the different ensemble methods\n",
    "plt.figure(figsize=(10,3))\n",
    "\n",
    "# Loop over estimators to compare\n",
    "for n, (name, estimator) in enumerate(estimators):\n",
    "    \n",
    "    # Compute predictions\n",
    "    y_predict = np.zeros(n_test)\n",
    "        \n",
    "    # Train the estimator (1 point)\n",
    "    estimator.fit(X_train, y_train)\n",
    "    \n",
    "    #Predict results using the estimator on X_test (1 point)\n",
    "    y_predict = estimator.predict(X_test)\n",
    "    \n",
    "    y_error = np.zeros(n_test)\n",
    "    \n",
    "    # Compute the mean sqaured error using y_test and y_predict and store it in y_error (1 point)\n",
    "    y_error = (y_test - y_predict)**2\n",
    "    \n",
    "    print(\"{0}: {1:.4f} (error)\".format(name,np.mean(y_error)))\n",
    "    \n",
    "    # Plot the Result\n",
    "    plt.subplot(1, n_estimators, n + 1)\n",
    "    plt.plot(np.arange(n_test), y_error, \"r\", label=\"$error(x)$\");\n",
    "    plt.ylim([0, 1300000])\n",
    "    plt.legend()\n",
    "    plt.title(name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Explain the differences (in 2-3 sentences) between the plots you obtain for **Tree** and **Bagging (Tree)**. (2 points)\n",
    "\n",
    "The plot in **Tree** reflects the test error of a single decision tree estimation y_predict of y ('Salary') based on the parameters of the model. The plot in **Bagging(Tree)** reflects the test error of y_predict by a regularized by bagging model; the error is lower than this in **Tree**.\n",
    "\n",
    "Bagging is a regularization technique where multiple training sets are sampled (with replacement) from the the original training set; the classification is performed on each of these training sets (default value for BaggingRegressor is 10) and the results of these classifications is averaged. In this way the variance of the prediction is reduced which leads to more robust model with lower overall mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2. Regularization: Early Stopping (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** To study how increasing neurons of a neural network (model complexity) affects the Early Stopping threshold.\n",
    "\n",
    "Download **MNIST** dataset from the NNIA's resource page on Piazza. We first update the feedforward neural network code below (at \"#TODO\") to calculate training and validation error at every 100 iterations of the training scheme. Then using this code for a different number of neurons (50, 100 and 200), plot the variation of training and validation error for every 100th iteration up to 1000 iterations. Studying these plots answer the questions given below.\n",
    "\n",
    "Note: to calculate the validation error use the test set as a proxy validation set. Also, notice the extra arguments that need to be provided to calculate the validation error at every 100 iterations in the fit function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit\n",
    "from __future__ import division\n",
    " \n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path, \n",
    "                               '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, \n",
    "                               '%s-images-idx3-ubyte' % kind)\n",
    "        \n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', \n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, \n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", \n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, \n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    " \n",
    "    return images, labels\n",
    "\n",
    "class MLP(object):\n",
    "    \"\"\" Feedforward neural network / Multi-layer perceptron classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    n_output : int\n",
    "        Number of output units, should be equal to the\n",
    "        number of unique class labels.\n",
    "        \n",
    "    n_features : int\n",
    "        Number of features (dimensions) in the target dataset.\n",
    "        Should be equal to the number of columns in the X array.\n",
    "        \n",
    "    n_hidden : int (default: 30)\n",
    "        Number of hidden units.\n",
    "        \n",
    "    l1 : float (default: 0.0)\n",
    "        Lambda value for L1-regularization.\n",
    "        No regularization if l1=0.0 (default)\n",
    "        \n",
    "    l2 : float (default: 0.0)\n",
    "        Lambda value for L2-regularization.\n",
    "        No regularization if l2=0.0 (default)\n",
    "        \n",
    "    epochs : int (default: 500)\n",
    "        Number of passes over the training set.\n",
    "        \n",
    "    eta : float (default: 0.001)\n",
    "        Learning rate.\n",
    "        \n",
    "    alpha : float (default: 0.0)\n",
    "        Momentum constant.\n",
    "        \n",
    "    decrease_const : float (default: 0.0)\n",
    "        Decrease constant. Shrinks the learning rate\n",
    "        after each epoch via eta / (1 + epoch*decrease_const)\n",
    "        \n",
    "    shuffle : bool (default: True)\n",
    "        Shuffles training data every epoch if True to prevent circles.\n",
    "        \n",
    "    minibatches : int (default: 1)\n",
    "        Divides training data into k minibatches for efficiency.\n",
    "        Normal gradient descent learning if k=1 (default).\n",
    "        \n",
    "    random_state : int (default: None)\n",
    "        Set random state for shuffling and initializing the weights.\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    cost_ : list\n",
    "      Sum of squared errors after each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "      \n",
    "    def __init__(self, n_output, n_features, n_hidden=30,\n",
    "                 l1=0.0, l2=0.0, epochs=50, eta=0.001,\n",
    "                 alpha=0.0, decrease_const=0.0, shuffle=True,\n",
    "                 minibatches=1, random_state=None):\n",
    "\n",
    "        np.random.seed(random_state)\n",
    "        self.n_output = n_output\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.w1, self.w2 = self._initialize_weights()\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "\n",
    "    def _encode_labels(self, y, k):    \n",
    "        \"\"\"Encode the labels using one-hot representation\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        y : y represents target values.\n",
    "\n",
    "        Returns\n",
    "        -----------\n",
    "        onehot array\n",
    "\n",
    "        \"\"\"\n",
    "       \n",
    "        onehot = np.zeros((k, y.shape[0]))\n",
    "        for idx, val in enumerate(y):\n",
    "            onehot[val, idx] = 1.0\n",
    "        \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        \n",
    "        w1 = np.random.uniform(-1.0, 1.0,\n",
    "                               size=self.n_hidden*(self.n_features + 1))\n",
    "        w1 = w1.reshape(self.n_hidden, self.n_features + 1)\n",
    "        w2 = np.random.uniform(-1.0, 1.0,\n",
    "                               size=self.n_output*(self.n_hidden + 1))\n",
    "        w2 = w2.reshape(self.n_output, self.n_hidden + 1)\n",
    "        \n",
    "        return w1, w2\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Compute logistic function (sigmoid)\n",
    "\n",
    "        Uses scipy.special.expit to avoid overflow\n",
    "        error for very small input values z.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        return expit(z)\n",
    "\n",
    "    def _sigmoid_gradient(self, z):\n",
    "        \"\"\"Compute gradient of the logistic function\"\"\"\n",
    "        \n",
    "        sg = self._sigmoid(z)\n",
    "        return sg * (1.0 - sg)\n",
    "\n",
    "    def _add_bias_unit(self, X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        \n",
    "        if how == 'column':\n",
    "            X_new = np.ones((X.shape[0], X.shape[1] + 1))\n",
    "            X_new[:, 1:] = X\n",
    "        elif how == 'row':\n",
    "            X_new = np.ones((X.shape[0] + 1, X.shape[1]))\n",
    "            X_new[1:, :] = X\n",
    "        else:\n",
    "            raise AttributeError('`how` must be `column` or `row`')\n",
    "        \n",
    "        return X_new\n",
    "\n",
    "    def _feedforward(self, X, w1, w2):\n",
    "        \"\"\"Compute feedforward step\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        a1 : array, shape = [n_samples, n_features+1]\n",
    "            Input values with bias unit.\n",
    "        z2 : array, shape = [n_hidden, n_samples]\n",
    "            Net input of hidden layer.\n",
    "        a2 : array, shape = [n_hidden+1, n_samples]\n",
    "            Activation of hidden layer.\n",
    "        z3 : array, shape = [n_output_units, n_samples]\n",
    "            Net input of output layer.\n",
    "        a3 : array, shape = [n_output_units, n_samples]\n",
    "            Activation of output layer.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        a1 = self._add_bias_unit(X, how='column')\n",
    "        z2 = w1.dot(a1.T)\n",
    "        a2 = self._sigmoid(z2)\n",
    "        a2 = self._add_bias_unit(a2, how='row')\n",
    "        z3 = w2.dot(a2)\n",
    "        a3 = self._sigmoid(z3)\n",
    "        \n",
    "        return a1, z2, a2, z3, a3\n",
    "\n",
    "    def _L2_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        \n",
    "        return (lambda_/2.0) * (np.sum(w1[:, 1:] ** 2) +\n",
    "                                np.sum(w2[:, 1:] ** 2))\n",
    "\n",
    "    def _L1_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L1-regularization cost\"\"\"\n",
    "        \n",
    "        return (lambda_/2.0) * (np.abs(w1[:, 1:]).sum() +\n",
    "                                np.abs(w2[:, 1:]).sum())\n",
    "\n",
    "    def _get_cost(self, y_enc, output, w1, w2):\n",
    "        \"\"\"Compute cost function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_enc : array, shape = (n_labels, n_samples)\n",
    "            one-hot encoded class labels.\n",
    "        output : array, shape = [n_output_units, n_samples]\n",
    "            Activation of the output layer (feedforward)\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        cost : float\n",
    "            Regularized cost.\n",
    "\n",
    "        \"\"\"\n",
    "        term1 = -y_enc * (np.log(output))\n",
    "        term2 = (1.0 - y_enc) * np.log(1.0 - output)\n",
    "        cost = np.sum(term1 - term2)\n",
    "        L1_term = self._L1_reg(self.l1, w1, w2)\n",
    "        L2_term = self._L2_reg(self.l2, w1, w2)\n",
    "        cost = cost + L1_term + L2_term\n",
    "        \n",
    "        return cost\n",
    "\n",
    "    def _get_gradient(self, a1, a2, a3, z2, y_enc, w1, w2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        a1 : array, shape = [n_samples, n_features+1]\n",
    "            Input values with bias unit.\n",
    "        a2 : array, shape = [n_hidden+1, n_samples]\n",
    "            Activation of hidden layer.\n",
    "        a3 : array, shape = [n_output_units, n_samples]\n",
    "            Activation of output layer.\n",
    "        z2 : array, shape = [n_hidden, n_samples]\n",
    "            Net input of hidden layer.\n",
    "        y_enc : array, shape = (n_labels, n_samples)\n",
    "            one-hot encoded class labels.\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        grad1 : array, shape = [n_hidden_units, n_features]\n",
    "            Gradient of the weight matrix w1.\n",
    "        grad2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Gradient of the weight matrix w2.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # backpropagation\n",
    "        sigma3 = a3 - y_enc\n",
    "        z2 = self._add_bias_unit(z2, how='row')\n",
    "        sigma2 = w2.T.dot(sigma3) * self._sigmoid_gradient(z2)\n",
    "        sigma2 = sigma2[1:, :]\n",
    "        grad1 = sigma2.dot(a1)\n",
    "        grad2 = sigma3.dot(a2.T)\n",
    "\n",
    "        # regularize\n",
    "        grad1[:, 1:] += self.l2 * w1[:, 1:]\n",
    "        grad1[:, 1:] += self.l1 * np.sign(w1[:, 1:])\n",
    "        grad2[:, 1:] += self.l2 * w2[:, 1:]\n",
    "        grad2[:, 1:] += self.l1 * np.sign(w2[:, 1:])\n",
    "\n",
    "        return grad1, grad2\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        y_pred : array, shape = [n_samples]\n",
    "            Predicted class labels.\n",
    "\n",
    "        \"\"\"\n",
    "        if len(X.shape) != 2:\n",
    "            raise AttributeError('X must be a [n_samples, n_features] array.\\n'\n",
    "                                 'Use X[:,None] for 1-feature classification,'\n",
    "                                 '\\nor X[[i]] for 1-sample classification')\n",
    "\n",
    "        a1, z2, a2, z3, a3 = self._feedforward(X, self.w1, self.w2)\n",
    "        y_pred = np.argmax(z3, axis=0)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def loss_function(self, yhat, y):\n",
    "        yhat_dec = np.equal(yhat, y)\n",
    "        loss = 1.0 - (np.sum(yhat_dec)/len(y))\n",
    "        return loss\n",
    "        \n",
    "    def fit(self, X, y, print_progress=False, validation_freq=0, X_val=None, y_val=None):\n",
    "        \"\"\" Learn weights from training data.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        y : array, shape = [n_samples]\n",
    "            Target class labels.\n",
    "        print_progress : bool (default: False)\n",
    "            Prints progress as the number of epochs\n",
    "            to stderr.\n",
    "        validation_freq : int (default: 0)\n",
    "            For the value \"i\" it takes, it calculates the \n",
    "            train set and validation set error every \"ith\" iteration\n",
    "        X_val : array, shape = [n_validation_samples, n_features]\n",
    "            the validation set X values, to be provided \n",
    "            when validation_freq > 0\n",
    "        y_val : array, shape = [n_validation_samples]\n",
    "            the validation set y values, to be provided\n",
    "            when validation_freq > 0\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.cost_ = []\n",
    "        self.train_err_ = []\n",
    "        self.val_err_ = []\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        y_enc = self._encode_labels(y, self.n_output)\n",
    "        \n",
    "        delta_w1_prev = np.zeros(self.w1.shape)\n",
    "        delta_w2_prev = np.zeros(self.w2.shape)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            self.eta /= (1 + self.decrease_const*i)\n",
    "\n",
    "            if print_progress:\n",
    "                print('\\nEpoch: %d/%d' % (i+1, self.epochs))\n",
    "            \n",
    "            # Implement a code block to check the training and validation error (in percentage) for every given number\n",
    "            # of iterations stored in validation frequency. The training error is calculated on input X and validation\n",
    "            # error on input X_val. Store the training and validation error in self.train_err_ and self.val_err_             \n",
    "            # respectively. This will help plotting the errors in the following code (2 points)\n",
    "            # TODO\n",
    "            if validation_freq != 0 :\n",
    "                if i % validation_freq == 0:\n",
    "                    yhat_train = self.predict(X)\n",
    "                    error_train = self.loss_function(yhat_train, y)\n",
    "                    self.train_err_.append(error_train)\n",
    "                    #print ('Train error: %f' % error_train)\n",
    "\n",
    "                    yhat_val = self.predict(X_val)\n",
    "                    error_val = self.loss_function(yhat_val, y_val)\n",
    "                    self.val_err_.append(error_val)\n",
    "                    #print ('Validation error: %f' % error_val)\n",
    "                \n",
    "            if self.shuffle:\n",
    "                idx = np.random.permutation(y_data.shape[0])\n",
    "                X_data, y_enc = X_data[idx], y_enc[:, idx]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                a1, z2, a2, z3, a3 = self._feedforward(X_data[idx],\n",
    "                                                       self.w1,\n",
    "                                                       self.w2)\n",
    "                cost = self._get_cost(y_enc=y_enc[:, idx],\n",
    "                                      output=a3,\n",
    "                                      w1=self.w1,\n",
    "                                      w2=self.w2)\n",
    "                self.cost_.append(cost)\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2 = self._get_gradient(a1=a1, a2=a2,\n",
    "                                                  a3=a3, z2=z2,\n",
    "                                                  y_enc=y_enc[:, idx],\n",
    "                                                  w1=self.w1,\n",
    "                                                  w2=self.w2)\n",
    "\n",
    "                delta_w1, delta_w2 = self.eta * grad1, self.eta * grad2\n",
    "                self.w1 -= (delta_w1 + (self.alpha * delta_w1_prev))\n",
    "                self.w2 -= (delta_w2 + (self.alpha * delta_w2_prev))\n",
    "                delta_w1_prev, delta_w2_prev = delta_w1, delta_w2\n",
    "                \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 60000, columns: 784\n",
      "Rows: 10000, columns: 784\n",
      "Train error: 0.922050\n",
      "Validation error: 0.920600\n",
      "Train error: 0.083650\n",
      "Validation error: 0.083200\n",
      "Train error: 0.063983\n",
      "Validation error: 0.065000\n",
      "Train error: 0.059050\n",
      "Validation error: 0.059700\n",
      "Train error: 0.052617\n",
      "Validation error: 0.054700\n",
      "Train error: 0.043417\n",
      "Validation error: 0.049300\n",
      "Train error: 0.035967\n",
      "Validation error: 0.042600\n",
      "Train error: 0.029650\n",
      "Validation error: 0.042800\n",
      "Train error: 0.025967\n",
      "Validation error: 0.041200\n",
      "Train error: 0.025100\n",
      "Validation error: 0.041100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/matplotlib/axes/_base.py:3245: UserWarning: Attempted to set non-positive ylimits for log-scale axis; invalid limits will be ignored.\n",
      "  'Attempted to set non-positive ylimits for log-scale axis; '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error: 0.025083\n",
      "Test error: 0.041000\n",
      "Train error: 0.904383\n",
      "Validation error: 0.907000\n",
      "Train error: 0.064617\n",
      "Validation error: 0.065000\n",
      "Train error: 0.048850\n",
      "Validation error: 0.049000\n",
      "Train error: 0.039633\n",
      "Validation error: 0.045100\n",
      "Train error: 0.032867\n",
      "Validation error: 0.041500\n",
      "Train error: 0.024517\n",
      "Validation error: 0.036200\n",
      "Train error: 0.016833\n",
      "Validation error: 0.033300\n",
      "Train error: 0.010933\n",
      "Validation error: 0.030700\n",
      "Train error: 0.009967\n",
      "Validation error: 0.031000\n",
      "Train error: 0.009833\n",
      "Validation error: 0.031200\n",
      "Train error: 0.009867\n",
      "Test error: 0.031200\n",
      "Train error: 0.881417\n",
      "Validation error: 0.887500\n",
      "Train error: 0.051350\n",
      "Validation error: 0.054200\n",
      "Train error: 0.041067\n",
      "Validation error: 0.043100\n",
      "Train error: 0.029400\n",
      "Validation error: 0.035500\n",
      "Train error: 0.021717\n",
      "Validation error: 0.031100\n",
      "Train error: 0.013533\n",
      "Validation error: 0.027400\n",
      "Train error: 0.006500\n",
      "Validation error: 0.026600\n",
      "Train error: 0.004117\n",
      "Validation error: 0.025100\n",
      "Train error: 0.003900\n",
      "Validation error: 0.025200\n",
      "Train error: 0.003850\n",
      "Validation error: 0.025100\n",
      "Train error: 0.003833\n",
      "Test error: 0.025100\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOMAAADgCAYAAADxGtHiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXEx4GJ2LCQ+jczHhYXQuJjyMzsXE/wclhXnYkdpAvwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c5e0438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm0AAADgCAYAAABRulGRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nImYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fde78d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load train and test data\n",
    "X_train, y_train = load_mnist('/Users/aydanrende/Documents/mnist/mnist/', kind='train')\n",
    "print('Rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1]))\n",
    "    \n",
    "X_test, y_test = load_mnist('/Users/aydanrende/Documents/mnist/mnist/', kind='t10k')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))\n",
    "\n",
    "neurons = [50, 100, 200]\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "for n, hidden in enumerate(neurons):\n",
    "    #create a MLP object\n",
    "    nn = MLP(n_output=10, \n",
    "             n_features=X_train.shape[1], \n",
    "             n_hidden=hidden, \n",
    "             l2=0.1, \n",
    "             l1=0.0, \n",
    "             epochs=1000, \n",
    "             eta=0.001,\n",
    "             alpha=0.001,\n",
    "             decrease_const=0.00001,\n",
    "             minibatches=50, \n",
    "             shuffle=True,\n",
    "             random_state=1)\n",
    "    \n",
    "    # Fit the data with NN\n",
    "    nn.fit(X_train, y_train, print_progress=False,\n",
    "             validation_freq=100,\n",
    "             X_val=X_test, y_val=y_test)\n",
    "    \n",
    "    iter = [ i*100 for i in range(1000 // 100)]\n",
    "    plt.subplot(1,len(neurons),n+1)\n",
    "    plt.plot(iter, nn.train_err_)\n",
    "    plt.plot(iter, nn.val_err_)\n",
    "    plt.yscale('log')\n",
    "    plt.ylim([0, 100])\n",
    "    plt.ylabel('Error')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.title(\"Neurons: {0} (error)\".format(hidden))\n",
    "    \n",
    "    # Print the training accuracy and test accuracy in percentage for each value of hidden layer size. (0.5 point) \n",
    "    # TODO\n",
    "    yhat_train = nn.predict(X_train)\n",
    "    error_train = nn.loss_function(yhat_train, y_train)\n",
    "    print ('Train error: %f' % error_train)\n",
    "    \n",
    "    yhat_test = nn.predict(X_test)\n",
    "    error_test = nn.loss_function(yhat_test, y_test)\n",
    "    print ('Test error: %f' % error_test)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Using these plots and the related variables from the code to make suggestions for an early stopping criteria for each hidden layer size. (1.5 points)\n",
    "\n",
    "The idea is; instead of running our optimization algorithm until we reach a (local) minimum of validation error, we run it until the error on the validation set has not improved for some amount of time. If we increase the training iterations, the validation error would increase and form an asymmetric U-shaped curve.\n",
    "\n",
    "- For 50 neurons, we can observe that the training objective decreases consistently over time, the validation set average loss also decreases but slower than the traning error so 800 epochs could be enough for this case.\n",
    "- For 100 neurons, the training error decreases over time, but the validation error eventually begins to increase again after almost 650 epochs. Therefore the training iterations could be set 650 for this case.\n",
    "- For 200 neurons, the validation error begins to increase earlier than the other cases. We could set the training iterations as 500 epochs.\n",
    "\n",
    "b) As the number of neurons are increased, you will observe differences in the early stopping criteria for each hidden layer size. Why do you observe such differences? (2 points)\n",
    "\n",
    "- When a neural network has too few hidden neurons, it does not have the capacity to learn enough of the underlying patterns to predict effectively. It would need more iterations to learn therefore we cannot stop earlier, we would need to run the training iterations more. When the neural network has more neurons, it learns in less training iterations and starts to do better so we can stop earlier (before becoming too specific to the training data). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 3. Regularization: Dropout (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** To implement and study dropout for neural networks.\n",
    "\n",
    "Implement dropout for layer 2 in the three-layered network you developed for Exercise 2. A simple dropout implementation creates a mask ($r^{(l)}_j$) for every neuron $j$ of the hidden layer $l$ by drawing from a [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution) with probability $p$.\n",
    "$$ r^{(l)}_j \\sim Bernoulli(p) $$\n",
    "This mask is then applied to the hidden layer output ($h^{(l)}$) to obtain the regularized hidden layer activation $\\hat{h}^{(l)}$\n",
    "$$ \\hat{h}^{(l)} = r^{(l)} * h^{(l)}$$\n",
    "However, such an implementation requires the layerl be multiplied by the dropout coefficient $p$ at evaluation time to balance the larger number of active units during testing.\n",
    "$$ \\hat{h}^{(l)} = p * h^{(l)}$$\n",
    "Such an implementation requires the code to switch between different code blocks for forward-pass evaluation during training and testing. Hence, a smoother way to implement dropout is to use ***inverted dropout*** where the mask generated at the training is multiplied by the inverse of the dropout coefficient.\n",
    "$$ r^{(l)}_j \\sim Bernoulli(p) * \\frac{1}{p}$$\n",
    "This scheme allows the scaling to be learned during training and hence, no switching between code blocks is required.\n",
    "\n",
    "Update the code below (specified by #TODO) to implement inverted dropout for a hidden layer size of 200 neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 5000, columns: 784\n",
      "Rows: 5000, columns: 784\n",
      "Dropout: 0.10\n",
      "Train error: 0.884200\n",
      "Test error: 0.890200\n",
      "Dropout: 0.30\n",
      "Train error: 0.722800\n",
      "Test error: 0.718400\n",
      "Dropout: 0.50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-096fb76ccfe9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;31m# Fit the data with NN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     nn.fit(X_train, y_train, print_progress=False,validation_freq=100,\n\u001b[0;32m--> 443\u001b[0;31m              X_val=X_test, y_val=y_test)\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;31m# Print the training accuracy and test accuracy in percentage for each value of dropout. This part is same as\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-096fb76ccfe9>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, print_progress, validation_freq, X_val, y_val)\u001b[0m\n\u001b[1;32m    387\u001b[0m                 a1, z2, a2, z3, a3 = self._feedforward(X_data[idx],\n\u001b[1;32m    388\u001b[0m                                                        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m                                                        self.w2)\n\u001b[0m\u001b[1;32m    390\u001b[0m                 cost = self._get_cost(y_enc=y_enc[:, idx],\n\u001b[1;32m    391\u001b[0m                                       \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-096fb76ccfe9>\u001b[0m in \u001b[0;36m_feedforward\u001b[0;34m(self, X, w1, w2)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_bias_unit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'column'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mz2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0ma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0ma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_bias_unit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit\n",
    "from __future__ import division\n",
    " \n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path, \n",
    "                               '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, \n",
    "                               '%s-images-idx3-ubyte' % kind)\n",
    "        \n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', \n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, \n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", \n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, \n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    " \n",
    "    return images, labels\n",
    "  \n",
    "\n",
    "class MLP(object):\n",
    "    \"\"\" Feedforward neural network / Multi-layer perceptron classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    n_output : int\n",
    "        Number of output units, should be equal to the\n",
    "        number of unique class labels.\n",
    "        \n",
    "    n_features : int\n",
    "        Number of features (dimensions) in the target dataset.\n",
    "        Should be equal to the number of columns in the X array.\n",
    "        \n",
    "    n_hidden : int (default: 30)\n",
    "        Number of hidden units.\n",
    "        \n",
    "    l1 : float (default: 0.0)\n",
    "        Lambda value for L1-regularization.\n",
    "        No regularization if l1=0.0 (default)\n",
    "        \n",
    "    l2 : float (default: 0.0)\n",
    "        Lambda value for L2-regularization.\n",
    "        No regularization if l2=0.0 (default)\n",
    "        \n",
    "    epochs : int (default: 500)\n",
    "        Number of passes over the training set.\n",
    "        \n",
    "    eta : float (default: 0.001)\n",
    "        Learning rate.\n",
    "        \n",
    "    alpha : float (default: 0.0)\n",
    "        Momentum constant.\n",
    "        \n",
    "    decrease_const : float (default: 0.0)\n",
    "        Decrease constant. Shrinks the learning rate\n",
    "        after each epoch via eta / (1 + epoch*decrease_const)\n",
    "        \n",
    "    shuffle : bool (default: True)\n",
    "        Shuffles training data every epoch if True to prevent circles.\n",
    "        \n",
    "    minibatches : int (default: 1)\n",
    "        Divides training data into k minibatches for efficiency.\n",
    "        Normal gradient descent learning if k=1 (default).\n",
    "        \n",
    "    random_state : int (default: None)\n",
    "        Set random state for shuffling and initializing the weights.\n",
    "    \n",
    "    dropout : float (default: 0.5)\n",
    "        Set the dropout coefficient\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    cost_ : list\n",
    "      Sum of squared errors after each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "      \n",
    "    def __init__(self, n_output, n_features, n_hidden=30,\n",
    "                 l1=0.0, l2=0.0, epochs=50, eta=0.001,\n",
    "                 alpha=0.0, decrease_const=0.0, shuffle=True,\n",
    "                 minibatches=1, random_state=1, dropout = 0.5):\n",
    "\n",
    "        self.n_output = n_output\n",
    "        self.r = np.random.RandomState(random_state)\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.w1, self.w2 = self._initialize_weights()\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        # Initialize the class variable \"dropout\" like other variables above. Also, initialize a variable mask to None.\n",
    "        # This will allow sharing dropout information during forward and backward pass of the neural networks. Note \n",
    "        # that the __init__ function has already been modified to include dropout coefficient as an argument. (0.5 points)\n",
    "        #TODO\n",
    "        self.mask = None\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def _encode_labels(self, y, k):    \n",
    "        \"\"\"Encode the labels using one-hot representation\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        y : y represents target values.\n",
    "\n",
    "        Returns\n",
    "        -----------\n",
    "        onehot array\n",
    "\n",
    "        \"\"\"\n",
    "       \n",
    "        onehot = np.zeros((k, y.shape[0]))\n",
    "        for idx, val in enumerate(y):\n",
    "            onehot[val, idx] = 1.0\n",
    "        \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        \n",
    "        w1 = self.r.uniform(-1.0, 1.0,\n",
    "                               size=self.n_hidden*(self.n_features + 1))\n",
    "        w1 = w1.reshape(self.n_hidden, self.n_features + 1)\n",
    "        w2 = self.r.uniform(-1.0, 1.0,\n",
    "                               size=self.n_output*(self.n_hidden + 1))\n",
    "        w2 = w2.reshape(self.n_output, self.n_hidden + 1)\n",
    "        \n",
    "        return w1, w2\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Compute logistic function (sigmoid)\n",
    "\n",
    "        Uses scipy.special.expit to avoid overflow\n",
    "        error for very small input values z.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        return expit(z)\n",
    "\n",
    "    def _sigmoid_gradient(self, z):\n",
    "        \"\"\"Compute gradient of the logistic function\"\"\"\n",
    "        \n",
    "        sg = self._sigmoid(z)\n",
    "        return sg * (1.0 - sg)\n",
    "\n",
    "    def _add_bias_unit(self, X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        \n",
    "        if how == 'column':\n",
    "            X_new = np.ones((X.shape[0], X.shape[1] + 1))\n",
    "            X_new[:, 1:] = X\n",
    "        elif how == 'row':\n",
    "            X_new = np.ones((X.shape[0] + 1, X.shape[1]))\n",
    "            X_new[1:, :] = X\n",
    "        else:\n",
    "            raise AttributeError('`how` must be `column` or `row`')\n",
    "        \n",
    "        return X_new\n",
    "\n",
    "    def _feedforward(self, X, w1, w2):\n",
    "        \"\"\"Compute feedforward step\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        a1 : array, shape = [n_samples, n_features+1]\n",
    "            Input values with bias unit.\n",
    "        z2 : array, shape = [n_hidden, n_samples]\n",
    "            Net input of hidden layer.\n",
    "        a2 : array, shape = [n_hidden+1, n_samples]\n",
    "            Activation of hidden layer.\n",
    "        z3 : array, shape = [n_output_units, n_samples]\n",
    "            Net input of output layer.\n",
    "        a3 : array, shape = [n_output_units, n_samples]\n",
    "            Activation of output layer.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        a1 = self._add_bias_unit(X, how='column')\n",
    "        z2 = w1.dot(a1.T)\n",
    "        a2 = self._sigmoid(z2)\n",
    "        a2 = self._add_bias_unit(a2, how='row')\n",
    "        \n",
    "        # Implement inverted dropout using class variables dropout and activation variable (a2) for the forward\n",
    "        # pass for the second hidden layer below. To create the mask you will have to use self.r.binomial for \n",
    "        # generating the bernoulli distribution. The mask created here needs to be stored in the appropriate mask\n",
    "        # variable defined in the __init__ function for further use by the backward pass. (2.5 points)\n",
    "        #TODO\n",
    "        self.mask=np.divide(self.r.binomial(1,self.dropout,size=self.n_hidden),self.dropout)\n",
    "        a2[1:,:]=a2[1:,:]*self.mask[:,np.newaxis]\n",
    "        \n",
    "        z3 = w2.dot(a2)\n",
    "        a3 = self._sigmoid(z3)\n",
    "        \n",
    "        return a1, z2, a2, z3, a3\n",
    "\n",
    "    def _L2_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        \n",
    "        return (lambda_/2.0) * (np.sum(w1[:, 1:] ** 2) +\n",
    "                                np.sum(w2[:, 1:] ** 2))\n",
    "\n",
    "    def _L1_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L1-regularization cost\"\"\"\n",
    "        \n",
    "        return (lambda_/2.0) * (np.abs(w1[:, 1:]).sum() +\n",
    "                                np.abs(w2[:, 1:]).sum())\n",
    "\n",
    "    def loss_function(self, yhat, y):\n",
    "        yhat_dec = np.equal(yhat, y)\n",
    "        loss = 1.0 - (np.sum(yhat_dec)/len(y))\n",
    "        return loss\n",
    "    \n",
    "    def _get_cost(self, y_enc, output, w1, w2):\n",
    "        \"\"\"Compute cost function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_enc : array, shape = (n_labels, n_samples)\n",
    "            one-hot encoded class labels.\n",
    "        output : array, shape = [n_output_units, n_samples]\n",
    "            Activation of the output layer (feedforward)\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        cost : float\n",
    "            Regularized cost.\n",
    "\n",
    "        \"\"\"\n",
    "        np.seterr(divide='ignore')\n",
    "        term1 = -y_enc * (np.log(output))\n",
    "        term2 = (1.0 - y_enc) * np.log(1.0 - output)\n",
    "        \n",
    "        term1[np.isneginf(term1)] = 0\n",
    "        term2[np.isneginf(term2)] = 0\n",
    "        \n",
    "        cost = np.sum(term1 - term2)\n",
    "        L1_term = self._L1_reg(self.l1, w1, w2)\n",
    "        L2_term = self._L2_reg(self.l2, w1, w2)\n",
    "        cost = cost + L1_term + L2_term\n",
    "        \n",
    "        return cost\n",
    "\n",
    "    def _get_gradient(self, a1, a2, a3, z2, y_enc, w1, w2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        a1 : array, shape = [n_samples, n_features+1]\n",
    "            Input values with bias unit.\n",
    "        a2 : array, shape = [n_hidden+1, n_samples]\n",
    "            Activation of hidden layer.\n",
    "        a3 : array, shape = [n_output_units, n_samples]\n",
    "            Activation of output layer.\n",
    "        z2 : array, shape = [n_hidden, n_samples]\n",
    "            Net input of hidden layer.\n",
    "        y_enc : array, shape = (n_labels, n_samples)\n",
    "            one-hot encoded class labels.\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        grad1 : array, shape = [n_hidden_units, n_features]\n",
    "            Gradient of the weight matrix w1.\n",
    "        grad2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Gradient of the weight matrix w2.\n",
    "        \"\"\"\n",
    "        \n",
    "        # backpropagation\n",
    "        sigma3 = a3 - y_enc\n",
    "        z2 = self._add_bias_unit(z2, how='row')\n",
    "        sigma2 = w2.T.dot(sigma3) * self._sigmoid_gradient(z2)\n",
    "        \n",
    "        # Implement dropout for the backward pass, use class variables for mask and dropout for this task (2.5 points)\n",
    "        # TODO\n",
    "        \n",
    "        sigma2[1:, :]=np.divide((sigma2[1:,:]*self.mask[:,np.newaxis]),self.dropout)\n",
    "        sigma2 = sigma2[1:, :]\n",
    "        grad1 = sigma2.dot(a1)\n",
    "        grad2 = sigma3.dot(a2.T)\n",
    "\n",
    "        # regularize\n",
    "        grad1[:, 1:] += self.l2 * w1[:, 1:]\n",
    "        grad1[:, 1:] += self.l1 * np.sign(w1[:, 1:])\n",
    "        grad2[:, 1:] += self.l2 * w2[:, 1:]\n",
    "        grad2[:, 1:] += self.l1 * np.sign(w2[:, 1:])\n",
    "\n",
    "        return grad1, grad2\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        y_pred : array, shape = [n_samples]\n",
    "            Predicted class labels.\n",
    "\n",
    "        \"\"\"\n",
    "        if len(X.shape) != 2:\n",
    "            raise AttributeError('X must be a [n_samples, n_features] array.\\n'\n",
    "                                 'Use X[:,None] for 1-feature classification,'\n",
    "                                 '\\nor X[[i]] for 1-sample classification')\n",
    "\n",
    "        a1, z2, a2, z3, a3 = self._feedforward(X, self.w1, self.w2)\n",
    "        y_pred = np.argmax(z3, axis=0)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    def fit(self, X, y, print_progress=False, validation_freq=0, X_val=None, y_val=None):\n",
    "        \"\"\" Learn weights from training data.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        y : array, shape = [n_samples]\n",
    "            Target class labels.\n",
    "        print_progress : bool (default: False)\n",
    "            Prints progress as the number of epochs\n",
    "            to stderr.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.cost_ = []\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        y_enc = self._encode_labels(y, self.n_output)\n",
    "\n",
    "        delta_w1_prev = np.zeros(self.w1.shape)\n",
    "        delta_w2_prev = np.zeros(self.w2.shape)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            self.eta /= (1 + self.decrease_const*i)\n",
    "\n",
    "            if print_progress:\n",
    "                print('\\nEpoch: %d/%d' % (i+1, self.epochs))\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx = np.random.permutation(y_data.shape[0])\n",
    "                X_data, y_enc = X_data[idx], y_enc[:, idx]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                a1, z2, a2, z3, a3 = self._feedforward(X_data[idx],\n",
    "                                                       self.w1,\n",
    "                                                       self.w2)\n",
    "                cost = self._get_cost(y_enc=y_enc[:, idx],\n",
    "                                      output=a3,\n",
    "                                      w1=self.w1,\n",
    "                                      w2=self.w2)\n",
    "                self.cost_.append(cost)\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2 = self._get_gradient(a1=a1, a2=a2,\n",
    "                                                  a3=a3, z2=z2,\n",
    "                                                  y_enc=y_enc[:, idx],\n",
    "                                                  w1=self.w1,\n",
    "                                                  w2=self.w2)\n",
    "\n",
    "                delta_w1, delta_w2 = self.eta * grad1, self.eta * grad2\n",
    "                self.w1 -= (delta_w1 + (self.alpha * delta_w1_prev))\n",
    "                self.w2 -= (delta_w2 + (self.alpha * delta_w2_prev))\n",
    "                delta_w1_prev, delta_w2_prev = delta_w1, delta_w2\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "# Load train and test data\n",
    "X_train, y_train = load_mnist('/Users/aydanrende/Documents/mnist/mnist/', kind='train')\n",
    "r = np.random.RandomState(1)\n",
    "idx = r.choice(y_train.shape[0],5000,replace=False)\n",
    "X_train, y_train = X_train[idx], y_train[idx]\n",
    "print('Rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1]))\n",
    "    \n",
    "X_test, y_test = load_mnist('/Users/aydanrende/Documents/mnist/mnist/', kind='t10k')\n",
    "idx = r.choice(y_test.shape[0],5000,replace=False)\n",
    "X_test, y_test = X_test[idx], y_test[idx]\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))\n",
    "  \n",
    "vals = np.arange(0.1, 1.0, 0.2)\n",
    "for dropout in vals:\n",
    "    # create a MLP object\n",
    "    print('Dropout: %.2f' % dropout)\n",
    "    nn = MLP(n_output=10, \n",
    "             n_features=X_train.shape[1], \n",
    "             n_hidden=50, \n",
    "             l2=0.0, \n",
    "             l1=0.0, \n",
    "             epochs=500, \n",
    "             eta=0.001,\n",
    "             alpha=0.001,\n",
    "             decrease_const=0.00001,\n",
    "             minibatches=50, \n",
    "             shuffle=False,\n",
    "             random_state=1,\n",
    "             dropout = dropout)\n",
    "    \n",
    "    # Fit the data with NN\n",
    "    nn.fit(X_train, y_train, print_progress=False,validation_freq=100,\n",
    "             X_val=X_test, y_val=y_test)\n",
    "    \n",
    "    # Print the training accuracy and test accuracy in percentage for each value of dropout. This part is same as \n",
    "    # a question asked in Exercise 2, so you may use the code from there. (0 points) \n",
    "    # TODO\n",
    "    yhat_train = nn.predict(X_train)\n",
    "    error_train = nn.loss_function(yhat_train, y_train)\n",
    "    print ('Train error: %f' % error_train)\n",
    "    \n",
    "    yhat_test = nn.predict(X_test)\n",
    "    error_test = nn.loss_function(yhat_test, y_test)\n",
    "    print ('Test error: %f' % error_test)\n",
    "    \n",
    "   '''THE CODE WORKS, PLS RUN IT TO SEE THE RESULTS WE HAD NO TIME FOR A COMPLETE RUN'''\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Intuitively, L1 and L2 minimize the interdependence and the value of feature weights by penalising the loss function. In the same vein, what kind of interdependence does dropout affect? (0.5 points)\n",
    "- A fully connected layer occupies most of the parameters, and hence, neurons develop co-dependency amongst each other during training which curbs the individual power of each neuron leading to over-fitting of training data.\n",
    "\n",
    "b) Why can Dropout be considered as an approximation to Bagging? (1 point)\n",
    "- Bagging, in which each member of the ensemble is trained with a different subsample of the input data, and thus has learned only a subset of the whole possible input feature space.\n",
    "\n",
    "- Dropout, then can be seen as an extreme version of bagging. At each training step in a mini-batch, the dropout procedure creates a different network (by randomly removing some units), which is trained using backpropagation as usual. Conceptually, then, the whole procedure is akin to using an ensemble of many different networks (one per step) each trained with a single sample.\n",
    "\n",
    "- At test time the whole network is used (all units) but with scaled down weights. Mathematically this approximates ensemble averaging (using the geometric mean as average)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "You should provide a single Jupyter notebook as a solution. The naming should include the assignment number and matriculation IDs of all team members in the following format:\n",
    "**assignment-7_matriculation1_matriculation2_matriculation3.ipynb** (in case of 3 team members). \n",
    "Make sure to keep the order matriculation1_matriculation2_matriculation3 the same for all assignments.\n",
    "\n",
    "Please, submit your solution to your tutor (with **[NNIA][assignment-7]** in email subject):\n",
    "1. Maksym Andriushchenko s8mmandr@stud.uni-saarland.de\n",
    "2. Marius Mosbach s9msmosb@stud.uni-saarland.de\n",
    "3. Rajarshi Biswas rbisw17@gmail.com\n",
    "4. Marimuthu Kalimuthu s8makali@stud.uni-saarland.de\n",
    "\n",
    "**If you are in a team, please submit only 1 solution to only 1 tutor.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
