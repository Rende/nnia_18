{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Sheet 6:  Regularization methods in Machine learning and their application in Feedforward neural networks  (deadline: 16 Dec, 23:59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Regularization methods in ML $~$ (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Study the effects of **L2** and **L1** regularization on the weights used for modelling the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ridge regression*** is very similar to least squares, except that the weights are estimated by minimizing a slightly different quantity. In particular, the ridge regression co-efficient estimates $\\mathbf{W_{ridge}}$ are the values that minimize, \n",
    "\n",
    "$$\\mathbf{J(W) ~~=~~ \\big|\\big|~Y-XW~\\big|\\big|_{2}^2 ~+~\\lambda~ \\big|\\big|~W~\\big|\\big|_{2}^2}$$ \n",
    "\n",
    "where,\n",
    "\n",
    "$\\mathbf{\\lambda>0}$ is the regularizer,\n",
    "\n",
    "**X** is the design matrix,\n",
    "\n",
    "$\\mathbf{W}$ is the weight vector and\n",
    "\n",
    "**Y** represents the responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ridge regression*** seeks weight estimate $W^{Ridge}$ that fit the data well by minimizing the squared error $~$ $\\mathbf{||~Y-XW~||^2}$ (which was also the linear regression cost function).\n",
    "However, the second term, $\\mathbf{||~W~||^2}$, called a ***shrinkage penalty*** is small when $\\mathbf{W}$, i.e., $(w_1, w_2, ..., w_d)^T$ are close to zero. Thus, it has the effect of shrinking the estimates of $w_i$ towards zero.\n",
    "\n",
    "The regularizer $\\mathbf{\\lambda}$ serves to control the relative impact of these two terms on the regression weight estimates. when $\\mathbf{\\lambda=0}$, the penalty term has no effect, and ridge regression will produce the least squares estimates. However, as $\\mathbf{\\lambda \\rightarrow \\infty}$, the impact of the shrinkage penalty grows and the ridge regression weight estimates will approach zero. Unlike least squares, which generates only one set of weight estimates, ridge regression will produce a different set of weight estimates, $\\mathbf{W_{\\lambda}^{Ridge}}$, for each value of $\\mathbf{\\lambda}$. Hence, selecting a good value of $\\mathbf{\\lambda}$ is critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$1.$ **Plot the magnitude of each weight in $\\mathbf{W^{Ridge}}$ vs $\\mathbf{\\lambda}$ and explain how the regularizer $\\mathbf{\\lambda}$ affects the Ridge weights $\\mathbf{W^{Ridge}}$.** $~$ ($2.5$ points)\n",
    "\n",
    "Download the dataset, **data.csv**, from the NNIA piazza page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "# TODO Implement\n",
    "\n",
    "# Read 'Salary' as your response/dependent variable\n",
    "# TODO Implement\n",
    "\n",
    "# Drop the column with the dependent variable 'Salary'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-99961fdc5389>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# TODO Implement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ax' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize values for the alphas\n",
    "lamdas = 10**np.linspace(10,-2,100)*0.5\n",
    "\n",
    "# Create a Ridge Object that performs ridge regression\n",
    "# TODO Implement\n",
    "\n",
    "# Create list to hold ridge weights\n",
    "# TODO Implement\n",
    "\n",
    "# Iterate over all lamdas, performing data fitting with ridge regression \n",
    "# and find the corresponding co-efficients\n",
    "\n",
    "#TODO Implement\n",
    "\n",
    "\n",
    "# Generate the plot\n",
    "# TODO Implement\n",
    "\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "\n",
    "# Name the plot\n",
    "# TODO Implement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now generate the same plot as above using Tensorflow\n",
    "# for the same set of lambdas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$2$. Next we deal with **L1 regularization** for which the corresponding method is called **Lasso.** In Lasso, we minimize the function, \n",
    "$$\\mathbf{J(W) ~~=~~ \\big|\\big|~Y-XW~\\big|\\big|_{2}^2 ~+~\\lambda~ \\big|\\big|~W~\\big|\\big|_{1}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the magnitude of each weight in $\\mathbf{W^{Lasso}}$ vs $\\mathbf{\\lambda}$ and explain how the regularizer $\\mathbf{\\lambda}$ affects the Lasso weights $\\mathbf{W^{Lasso}}$.** $~$ ($2.5$ points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a Lasso Object(set max_iter to 10000)\n",
    "# TODO Implement\n",
    "\n",
    "# Create list to hold lasso weights\n",
    "# TODO Implement\n",
    "\n",
    "# Iterate over all alphas, performing data fitting with Lasso\n",
    "# and find the corresponding co-efficients\n",
    "# TODO Implement\n",
    "\n",
    "\n",
    "# Generate the plot\n",
    "# TODO Implement\n",
    "\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "\n",
    "# Name the plot\n",
    "# TODO Implement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now generate the same plot as above using Tensorflow\n",
    "# for the same set of lambdas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment on the plots generated in problems $1$ and $2$ respectively.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Impact of norms in the  Regularizer $~$ (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$4$. Assume$~$ $\\mathbf{x} \\in R^2$, $(x_1, x_2) \\in [-1, 1]\\times[-1, 1]$. $~$ (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, draw the contour plots for $\\mathbf{\\big|\\big|~x~\\big|\\big|_{0}}$, $\\mathbf{\\big|\\big|~x~\\big|\\big|_{1}}$, $\\mathbf{\\big|\\big|~x~\\big|\\big|_{2}}$ and $\\mathbf{\\big|\\big|~x~\\big|\\big|_{\\infty}}$ norms (consider all possible isolines in the given interval,i.e., ($[-1,1]\\times[-1,1]$) and **explain** how you get the corresponding plot, i.e., provide the mathematical formula for getting the outermost isoline in each case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$5$. Sketch the **Lasso** optimization function, $~$ $\\mathbf{J(W) ~~=~~ \\big|\\big|~Y-XW~\\big|\\big|_{2}^2 ~+~\\lambda~ \\big|\\big|~W~\\big|\\big|_{1}}$ $~$ in two dimensions. From this sketch try to explain why **Lasso** induces **sparsity.** $~$ (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting to know Back-Propagation in details $~$ (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neural Network](https://github.com/mmarius/nnia-tutorial/blob/master/neural-net.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a **Feedforward Neural network** with one **input layer**, one **hidden layer** and one **output layer.** The **hidden layer** and **output layer** use the sigmoid function, $\\mathbf{\\sigma(x) = \\frac{1}{1+exp(-x)}}$, as **activation function.** Also note, that the network minimizes **Binary Cross Entropy loss**, given by, $$\\mathbf{J = \\sum -y\\log(\\hat{y}) - (1-y)\\log(1-\\hat{y})}$$\n",
    "\n",
    "We consider the true class labels to be **binary**, i.e., either $0$ or $1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the purpose of computing the derivatives of the loss/cost function consider the numerical values obtained by the network.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input layer** consists of two nodes, $x_1$ and $x_2$ respectively. For our problem consider the following input,\n",
    "$$\\begin{bmatrix} x_1\\\\ x_2\\\\ \\end{bmatrix} = \\begin{bmatrix} -1\\\\ 1\\\\ \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hidden layer** is made up of 3 neurons and the corresponding matrix of weights is as given:\n",
    "$$\n",
    "\\mathbf{W_{hidden}}\n",
    "~=~\n",
    "\\begin{bmatrix} w_1^{1} & w_1^{2} & w_1^{3} \\\\ w_2^{1} & w_2^{2} & w_2^{3} \\end{bmatrix}\n",
    "~=~\n",
    "\\begin{bmatrix} 0.15 & -0.25 & 0.05\\\\ 0.20 & 0.10 & -0.15 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Note:** Output of **Hidden layer** is given by, $~~$ $\\mathbf{a=\\sigma~(W_{hidden}^{T}x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output layer** consists of one neuron, i.e., the **network** generates a single output. **For our problem, the true class label is $1$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix corresponding to the **Output layer** is given by,\n",
    "$$\n",
    "\\mathbf{W_{out}}\n",
    "~=~\n",
    "\\begin{bmatrix} w_1\\\\w_2 \\\\w_3\\end{bmatrix}\n",
    "~=~\n",
    "\\begin{bmatrix} 0.20\\\\-0.35\\\\0.15 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Note:** output from the **Output layer** is given by, $~$ $\\mathbf{\\hat{y} = \\sigma~(W_{out}^Ta)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$6$. Execute the following sequence of operations and **show that Binary Cross Entropy loss is getting reduced, i.e., $ C^2 < C^1$** $~$ ($3$ points)\n",
    "\n",
    "**Perform Forward-propagation to generate output** $\\to$ **Compute loss or cost ($C^1$)** $\\to$ **perform Back-propagation to compute the error** $\\to$ **perform Gradient descent to update the weights** $\\to$ **peform Forward-propagation again with updated weights** $\\to$ **Compute loss or cost ($C^2$)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  $C^i$ denotes the loss or cost at the $i^{th}$ iteration, for performing Gradient descent consider a learning rate of $0.1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-forward Neural Network with L1 and L2 regularization $~$ (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise you would build a **feed-forward network** from scratch using only **Numpy** in python. For this, you also have to implement **Back-propagation** in python. Additionally, this network should have the option of **L1 and L2 regularization** enabled within it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download **mnist** dataset from NNIA piazza page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    " \n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"function for loading data\"\"\"\n",
    "    labels_path = os.path.join(path, \n",
    "                               '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, \n",
    "                               '%s-images-idx3-ubyte' % kind)\n",
    "        \n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', \n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, \n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", \n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, \n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    " \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_mnist('/Users/aydanrende/Documents/mnist/mnist/', kind='train')\n",
    "print('Rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = load_mnist('/Users/aydanrende/Documents/mnist/mnist/', kind='t10k')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "class MLP(object):\n",
    "    n_output = 0\n",
    "    n_features = 0\n",
    "    n_hidden = 0\n",
    "    l1 = 0.0\n",
    "    l2 = 0.0\n",
    "    epochs = 0\n",
    "    eta = 0.001\n",
    "    decrease_const = 0.0\n",
    "    shuffle = True\n",
    "    minibatches = 1\n",
    "    random_state = None\n",
    "    \"\"\" Feedforward neural network with a single hidden layer.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    n_output : int\n",
    "        Number of output units, should be equal to the\n",
    "        number of unique class labels.\n",
    "        \n",
    "    n_features : int\n",
    "        Number of features (dimensions) in the target dataset.\n",
    "        Should be equal to the number of columns in the X array.\n",
    "        \n",
    "    n_hidden : int\n",
    "        Number of hidden units.\n",
    "        \n",
    "    l1 : float\n",
    "        Regularizer for L1-regularization.\n",
    "        l1=0.0 implies no regularization\n",
    "        \n",
    "    l2 : float\n",
    "        Lambda value for L2-regularization.\n",
    "        l2=0.0 implies no regularization\n",
    "        \n",
    "    epochs : int\n",
    "        Number of passes over the training set.\n",
    "        \n",
    "    eta : float (default: 0.001)\n",
    "        Learning rate.\n",
    "        \n",
    "    decrease_const : float (default: 0.0)\n",
    "        Decrease constant. Shrinks the learning rate\n",
    "        after each epoch via eta / (1 + epoch*decrease_const)\n",
    "        \n",
    "    shuffle : bool (default: True)\n",
    "        Shuffles training data every epoch if True to prevent circles.\n",
    "        \n",
    "    minibatches : int (default: 1)\n",
    "        Divides training data into k minibatches for efficiency.\n",
    "        \n",
    "    random_state : int (default: None)\n",
    "        Set random state for shuffling and initializing the weights.\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    cost_ : list\n",
    "      Sum of squared errors after each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_output, n_features, n_hidden=30,\n",
    "                 l1=0.0, l2=0.0, epochs=50, eta=0.001,\n",
    "                 decrease_const=0.0, shuffle=True,\n",
    "                 minibatches=1, random_state=None):\n",
    "        \n",
    "        self.n_output = n_output\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def encode_labels(self, y, k):\n",
    "        targets = y.reshape(-1)\n",
    "        one_hot_targets = np.eye(k)[targets]\n",
    "        return one_hot_targets\n",
    "        \"\"\"Encode the labels using one-hot representation\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        y : y represents target values.\n",
    "        k : number of classes\n",
    "\n",
    "        Returns\n",
    "        -----------\n",
    "        onehot array\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "\n",
    "    def initialize_weights(self):\n",
    "        \"\"\"Initialize using random numbers.\"\"\"\n",
    "        w1 = np.random.randn(self.n_hidden, self.n_features+1)\n",
    "        w2 = np.random.randn(self.n_output, self.n_hidden)\n",
    "        return w1, w2\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        if z >= 0:\n",
    "            a = np.exp(-z)\n",
    "            return 1 / (1 + a)\n",
    "        else:\n",
    "            a = np.exp(z)\n",
    "            return a / (1 + a)\n",
    "        \n",
    "        \n",
    "\n",
    "    def sigmoid_gradient(self, z):\n",
    "        \"\"\"Compute gradient of the sigmoid function\"\"\"\n",
    "        if z >= 0:\n",
    "            return (1 + np.exp(-z))**(-2)\n",
    "        else:\n",
    "            return np.exp(z)\n",
    "        \n",
    "        \n",
    "\n",
    "    def add_bias_unit(self, X, how='column'):\n",
    "        \"\"\"Add bias unit to array at index 0\"\"\"\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "        return X\n",
    "        \n",
    "        \n",
    "    def feedforward(self, X, w1, w2):\n",
    "        \n",
    "        a1 = self.add_bias_unit(X, how='column')\n",
    "        w1_t = np.transpose(w1)\n",
    "        z2 = np.matmul(a1, w1_t)\n",
    "        sigmoid_vec = np.vectorize(self.sigmoid)\n",
    "        a2 = sigmoid_vec(z2)\n",
    "        w2_t = np.transpose(w2)\n",
    "        z3 = np.matmul(a2, w2_t)\n",
    "        a3 = sigmoid_vec(z3)\n",
    "        \n",
    "        return a1, z2, a2, z3, a3\n",
    "        \n",
    "        \n",
    "        \"\"\"Compute feedforward step\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        a1 : array,\n",
    "            Input values with bias unit.\n",
    "        z2 : array,\n",
    "            Net input of hidden layer.\n",
    "        a2 : array,\n",
    "            Activation of hidden layer.\n",
    "        z3 : array,\n",
    "            Net input of output layer.\n",
    "        a3 : array,\n",
    "            Activation of output layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def L2_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        reg = lambda_/2 * (np.sum(np.square(w1)) + np.sum(np.square(w2)))\n",
    "        return reg\n",
    "\n",
    "    def L1_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L1-regularization cost\"\"\"\n",
    "        reg = lambda_/2 * (np.sum(np.absolute(w1)) + np.sum(np.absolute(w2)))\n",
    "        return reg\n",
    "        \n",
    "    \n",
    "    def get_cost(self, y_enc, output, w1, w2):\n",
    "        \"\"\"Compute cost function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_enc : array, one-hot encoded class labels.\n",
    "        \n",
    "        output : array, Activation of the output layer (feedforward)\n",
    "        \n",
    "        w1 : array, Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        cost : float, Regularized cost.\n",
    "\n",
    "        \"\"\"\n",
    "        \"\"\"COST IS NOT CORRECT IT MUST BE CROSS ENTROPY CALCULATION\"\"\"\n",
    "        #cost = - np.sum(np.multiply(y_enc, np.log(output)) + np.multiply((1-y_enc), np.log(1-output)))\n",
    "        reg_loss = loss + L2_reg(self, self.l2, w1, w2)\n",
    "        return reg_loss\n",
    "        \n",
    "\n",
    "    def get_gradient(self, a1, a2, a3, z2, z3, y_enc, w1, w2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        a1 : array, Input values with bias unit.\n",
    "        a2 : array, Activation of hidden layer.\n",
    "        a3 : array, Activation of output layer.\n",
    "        z2 : array, Net input of hidden layer.\n",
    "        y_enc : array, one-hot encoded class labels.\n",
    "        w1 : array, Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        grad1 : array, Gradient of the weight matrix w1.\n",
    "        grad2 : array, Gradient of the weight matrix w2.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # backpropagation\n",
    "        #TODO Implement\n",
    "        print (y_enc)\n",
    "        print (a3)\n",
    "        m = y_enc.shape[0]\n",
    "        grad = self.softmax(a3)\n",
    "        grad[range(m),y_enc] -= 1\n",
    "        grad = grad/m \"\"\"ONLY THIS PART IS WRONG\"\"\"\n",
    "        \n",
    "        dw2 = np.matmul(np.transpose(a2), grad)\n",
    "        delta2 = np.matmul(grad, (np.transpose(w2))) * sigmoid_gradient(self, a2)\n",
    "        dw1 = np.matmul(np.transpose(a1), delta2)\n",
    "        \n",
    "        return dw1, dw2\n",
    "        \n",
    "        # regularize\n",
    "        #TODO Implement\n",
    "        \n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        exp_scores = np.exp(X)\n",
    "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        \"\"\"Predict class labels\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, Input layer with original features.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        y_pred : array, Predicted class labels.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO Implement\n",
    "        \n",
    "    def shuffle_train_data(self, X_train, Y_train):\n",
    "        \"\"\"called after each epoch\"\"\"\n",
    "        perm = np.random.permutation(len(Y_train))\n",
    "        Xtr_shuf = X_train[perm]\n",
    "        Ytr_shuf = Y_train[perm]\n",
    "        return Xtr_shuf, Ytr_shuf\n",
    "        \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\"NOT SURE ABOUT IT\"\"\"\n",
    "        w1, w2 = self.initialize_weights()\n",
    "        for epoch in range(self.epochs):\n",
    "            if self.shuffle == True:\n",
    "                X, y = self.shuffle_train_data(X, y)\n",
    "            pointer = 0\n",
    "            while pointer + self.minibatches <= len(X):\n",
    "                x_batch = X[pointer: pointer + self.minibatches]\n",
    "                y_batch = y[pointer: pointer + self.minibatches]\n",
    "                y_enc = self.encode_labels(y_batch, 10)\n",
    "                a1, z2, a2, z3, a3 = self.feedforward(x_batch, w1, w2)\n",
    "                dw1, dw2 = self.get_gradient(a1, a2, a3, z2, z3, y_enc, w1, w2)\n",
    "                w1 += -eta * dw1\n",
    "                w2 += -eta * dw2\n",
    "                pointer += batch_size\n",
    "            cost = self.get_cost(y_enc, output, w1, w2)\n",
    "            print('epoch: ', epoch + 1, ' cost =', cost)\n",
    "            eta = eta / (1 + epoch * decrease_const)\n",
    "            \n",
    "            \n",
    "        \"\"\" Learn weights from training data.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, Input layer with original features.\n",
    "        y : array, Target class labels.\n",
    "        print_progress : bool, Prints the progress\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLP(n_output=10, \n",
    "                  n_features=X_train.shape[1], \n",
    "                  n_hidden=50, \n",
    "                  l2=0.1, \n",
    "                  l1=0.0, \n",
    "                  epochs=1000, \n",
    "                  eta=0.001,\n",
    "                  decrease_const=0.00001,\n",
    "                  minibatches=50, \n",
    "                  shuffle=True,\n",
    "                  random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]]\n",
      "[[  2.57448478e-02   5.99421176e-01   1.26945052e-02   2.28397370e-02\n",
      "    6.73295867e-03   4.32540050e-01   2.32872622e-01   9.98748582e-01\n",
      "    3.56026506e-02   8.84334365e-01]\n",
      " [  6.41486543e-03   9.47423524e-01   1.05017594e-02   9.78340379e-01\n",
      "    1.89416841e-02   8.93068742e-01   8.31932912e-01   9.97188866e-01\n",
      "    9.94135998e-01   9.99711031e-01]\n",
      " [  1.63992081e-03   2.31329316e-01   1.13408193e-03   2.17218301e-01\n",
      "    1.94212086e-02   3.62423855e-01   2.26774871e-02   9.99016150e-01\n",
      "    2.45306998e-01   9.95372699e-01]\n",
      " [  1.90290506e-03   1.26876480e-01   6.77771890e-03   1.67888429e-02\n",
      "    1.56216532e-01   3.50237779e-04   3.47014239e-03   8.33174462e-01\n",
      "    8.52942865e-01   9.97878944e-01]\n",
      " [  1.66694912e-01   7.11134271e-01   2.53588370e-02   2.85945259e-02\n",
      "    1.11545073e-02   4.16331452e-01   1.33570898e-03   9.99909798e-01\n",
      "    3.43156071e-01   7.95637881e-01]\n",
      " [  1.52803121e-03   6.95149943e-04   9.60227538e-05   2.04647185e-03\n",
      "    3.14756280e-02   7.71869933e-01   2.40600742e-02   9.56670580e-01\n",
      "    6.87403767e-01   4.26119267e-01]\n",
      " [  2.61554535e-02   9.97675881e-01   3.76721241e-04   2.48122178e-03\n",
      "    9.68293701e-01   8.37019519e-01   7.07652338e-03   9.76210238e-01\n",
      "    2.07071688e-01   7.02697064e-01]\n",
      " [  3.11278339e-01   2.80760471e-01   1.02758632e-01   7.10348615e-02\n",
      "    4.56503694e-01   9.76082838e-01   7.00265427e-04   9.96055251e-01\n",
      "    9.86827394e-01   6.46044265e-01]\n",
      " [  2.95478966e-02   3.06866322e-01   4.17734923e-04   6.60178151e-01\n",
      "    1.81088365e-01   9.97840899e-01   1.46700523e-02   9.99652817e-01\n",
      "    9.20262395e-01   2.04505991e-01]\n",
      " [  5.89681727e-03   8.42139777e-02   1.49271519e-02   2.96264304e-01\n",
      "    1.68517895e-01   8.88482782e-01   5.32003814e-02   9.98196996e-01\n",
      "    9.92117535e-01   9.93021075e-01]\n",
      " [  2.54428475e-02   2.10827341e-01   1.19280891e-04   1.39682761e-02\n",
      "    5.66274932e-01   1.04063186e-01   2.62583935e-01   9.98399060e-01\n",
      "    1.10603575e-02   9.99622024e-01]\n",
      " [  5.59933037e-03   7.44095761e-01   8.45520990e-02   1.17012826e-01\n",
      "    6.12448923e-01   1.97909340e-03   3.18401613e-02   9.91238909e-01\n",
      "    1.29433416e-02   9.67851301e-01]\n",
      " [  7.27142650e-01   9.03274182e-01   7.23246430e-01   3.35869329e-04\n",
      "    9.95834537e-01   9.57502582e-01   1.71209626e-01   2.55762879e-01\n",
      "    2.44733494e-01   4.38527929e-01]\n",
      " [  4.44986181e-02   7.70289085e-01   6.87438165e-01   1.08664720e-01\n",
      "    7.65642342e-01   9.97500795e-01   3.14967762e-02   9.85898154e-01\n",
      "    9.89474708e-01   7.86775284e-01]\n",
      " [  6.33532133e-01   2.94111143e-02   7.76917667e-01   2.22799789e-02\n",
      "    4.16246536e-03   8.45365401e-01   6.50199133e-01   8.61856394e-01\n",
      "    8.80599742e-01   9.98447737e-01]\n",
      " [  2.53012496e-01   8.14508342e-03   7.52174663e-03   8.97944140e-03\n",
      "    6.42067771e-01   2.32029817e-02   2.20379554e-01   9.99616206e-01\n",
      "    9.68547208e-01   9.97141939e-01]\n",
      " [  9.75049824e-01   1.67771894e-02   8.27245550e-03   4.00592336e-02\n",
      "    1.25862086e-02   1.54575615e-02   4.06815689e-02   9.99977872e-01\n",
      "    1.30719134e-01   9.99839533e-01]\n",
      " [  1.95164307e-01   4.28917559e-01   1.25228486e-04   3.95583414e-03\n",
      "    3.02725375e-01   2.06229407e-03   4.88193813e-02   9.92458214e-01\n",
      "    1.30765916e-01   9.98731362e-01]\n",
      " [  1.25564078e-02   8.51736831e-02   1.17425345e-02   3.50390672e-02\n",
      "    7.59934322e-01   6.61366760e-01   2.79420010e-03   9.97799238e-01\n",
      "    9.97814115e-01   2.75428006e-01]\n",
      " [  2.64134939e-03   2.01851809e-02   7.80039618e-05   2.55470194e-01\n",
      "    4.43101676e-01   9.39504957e-01   4.69630067e-01   9.90911710e-01\n",
      "    9.40911372e-01   9.95333231e-01]\n",
      " [  9.79502601e-01   7.16683696e-01   5.64311350e-02   7.54774489e-03\n",
      "    4.61983280e-03   8.07818412e-01   4.13928524e-02   9.96156757e-01\n",
      "    1.16115330e-02   9.98468846e-01]\n",
      " [  3.16846870e-01   3.19892611e-02   2.14054030e-03   1.67142303e-02\n",
      "    3.44886624e-01   1.35321050e-02   6.25168287e-02   5.67291523e-01\n",
      "    8.89441297e-01   2.01721932e-01]\n",
      " [  3.80908581e-04   9.42492430e-05   4.55129708e-03   4.64236112e-03\n",
      "    5.34953346e-01   4.37066667e-01   8.68539885e-01   9.36781771e-01\n",
      "    9.96420706e-01   8.83911135e-01]\n",
      " [  8.50011391e-03   1.17953850e-02   8.64353072e-03   9.94801902e-01\n",
      "    1.80790979e-02   9.97194635e-01   9.31763795e-02   9.99991973e-01\n",
      "    9.16276597e-01   9.93630514e-01]\n",
      " [  2.02813065e-01   3.27872995e-03   4.97411782e-03   4.29356601e-03\n",
      "    5.33335324e-02   7.97375652e-01   8.91310305e-02   9.27501935e-01\n",
      "    3.24274041e-01   9.99387136e-01]\n",
      " [  7.36123652e-01   7.77028595e-03   2.23032159e-02   2.35923325e-02\n",
      "    3.35694746e-01   3.08468636e-03   2.48036316e-01   7.64810019e-01\n",
      "    3.16934653e-01   9.99212499e-01]\n",
      " [  2.26984038e-01   2.13237536e-02   1.97667178e-05   1.54699532e-04\n",
      "    1.17637792e-01   2.22521342e-01   1.51021761e-01   9.98472779e-01\n",
      "    1.11296703e-01   9.96246788e-01]\n",
      " [  8.08299278e-01   3.51674763e-01   3.66329027e-03   1.45274578e-03\n",
      "    3.29806469e-03   9.27191155e-01   2.86724567e-01   9.99483461e-01\n",
      "    6.32953049e-02   9.24304546e-01]\n",
      " [  7.34422209e-01   8.22177922e-05   4.55249553e-01   7.78610598e-02\n",
      "    3.88332225e-03   9.71238281e-01   4.65922359e-01   8.92227773e-01\n",
      "    9.97556077e-01   8.39061302e-01]\n",
      " [  1.09974660e-01   8.88630039e-01   3.31143361e-02   5.82537593e-02\n",
      "    7.30593876e-01   1.01029881e-02   5.60277835e-03   9.99145011e-01\n",
      "    9.94507058e-01   9.81156906e-01]\n",
      " [  1.50506893e-01   6.08405228e-02   1.89352325e-02   2.20686829e-01\n",
      "    5.62672939e-01   8.41499039e-02   5.18627069e-03   7.74160071e-02\n",
      "    8.55665860e-01   9.07877287e-01]\n",
      " [  2.98647694e-01   6.35973227e-02   1.17099222e-04   1.01490293e-04\n",
      "    4.77655212e-01   3.76017384e-02   2.23384950e-02   9.77088999e-01\n",
      "    1.48988413e-01   7.20523172e-01]\n",
      " [  4.03588490e-04   1.63662927e-02   1.36246690e-03   3.84567157e-03\n",
      "    4.15357891e-01   1.69050146e-01   3.64161711e-02   8.96984636e-01\n",
      "    9.61604845e-01   9.56401125e-01]\n",
      " [  7.58713546e-01   1.02777380e-01   5.58551958e-04   4.55070395e-04\n",
      "    8.43456638e-01   4.13860506e-01   4.21865799e-03   9.98947611e-01\n",
      "    1.42281224e-01   9.83805264e-01]\n",
      " [  1.94743288e-01   1.09862720e-03   4.18115416e-03   3.89744248e-01\n",
      "    2.68636963e-04   9.72931432e-02   9.43505001e-01   9.99919801e-01\n",
      "    8.27765427e-02   9.99425041e-01]\n",
      " [  9.94717236e-01   1.74731955e-02   2.07202980e-01   1.18260573e-02\n",
      "    1.69412904e-02   7.77371047e-01   3.20936711e-02   9.26925093e-01\n",
      "    9.50551516e-01   9.99786829e-01]\n",
      " [  1.69401677e-02   1.64284661e-01   7.72314838e-03   2.03635534e-03\n",
      "    1.06212141e-01   4.71806877e-01   4.56431876e-02   9.76596023e-01\n",
      "    1.26548182e-01   9.96344241e-01]\n",
      " [  9.98434973e-01   7.02176176e-04   3.14701818e-01   4.61207194e-04\n",
      "    6.02740641e-03   9.37670351e-01   9.90060809e-02   8.86685133e-01\n",
      "    1.37237731e-02   9.99973953e-01]\n",
      " [  5.14651488e-01   5.64041323e-03   1.05209604e-01   1.44175829e-02\n",
      "    4.91846166e-01   7.54291968e-01   4.88384104e-01   9.98667889e-01\n",
      "    1.33240445e-01   9.99891910e-01]\n",
      " [  1.10591368e-02   2.19089277e-01   3.34709200e-01   9.94854427e-01\n",
      "    5.29216051e-02   9.71448080e-01   7.58837959e-01   9.99969744e-01\n",
      "    2.89985520e-01   9.99904449e-01]\n",
      " [  2.29951468e-02   5.09103333e-01   3.98669321e-03   7.59136104e-03\n",
      "    7.40999719e-01   5.22964705e-03   7.14815125e-03   4.75342775e-01\n",
      "    9.97494486e-01   2.85679180e-01]\n",
      " [  3.70952200e-02   9.23588380e-01   1.38247155e-02   5.63421248e-03\n",
      "    8.56273134e-01   9.68435781e-01   3.73136723e-02   9.81682321e-01\n",
      "    9.78643864e-01   5.54914252e-01]\n",
      " [  2.99590959e-01   1.39588631e-03   2.18776692e-03   5.90331879e-01\n",
      "    1.08128701e-03   2.20979825e-01   1.94842245e-01   9.99005246e-01\n",
      "    9.71684692e-01   9.99878618e-01]\n",
      " [  2.27476045e-01   5.73575279e-02   2.17026181e-02   3.59475073e-03\n",
      "    2.36109428e-01   4.81340448e-01   6.04008928e-02   9.96581219e-01\n",
      "    9.88323959e-01   9.09928633e-01]\n",
      " [  2.37723926e-02   6.58152887e-03   7.91602747e-05   6.18355182e-03\n",
      "    1.26705754e-01   4.93351159e-01   3.15049070e-02   9.90606433e-01\n",
      "    8.06598097e-02   9.18887584e-01]\n",
      " [  3.54805767e-02   5.39318446e-04   5.35478980e-03   2.08007531e-02\n",
      "    9.93530277e-01   7.24679379e-03   1.18711068e-01   9.30845061e-01\n",
      "    9.00504512e-01   9.78689480e-01]\n",
      " [  7.83790353e-01   7.64169051e-03   1.89400776e-04   2.64645975e-01\n",
      "    7.64577742e-04   9.81386642e-01   4.97520920e-02   9.99997756e-01\n",
      "    6.65658910e-02   9.97508073e-01]\n",
      " [  3.17949757e-02   4.95826168e-02   4.60283434e-02   2.61040171e-01\n",
      "    2.61899994e-01   9.48642997e-01   4.08407210e-01   8.95166233e-01\n",
      "    9.99290486e-01   9.35704469e-01]\n",
      " [  7.96316750e-01   6.64994955e-03   8.42024034e-04   1.61588975e-02\n",
      "    2.89325769e-04   9.90149741e-01   9.16814974e-01   9.99639650e-01\n",
      "    6.61163707e-04   9.99789493e-01]\n",
      " [  7.63492051e-04   1.19147295e-03   5.10321065e-03   5.44453571e-02\n",
      "    2.60472648e-03   9.86385398e-01   5.82165109e-01   9.99036651e-01\n",
      "    1.98337962e-01   9.99525722e-01]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-8bfb3b7425d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-82-fb870005f094>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, print_progress)\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0my_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m                 \u001b[0mdw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdw2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m                 \u001b[0mw1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0meta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdw1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0mw2\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0meta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdw2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-82-fb870005f094>\u001b[0m in \u001b[0;36mget_gradient\u001b[0;34m(self, a1, a2, a3, z2, z3, y_enc, w1, w2)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_enc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_enc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "nn.fit(X_train, y_train, print_progress=True)\n",
    "z=np.array([-4, 1, 2, 3])\n",
    "print (nn.sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training error for every iteration\n",
    "# in every epoch\n",
    "\n",
    "# TODO Implement\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training error in every epoch\n",
    "# TODO Implement\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Training Accuracy\n",
    "# TODO Implement\n",
    "\n",
    "print('Training accuracy: %.2f%%' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Test Accuracy\n",
    "# TODO Implement\n",
    "\n",
    "print('Test accuracy: %.2f%%' % (acc * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
