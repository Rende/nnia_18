{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment sheet 3: Numerical Computation and Prinicipal Component Analysis (Deadline: Nov 24, 23:59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set notebook to full width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Issues with Softmax $~$ (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture you were introduced to the softmax function which is used to generate probabilities corresponding to the output labels. Typically, the input to the softmax function is a vector of numerical values over the labels and the output is a vector(of same dimension as the input vector) of corresponding probabilities.\n",
    "**Softmax function is given by,** $~$\n",
    "$$Softmax(x)_i = \\frac{exp(x_i)}{\\sum_{j=1}^n exp(x_j)}$$\n",
    "\n",
    "**Numerical issues might occur when computing softmax functions on a computer which can perform computations\n",
    "only upto a certain precision.** [Suggested reading $-$ [chapter 4.1 of DeepLearningBook](http://www.deeplearningbook.org/contents/numerical.html)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$1$. Name these numerical issues and explain them. ($1$ points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$2$. Suggest a remedy (with explanation on why it works) to overcome these numerical issues occuring with Softmax computation. Prove that this remedy actually does not change the softmax criteria. Describe a situation where the proposed remedy still fails to remove instability. ($1$ point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$3$. First write a naive Softmax implementation, in numpy, that can produce numerical instability. Then write a modified Softmax implementation which is numerically stable.  ($0.5 + 0.5 = 1$ points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO : Define inputs\n",
    "\n",
    "def softmax_naive(inputs):\n",
    "    \"\"\"Unstable Softmax function\"\"\"\n",
    "    \n",
    "    # TODO : Implement\n",
    "    pass\n",
    "\n",
    "def softmax_modified(inputs):\n",
    "    \"\"\"Stable Softmax function\"\"\"\n",
    "    \n",
    "    # TODO : Implement\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis $~$ (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$4$. Is PCA supervised or unsupervised, logically explain your answer. Which is the tunable parameter in PCA?\n",
    "Briefly explain the role of this parameter in PCA.  ($1+0.5+0.5 = 2$ points)\n",
    "\n",
    "1)Unsupervised,PCA helps in producing low dimensional representation of the dataset by identifying a set of linear combination of features which have maximum variance and are mutually un-correlated. This linear dimensionality technique could be helpful in understanding latent interaction between the variable in an unsupervised setting.\n",
    "In a supervised setting such as Classification or Regression, one observes both a set of input variables(X1, .. Xn ) and response or output variables (Y). However, in un-supervised setting the goal is to identify “meaningful” informative patterns in the given data. There is no corresponding output Y of each input X here in PCA. \n",
    "\n",
    "2)Tunable parameter is what we want to reduce the dimension to,here in this Task 4, we reduce dimensions from 2 to 1. \n",
    "\n",
    "3)The encoding formula,  $f(x)=D^Tx$, D contains the eigenvectors corresponding to the larger eigenvalues of $X^TX$, so the column of D represents the degree of matrix after reducing dimensions.In PCA, the eigendecomposition is to tune this parameter, we want to use lower dimensions of data to represents the oringinal one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$5$. Consider the following data:\n",
    "\n",
    "setA: ${\\bf x}^{(1)}$=$(2, 4)^T$, ${\\bf x}^{(2)}$=$(2, 2)^T$, ${\\bf x}^{(3)}$=$(3, 1)^T$, ${\\bf x}^{(4)}$=$(5, 1)^T$ \n",
    "\n",
    "setB: ${\\bf x}^{(1)}$=$(-1, 1)^T$, ${\\bf x}^{(2)}$=$(-2, 2)^T$, ${\\bf x}^{(3)}$=$(-1, 3)^T$, ${\\bf x}^{(4)}$=$(-1, 4)^T$\n",
    "\n",
    "$(a)$ Compress the above sets of vectors into a one-dimensional set using PCA, i.e., derive the encoder function $f(x)=D^{T}x$ as defined in the lecture. Then apply f to the datasets in order to compress them. ($1.5 + 1.5$ points)\n",
    "\n",
    "\n",
    "\n",
    "$\\mathbf{SetA}=\\left[\\begin{array}{cccc}\n",
    "   2 & 4\\\\\n",
    "   2 & 2\\\\\n",
    "   3 & 1\\\\\n",
    "   5 & 1\\\\\n",
    "  \\end{array}\\right]$   ,    Mean of vectors in Set A: $\\mathbf{MeanA}=\\left[\\begin{array}{cccc}\n",
    "   3 & 2\\\\ \n",
    "  \\end{array}\\right]$\n",
    "  Remove mean, $\\mathbf{X}=\\left[\\begin{array}{cccc}\n",
    "   -1 & 2\\\\\n",
    "   -1 & 0\\\\\n",
    "   0 & -1\\\\\n",
    "   2 & 1\\\\\n",
    "  \\end{array}\\right]$\n",
    "  \n",
    "  $\\mathbf{x=X^T}=\\left[\\begin{array}{cccc}\n",
    "   -1 & -1 & 0 & 2\\\\\n",
    "   2 & 0 & -1 & -1\\\\\n",
    "  \\end{array}\\right]$\n",
    "  \n",
    "  \n",
    "  $$\\mathbf{X^TX}=\\left[\\begin{array}{cccc}\n",
    "   6 & -4\\\\\n",
    "  -4 &  6\\\\\n",
    "  \\end{array}\\right] $$Make Eigendecomposition of $X^TX$,\n",
    " let $|X^TX-\\lambda I|=0$\n",
    " The larger eigenvalue is:$\\lambda_1=10$\n",
    " \n",
    " From $|X^TX-10I|=0$, we have D containing the eigenvectors corresponding to the largest eigenvalues of $X^TX$,\n",
    "  $\\mathbf{D_1}=\\left[\\begin{array}{cccc}\n",
    "   \\frac{\\sqrt{2}}{2}\\\\\n",
    "  \\frac{\\sqrt{2}}{-2}\n",
    "  \\end{array}\\right]$ \n",
    "\n",
    "Now we encode the data,$\\mathbf{D_1^Tx}=\\left[\\begin{array}{cccc}\n",
    "   \\frac{-3\\sqrt{2}}{2} &\n",
    "   \\frac{\\sqrt{2}}{-2}  &\n",
    "   \\frac{\\sqrt{2}}{2}   &\n",
    "   \\frac{3\\sqrt{2}}{2}\\\\\n",
    "  \\end{array}\\right]$ \n",
    "\n",
    "As to $\\mathbf{SetB}=\\left[\\begin{array}{cccc}\n",
    "   -1 & 1\\\\\n",
    "   -2 & 2\\\\\n",
    "   -1 & 3\\\\\n",
    "   -1 & 4\\\\\n",
    "  \\end{array}\\right]$,\n",
    "following the same steps, we have $\\mathbf{Y}=\\left[\\begin{array}{cccc}\n",
    "   0.25 & -1.5\\\\\n",
    "   -0.75 & 0.5\\\\\n",
    "   0.25 & 0.5\\\\\n",
    "   0.25 & 1.5\\\\\n",
    "  \\end{array}\\right]$ , as to Y,\n",
    "the larger eigenvalue is $\\lambda_2=5.058$,\n",
    "  $\\mathbf{D_2}=\\left[\\begin{array}{cccc}\n",
    "   0.1153\\\\\n",
    "   0.9933\\\\\n",
    "  \\end{array}\\right]$ \n",
    "  Now we encode the data,$\\mathbf{D_2^Ty}=\\left[\\begin{array}{cccc}\n",
    "   -1.4611 &-0.5831 & 0.5255 & 1.5188\n",
    "   \\\\\n",
    "  \\end{array}\\right]$ \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(b)$. For both the above sets sketch the corresponding datasets in a separate figure. \n",
    "Also include the reconstructed vectors into the corresponding figures. ($2$ points)\n",
    "\n",
    "$ reconstructed vector_1=D_1D_1^Tx=\\left[\\begin{array}{cccc}\n",
    "   -1.5 & -.5 & 0.5 & 1.5\\\\\n",
    "   1.5 & 0.5 &-.5 & -1.5\\\\\n",
    "  \\end{array}\\right]$\n",
    "  \n",
    "$ reconstructed vector_2=D_2D_2^Ty=\\left[\\begin{array}{cccc}\n",
    "   -0.1685 & -0.0672 & 0.0606 & 0.1751\\\\\n",
    "   -1.4513 & -0.5792 & 0.5220 & 1.5083\\\\\n",
    "  \\end{array}\\right]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1110234a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110e6f4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x1=np.array([-1,-1,0,2])\n",
    "y1=np.array([2,0,-1,-1])\n",
    "\n",
    "x1_rec=np.array([-1.5,-0.5,0.5,1.5])\n",
    "y1_rec=np.array([1.5,0.5,-0.5,-1.5])\n",
    "\n",
    "\n",
    "plt.scatter(x1, y1)\n",
    "plt.scatter(x1_rec, y1_rec)\n",
    "\n",
    "plt.legend(['originalof set A', 'reconstrcted of set A'], loc='upper right')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "x2=np.array([0.25,-0.75,0.25,0.25])\n",
    "y2=np.array([-1.5,-0.5,0.5,-1.5])\n",
    "\n",
    "x2_rec=np.array([-0.1685,-0.0672,0.0606,0.1751])\n",
    "y2_rec=np.array([-1.4513,-0.5792,0.5220,1.5086])\n",
    "\n",
    "\n",
    "plt.scatter(x2, y2)\n",
    "plt.scatter(x2_rec, y2_rec)\n",
    "\n",
    "plt.legend(['original of set B', 'reconstrcted of set B'], loc='upper left')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent and Newton's method $~$ (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppose $f(x) = 2x^3 - 5x + 6$ **\n",
    "\n",
    "$6$. Write down the mathematical expressions for minimizing f(x) using Gradient descent(GD) and then using Newton's Method(NM). ($1$ points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$7$. Report the updated values of x, both for GD and NM, at $x = 0$. what do you observe? ($1$ points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$8$. Perform GD and NM for the above function using Tensorflow. ($1.5 + 1.5$ points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-b7ba9ee1d0d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# TODO : Implement Gradient Descent with Tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# TODO : Implement Gradient Descent with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# TODO : Implement Newton's Method with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent computation and visualisation $~$ (3 + 2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now visualize the Gradient Descent algorithm to fit a straight line to data generated using  $y = \\theta_{true}x$ $~$, i.e., use this expression to first produce the data (see code below the lines starting with m=20 and following) and then try to fit a straight line to this data. Fitting a straight line means that you have to approximate this $\\theta_{true}$ parameter using the hypothesis or predictive model by minimizing the cost function defined below.\n",
    "\n",
    "**For this task you should minimize a cost function of the form:**\n",
    "$$\\frac{1}{2m}\\sum_{i=1}^m [h_{\\theta}(x^i)-y^i]^2$$\n",
    "where\n",
    "- $x^i$ is the $i^{th}$ input \n",
    "\n",
    "- $y^i$ is the true $i^{th}$ response or output\n",
    "\n",
    "- $h_{\\theta}(x)$ is the hypothesis or predictive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assume $~$ $h_{\\theta}(x) = \\theta x$ $~$ to be the hypothesis or predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate the true data which is to be fitted\n",
    "m = 20                      # number of data points for x\n",
    "theta_true = 0.5            # corresponds to the true slope\n",
    "x = np.linspace(-1,1,m)     # x values or inputs\n",
    "y = theta_true * x          # True response\n",
    "\n",
    "\n",
    "# Create a subplot window\n",
    "# On the left window plot the true data and the approximation \n",
    "# that you obtain with different estimates of the slope theta_true\n",
    "# and on the right window plot the cost function \n",
    "\n",
    "# TODO : Create the subplot window\n",
    "\n",
    "def hypothesis(x, theta):\n",
    "    \"\"\"Our \"hypothesis or predictive model\", a straight line through the origin.\"\"\"\n",
    "    \n",
    "    # TODO : Implement\n",
    "    pass\n",
    "\n",
    "def cost_func(theta):\n",
    "    \"\"\"The cost function describing the goodness of fit.\"\"\"  \n",
    "    \n",
    "    # TODO : Implement\n",
    "    pass\n",
    "\n",
    "\n",
    "# First construct a grid of theta parameter and their corresponding\n",
    "# cost function values.\n",
    "theta_grid = np.linspace(-0.2,1,50)\n",
    "# Find the cost function values to be stored in J_grid\n",
    "# TODO : Create J_grid\n",
    "\n",
    "\n",
    "# Plot the cost function as a function of theta.\n",
    "# TODO : Do the plot\n",
    "\n",
    "\n",
    "# Take N steps with learning rate alpha down the steepest gradient,\n",
    "# starting at theta = 0.\n",
    "N = 10\n",
    "alpha = 1 \n",
    "# this is just a starting value of alpha, \n",
    "# you must consider different values of alpha (try using large values)\n",
    "# and redo the steps below to generate different plots\n",
    "theta = [0]\n",
    "\n",
    "\n",
    "# TODO :Compute the N steps down the steepest gradient\n",
    "\n",
    "# TODO : Annotate the cost function plot with coloured points indicating the\n",
    "# parameters chosen and red arrows indicating the steps down the gradient.\n",
    "# Also plot the fit function on the left window of the subplot in a matching colour.\n",
    "\n",
    "# TODO : Put the labels, titles and a legend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now assume that the data is generated using  $y = \\theta_1x + \\theta_0$\n",
    "** Following the same logic you applied for the above task define a predictive model \n",
    "and perform 5 steps of gradient descent with learning rate alpha = 0.7 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate the true data which is to be fitted\n",
    "m = 20\n",
    "theta0_true = 2\n",
    "theta1_true = 0.5\n",
    "x = np.linspace(-1,1,m)\n",
    "y = theta0_true + theta1_true * x\n",
    "\n",
    "# Create the sub-plot: left window is the data, right window will be the cost function.\n",
    "# TODO\n",
    "\n",
    "\n",
    "def hypothesis(x, theta0, theta1):\n",
    "    \"\"\"Our \"hypothesis function\", a straight line.\"\"\"\n",
    "    \n",
    "    # TODO : Implement\n",
    "    pass\n",
    "\n",
    "def cost_func(theta0, theta1):\n",
    "    \"\"\"The cost function, J(theta0, theta1) describing the goodness of fit.\"\"\"\n",
    "    \n",
    "    # TODO : Implement\n",
    "    pass\n",
    "\n",
    "\n",
    "# First construct a grid of (theta0, theta1) parameter pairs and their\n",
    "# corresponding cost function values.\n",
    "theta0_grid = np.linspace(-1,4,101)\n",
    "theta1_grid = np.linspace(-5,5,101)\n",
    "\n",
    "# TODO : Compute the cost function values\n",
    "\n",
    "\n",
    "# TODO : Do a labeled contour plot for the cost function on right window of the above subplot\n",
    "\n",
    "\n",
    "# TODO : Take 5 steps with learning rate alpha = 0.7 down the steepest gradient,\n",
    "# starting at (theta0, theta1) = (0, 0).\n",
    "\n",
    "\n",
    "# TODO : Annotate the cost function plot with coloured points indicating the\n",
    "# parameters chosen and red arrows indicating the steps down the gradient.\n",
    "# Also plot the fit function on the left window in a matching colour.\n",
    "\n",
    "\n",
    "# TODO : Add the labels, titles and a legend to the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Algebra Bonus\n",
    "- [Additional material - Linear Algebra Basics](http://www.cs.ubc.ca/~schmidtm/Documents/2009_Notes_LinearAlgebra.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trace of a Matrix $~$ (3 points)\n",
    "- [Reading material on Trace](https://en.wikipedia.org/wiki/Trace_(linear_algebra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prove that the trace of a ***symmetric positive definite*** matrix is the sum of its eigenvalues.    ($0.5$ points)\n",
    "\n",
    "Suppose $\\mathbf{Y}$ is a $m \\times n$ matrix with $m \\leq n$ and has ***full rank***, then\n",
    "\n",
    "$(a)$.   Give the rank of $\\mathbf{Y}$.                                                                 ($0.5$ points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(b)$.  Show that trace of $\\mathbf{Y}^{T}(\\mathbf{Y}^T\\mathbf{Y})^{-1}\\mathbf{Y}$ = rank($\\mathbf{Y}$)                                     ($1$ points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(c)$. Prove that $\\mathbf{Y}^{T}(\\mathbf{Y}^T\\mathbf{Y})^{-1}\\mathbf{Y}$ is the projection matrix w.r.t space defined by $\\mathbf{Y}$.     ($1$ points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jacobian $~$ (3 points)\n",
    "\n",
    "***[Reading material on Jacobian](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that the Jacobian determinant of $\\frac{\\partial(fg, h)}{\\partial(u, v)}$ is equal to $\\frac{\\partial(f, h)}{\\partial(u, v)}g + f\\frac{\\partial(g, h)}{\\partial(u, v)}$,\n",
    "\n",
    "where $f$,$g$, and $h$ are functions of $u$ and $v$ (i.e., $f(u,v)$, $g(u,v)$, and $h(u,v)$)   ($3$ points)\n",
    "\n",
    "Hint: Use the property $\\frac{\\partial(y, x)}{\\partial(u, v)} = \\frac{\\partial(y)}{\\partial(u)}\\frac{\\partial(x)}{\\partial(v)}-\\frac{\\partial(y)}{\\partial(v)}\\frac{\\partial(x)}{\\partial(u)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hessian $~$ (2 points)\n",
    "***[Reading material on Hessian](https://en.wikipedia.org/wiki/Hessian_matrix)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathbf{M}=\\left[\\begin{array}{cccc}\n",
    "   5 & 1 & 0 & 1\\\\\n",
    "   1 & 4 & 1 & 0\\\\\n",
    "   0 & 1 & 3 & 1\\\\\n",
    "   1 & 0 & 1 & 2\\\\\n",
    "  \\end{array}\\right]$\n",
    "  \n",
    "denote the Hessian matrix at particular point for a particular function.\n",
    "\n",
    "$(a)$. What properties of the functional can you infer from the above information.(give mathematical reasons) ($1$ point)\n",
    "\n",
    "Set $|M-\\lambda I|=0$\n",
    "The eingenvalues are $1, 3, 4 ,6 $ which are pisitive values, so M is positive defined. It means at this particular point, it is a local minimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(b)$. Provide a generic mathematical representation (e.g. the generic representation of a straight line is $ax+by+c=0$) for the above function. ($1$ point)\n",
    "$ \\frac{5}{2}x_1^2+x_1x_2+x_1x_4+2x_2^2+x_2x_3+x_3x_4+x_4^2+\\frac{3x_3^2 }{2}+c=0$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
